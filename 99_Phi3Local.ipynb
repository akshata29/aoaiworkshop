{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --pre onnxruntime-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the ONNX model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import readline\n",
    "import glob\n",
    "\n",
    "import onnxruntime_genai as og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = og.Model(\"./phi3vision/cpu-int4-rtn-block-32-acc-level-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageAnswer(question, imagePath=None):\n",
    "    processor = model.create_multimodal_processor()\n",
    "    tokenizer_stream = processor.create_stream()\n",
    "    prompt = \"<|user|>\\n\"\n",
    "    if imagePath:\n",
    "        image = og.Images.open(imagePath)\n",
    "    prompt += \"<|image_1|>\\n\"\n",
    "\n",
    "    prompt += f\"{question}<|end|>\\n<|assistant|>\\n\"\n",
    "\n",
    "    if imagePath:\n",
    "        print(\"Processing image and prompt...\")\n",
    "        inputs = processor(prompt, images=image)\n",
    "    else:\n",
    "        inputs = processor(prompt, images=None)\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.set_inputs(inputs)\n",
    "    params.set_search_options(max_length=3072)\n",
    "\n",
    "    generator = og.Generator(model, params)\n",
    "\n",
    "    while not generator.is_done():\n",
    "        generator.compute_logits()\n",
    "        generator.generate_next_token()\n",
    "\n",
    "        new_token = generator.get_next_tokens()[0]\n",
    "        print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
    "\n",
    "    for _ in range(3):\n",
    "        print()\n",
    "\n",
    "    # Delete the generator to free the captured graph before creating another one\n",
    "    del generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image and prompt...\n",
      "Generating response...\n",
      "The image depicts a lively scene from a 'Despicable Me' movie set. In the center, Gru, the main character, is seated at a round table with a blue tablecloth, surrounded by various Minions. He is wearing a white lab coat and is holding a clipboard, suggesting he is in a scientific or research role. The Minions are in different poses, some are standing, and others are seated on stools around the table. The background features a large, open space with a staircase leading to a higher level, and the walls are adorned with various mechanical and scientific equipment, giving the impression of a laboratory or workshop. The overall atmosphere is one of organized chaos, typical of a 'Despicable Me' movie set.</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getImageAnswer(\"Describe the image and details \", './Data/Gru/images/1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Phi3 using Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of models in library - https://ollama.com/library\n",
    "\n",
    "##### Currently phi3 vision is not supported yet, but eventually it will be just change in the name - https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "https://github.com/ollama/ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download/Serve the model using ollama ollama run phi3\n",
    "##### If required set the environment variable for OLLAMA_HOST = 127.0.0.1:11435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The sky appears blue to us because of a phenomenon called Rayleigh scattering. When sunlight reaches Earth's atmosphere, it encounters molecules and small particles that are present in the air. Sunlight is composed of different colors of light, each with its own wavelength. Blue light has a shorter wavelength compared to other visible colors like red or yellow.\n",
      "\n",
      "As sunlight travels through Earth's atmosphere, it interacts with these molecules and particles, causing the blue light to scatter in all directions more efficiently than the other colors due to its shorter wavelength. This scattered blue light reaches our eyes from different angles, which gives us the impression that the sky is predominantly blue during daytime.\n",
      "\n",
      "In contrast, at sunrise or sunset, when the Sun's rays have to travel through a greater thickness of Earth's atmosphere, most of the shorter wavelength colors (blue and green) are scattered away from our line of sight, leaving behind longer-wavelength colors like red, orange, and pink, which explains why we see these hues during those times.\n"
     ]
    }
   ],
   "source": [
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11435',\n",
    "    model=\"phi3\"\n",
    ")\n",
    "print(ollama.invoke(\"why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QnA usig langchain and Phi3 locally on existing index we created in 2_AskQuestion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI\n",
    "from Utilities.envVars import *\n",
    "import os\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain import hub\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "os.environ[\"AZURESEARCH_FIELDS_ID\"] = \"id\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT\"] = \"content\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"contentVector\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_TAG\"] = \"{}\"\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "azure_endpoint  = f\"{OpenAiEndPoint}\"\n",
    "api_key = OpenAiKey\n",
    "api_version = OpenAiVersion\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=OpenAiEndPoint, azure_deployment=OpenAiEmbedding, api_key=OpenAiKey, openai_api_type=\"azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDocs=[]\n",
    "csVectorStore: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "    azure_search_key=SearchKey,\n",
    "    index_name=indexName,\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    semantic_configuration_name=\"semanticConfig\",\n",
    ")\n",
    "retriever = csVectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrievedDocs = retriever.get_relevant_documents(query)\n",
    "for doc in retrievedDocs:\n",
    "    rawDocs.append(doc.page_content)\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatDocs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Answer:  Microsoft Fabric is a unified analytics platform that integrates various experiences like Data Engineering, Data Factory, Data Science, and more onto a shared SaaS foundation for easy accessibility and governance within an organization's data ecosystem.\n"
     ]
    }
   ],
   "source": [
    "ragChain = (\n",
    "        {\"context\": retriever | formatDocs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | ollama\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "try:\n",
    "    #modifiedAnswer = ragChainWithSource.invoke(question)['answer']\n",
    "    modifiedAnswer = ragChain.invoke(query)\n",
    "    modifiedAnswer = modifiedAnswer.replace(\"Answer: \", '')\n",
    "    print(\"Modified Answer: \" + modifiedAnswer)\n",
    "except Exception as e:\n",
    "    print(\"Error in RAG Chain: \" + str(e))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
