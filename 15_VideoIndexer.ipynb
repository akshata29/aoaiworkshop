{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from datetime import datetime, timedelta\n",
    "from azure.storage.blob import generate_container_sas\n",
    "from azure.identity import ManagedIdentityCredential, AzureCliCredential, ChainedTokenCredential\n",
    "from Utilities.envVars import *\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "from Utilities.envVars import *\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "llm = AzureChatOpenAI(\n",
    "        azure_endpoint=OpenAiEndPoint,\n",
    "        api_version=OpenAiVersion,\n",
    "        azure_deployment=OpenAiChat16k,\n",
    "        temperature=0.1,\n",
    "        api_key=OpenAiKey,\n",
    "        max_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Function to create the SAS Token.  SAS Token is required to access the Video file that is uploaded to storage account that will be used to send it to Video Indexer for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create Shared Access Signature\n",
    "def getSasUrlForContainer(blobAccount, blobKey, blobContainer, uri: str):\n",
    "    # Generate Shared Access Signature with read permission\n",
    "    sas_token = generate_container_sas(blobAccount,blobContainer, blobKey, permission=\"rw\", expiry=datetime.utcnow() + timedelta(days=31))\n",
    "    return uri + '?' + sas_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create Shared Access Signature\n",
    "def getSasUrl(uri: str):\n",
    "    # Get Azure Blob Storage configuration\n",
    "    blob_account            = VideoIndexerBlobAccount\n",
    "    blob_key                = VideoIndexerBlobKey\n",
    "    blob_container_source   = VideoIndexerBlobContainer\n",
    "\n",
    "    # Generate Shared Access Signature with read permission\n",
    "    sas_token = generate_container_sas(blob_account,blob_container_source, blob_key, permission=\"rw\", expiry=datetime.utcnow() + timedelta(days=31))\n",
    "    return uri + '?' + sas_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper function to Perform the indexing on the Video.  It is 2 step process, with Authentication to get Access token from Management Plane and using that access Token as a part of the Upload URL call that will invoke the Video Upload Functionality to Video Indexer AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccessToken():\n",
    "    resource_id     = VideoIndexerResourceId \n",
    "\n",
    "    # Get access token to AVAM using ManagedIdentity from Azure Function or AzureCLI when executing locally\n",
    "    credential = ChainedTokenCredential(ManagedIdentityCredential(), AzureCliCredential())\n",
    "    access_token = credential.get_token(\"https://management.azure.com\")\n",
    "    url = f\"https://management.azure.com{resource_id}/generateAccessToken?api-version=2021-10-18-preview\"\n",
    "    headers = {\n",
    "        \"Authorization\" : f\"Bearer {access_token.token}\"\n",
    "    }\n",
    "    body = {\n",
    "    \"permissionType\": \"Contributor\",\n",
    "    \"scope\": \"Account\"\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=body)\n",
    "    video_access_token = response.json()['accessToken']\n",
    "\n",
    "    return video_access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Video Indexer to perform video processing\n",
    "def startVideoIndexing(video_name: str, video_url: str, access_token: str):\n",
    "    # Get Video Indexer configuration\n",
    "    endpoint        = VideoIndexerEndPoint\n",
    "    account_id      = VideoIndexerAccountId\n",
    "    location        = VideoIndexerLocation\n",
    "\n",
    "    # Call Video Indexer to start processing the video\n",
    "    video_url = urllib.parse.quote(video_url)\n",
    "    video_name = video_name.split('/')[-1] # extract just video name, remove container and folder path\n",
    "    video_name = urllib.parse.quote(video_name)\n",
    "    #function_url = urllib.parse.quote(function_url)\n",
    "    privacy = \"Private\" # Set visibility for the video [Private, Public]\n",
    "\n",
    "    #upload_video_url = f\"{endpoint}/{location}/Accounts/{account_id}/Videos?accessToken={access_token}&name={video_name}&videoUrl={video_url}&privacy={privacy}&callbackUrl={function_url}&language=auto\"\n",
    "    upload_video_url = f\"{endpoint}/{location}/Accounts/{account_id}/Videos?accessToken={access_token}&name={video_name}&videoUrl={video_url}&privacy={privacy}&language=auto\"\n",
    "    logging.info(upload_video_url)\n",
    "    upload_result = requests.post(upload_video_url)\n",
    "\n",
    "    return upload_result.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Video indexer will take time to process, so we will wait for the video to be processed and then we will get the insights from the Video Indexer.  The key attribute required for next step of the Process is the Video ID.  We will use that to get the insights from the Video Indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - The user uploaded the video and we will process that video and perform indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dataaiopenaistor.blob.core.windows.net/videos/BryanMsft.mp4?se=2024-02-22T03%3A00%3A49Z&sp=rw&sv=2022-11-02&sr=c&sig=/EP5ioCfamGKVuRhcbwvVifPxi%2Bzxc69gx2ctMl99NQ%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accountId': 'dc8a63da-2623-4881-85de-15016f35fa99',\n",
       " 'id': 'b25e60b852',\n",
       " 'partition': None,\n",
       " 'externalId': None,\n",
       " 'metadata': None,\n",
       " 'name': 'BryanMsft',\n",
       " 'description': None,\n",
       " 'created': '2024-01-22T03:00:56.7933333+00:00',\n",
       " 'lastModified': '2024-01-22T03:00:56.7933333+00:00',\n",
       " 'lastIndexed': '2024-01-22T03:00:56.7933333+00:00',\n",
       " 'privacyMode': 'Private',\n",
       " 'userName': 'Ashish Talati',\n",
       " 'isOwned': True,\n",
       " 'isBase': True,\n",
       " 'hasSourceVideoFile': True,\n",
       " 'state': 'Uploaded',\n",
       " 'moderationState': 'OK',\n",
       " 'reviewState': 'None',\n",
       " 'processingProgress': '1%',\n",
       " 'durationInSeconds': 0,\n",
       " 'thumbnailVideoId': 'b25e60b852',\n",
       " 'thumbnailId': '00000000-0000-0000-0000-000000000000',\n",
       " 'searchMatches': [],\n",
       " 'indexingPreset': 'Default',\n",
       " 'streamingPreset': 'Default',\n",
       " 'sourceLanguage': None,\n",
       " 'personModelId': '00000000-0000-0000-0000-000000000000'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://dataaiopenaistor.blob.core.windows.net/videos/BryanMsft.mp4\"\n",
    "sasUrl = getSasUrlForContainer(OpenAiDocStorName, OpenAiDocStorKey, OpenAiVideoContainer, url)\n",
    "print(sasUrl)\n",
    "videoName = \"BryanMsft\"\n",
    "accessToken = getAccessToken()\n",
    "# Call Video Indexer service with Shared Access Signature to index the video\n",
    "videoResult = startVideoIndexing(videoName,sasUrl, accessToken)\n",
    "videoResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videoResult = {'accountId': 'dc8a63da-2623-4881-85de-15016f35fa99',\n",
    "#  'id': '681f932fe2',\n",
    "#  'partition': None,\n",
    "#  'externalId': None,\n",
    "#  'metadata': None,\n",
    "#  'name': 'dataaimediasa',\n",
    "#  'description': None,\n",
    "#  'created': '2024-01-20T17:21:23.6733333+00:00',\n",
    "#  'lastModified': '2024-01-20T17:21:23.6733333+00:00',\n",
    "#  'lastIndexed': '2024-01-20T17:21:23.6733333+00:00',\n",
    "#  'privacyMode': 'Private',\n",
    "#  'userName': 'Ashish Talati',\n",
    "#  'isOwned': True,\n",
    "#  'isBase': True,\n",
    "#  'hasSourceVideoFile': True,\n",
    "#  'state': 'Uploaded',\n",
    "#  'moderationState': 'OK',\n",
    "#  'reviewState': 'None',\n",
    "#  'processingProgress': '1%',\n",
    "#  'durationInSeconds': 0,\n",
    "#  'thumbnailVideoId': '681f932fe2',\n",
    "#  'thumbnailId': '00000000-0000-0000-0000-000000000000',\n",
    "#  'searchMatches': [],\n",
    "#  'indexingPreset': 'Default',\n",
    "#  'streamingPreset': 'Default',\n",
    "#  'sourceLanguage': None,\n",
    "#  'personModelId': '00000000-0000-0000-0000-000000000000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Video Insights from Video Indexer\n",
    "def getIndexStatus(video_id: str, video_access_token: str):\n",
    "    # Get Video Indexer configuration\n",
    "    endpoint        = VideoIndexerEndPoint\n",
    "    account_id      = VideoIndexerAccountId\n",
    "    location        = VideoIndexerLocation\n",
    "\n",
    "    # Create video URL\n",
    "    video_url = f\"{endpoint}/{location}/Accounts/{account_id}/Videos/{video_id}/Index?accessToken={video_access_token}\"\n",
    "\n",
    "    # Get Video Data\n",
    "    search_video = requests.get(video_url)#, headers=headers)\n",
    "    video_data = search_video.json()\n",
    "\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#videoId = videoResult['id']\n",
    "videoId = 'bedc54f751'\n",
    "accessToken = getAccessToken()\n",
    "while True:\n",
    "    indexStatus = getIndexStatus(videoId, accessToken)\n",
    "    if indexStatus['state'] != 'Uploaded' and indexStatus['state'] != 'Processing':\n",
    "        print('Processing Completed')\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(20)\n",
    "        print('Processing...')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Video Insights from Video Indexer\n",
    "def createPromptContent(video_id: str, video_access_token: str):\n",
    "    # Get Video Indexer configuration\n",
    "    endpoint        = VideoIndexerEndPoint\n",
    "    account_id      = VideoIndexerAccountId\n",
    "    location        = VideoIndexerLocation\n",
    "\n",
    "    # Create video URL\n",
    "    video_url = f\"{endpoint}/{location}/Accounts/{account_id}/Videos/{video_id}/PromptContent?accessToken={video_access_token}\"\n",
    "\n",
    "    # Get Video Data\n",
    "    search_video = requests.post(video_url)#, headers=headers)\n",
    "\n",
    "    return search_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Video Insights from Video Indexer\n",
    "def getPromptContent(video_id: str, video_access_token: str):\n",
    "    # Get Video Indexer configuration\n",
    "    endpoint        = VideoIndexerEndPoint\n",
    "    account_id      = VideoIndexerAccountId\n",
    "    location        = VideoIndexerLocation\n",
    "\n",
    "    # Create video URL\n",
    "    video_url = f\"{endpoint}/{location}/Accounts/{account_id}/Videos/{video_id}/PromptContent?accessToken={video_access_token}\"\n",
    "\n",
    "    # Get Video Data\n",
    "    search_video = requests.get(video_url)#, headers=headers)\n",
    "    video_data = search_video.json()\n",
    "\n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2 - Create Prompt Content so that we can use that for our RAG pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessToken = getAccessToken()\n",
    "promptContentResp = createPromptContent(videoId, accessToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'partition': None, 'name': 'ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure', 'sections': [{'id': 0, 'start': '0:00:01.3013', 'end': '0:00:19.052367', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] book, chair, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, display, software\\n[OCR] OpenAI, GPT-3, Codex, DALL-E 2, Cognitive Services, Azure OpenAl Studio - Preview, Models, Try it out, Create customized model, Management, Training data, Validation data, Advanced options, Review and train, Cancel, Base model, Base model type, Next, O Advanced options, File Management, ada, babbage, curie, Nex, Let's start with your content, Back, Done, grass field with trees, Transformer Decoders, // . a. function. that . computes . the . sum . of . squares. of . numbers . in . an . array, Advanced code generation, mfu tvn > 1<, uhwxuq vxp, Playground, Deployments, Parameters, Examples, Top probabilities, A more descriptive word for happy is joyful, A word to describe going fast into a turn is, Swerving, Regenerate, Undo, Zero Shot, Concisely answer questions about this paragraph:, Azure Cognitive Services are cloud-based artificial intelligence (Al) services that help developers build cognitive, intelligence into applications without having direct Al or data science skills or knowledge. They are available through, REST APIs and client library SDKs in popular development languages. Azure Cognitive Services enables developers to, easily add cognitive features into their applications with cognitive solutions that can see, hear, speak, and analyze., Q: Are Cognitive Services cloud-based?, A: Yes, Q: Do I need to have direct Al or data science skills to use Cognitive Service?, A: No, Q: Are Cognitive Services available through APIs?, Few Shot, Cognitive Services | Azure O, penAl, ound, ments, Frequency penalty, Presence penalty, Pre-response text, Post-response text\\n[Transcript] Up next, we take an inside look at Azure's Open AI service that lets you leverage large scale generative AI models based in Azure that have a deep understanding of language and code as you build new applications.\"}, {'id': 1, 'start': '0:00:19.052367', 'end': '0:00:24.057367', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Visual labels] software\\n[OCR] 0.50, 100, 0.5, Cognitive Services | Azure OpenAl Studio - Preview, Pablo Castro, pablooai (South Central US, SO), PC, Azure OpenAl Studio > Playground, Temperature, davinci2, Summarize Text, Code View, Max length (tokens), Answer each question by extracting key facts from as many statements as possible., Statements:, - The West Census Region, where 8.9% of single-family homes (2020 data) had small-scale solar generation, had the highest percentage of, Stop sequences, homes with small-scale solar generation, mostly in California., - The Northeast Census Region had the second-largest percentage of homes with small-scale solar generation at 4.7% (2020 data), - U.S. shipments of solar photovoltaic (PV) modules (solar panels) rose to a record electricity-generating capacity of 28.8 million peak, kilowatts (kW) in 2021, from 21.8 million peak kW in 2020, based on data from our Annual Photovoltaic Module Shipments Report., - The United States added 13.2 gigawatts (GW) of utility-scale solar capacity in 2021, an annual record and 25% more than the 10.6 GW, added in 2020, according to our Annual Electric Generator Report., - Small-scale solar capacity installations in the United States increased by 5.4 GW in 2021, up 23% from 2020 (4.4 GW). Most of the small, scale solar capacity added in 2021 was installed on homes, Question:, Percentage of homes with solar in top regions, Best of, Answer:, Enter text, Tokens: 279, The West Census Region had the highest percentage of homes with small, The West Census Region had the highest percentage of homes with small-scale solar generation at 8.9%. The Northeast Census Region had, the second-largest percentage of homes with small-scale solar generation at 4.7%.'}, {'id': 2, 'start': '0:00:24.057367', 'end': '0:00:28.24', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Visual labels] software, electronics\\n[OCR] 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, public SearchService(string serviceName, string indexName), _client= new SearchClient(new Uri($\"https://{serviceName}.search.windows.net\"), indexName, new DefaultAzureCred, _openai = new OpenAIClient(\"pablooai\", \"davinci2\");, 1 reference, public async Task<SearchResults> Search(string query), atements, var options = new SearchOptions { Size = 5, QueryType = SearchQueryType. Semantic, QueryLanguage = QueryLanguage, var response = await _client. SearchAsync<SearchDocument>(query, options);, var results = response. Value.GetResults().Select(r => new SearchResult(r. Document.metadata_storage_name, r. Docum, s (2020 da, string? summary = null;, if (results. Any()), els) rose, sed on da, solar cap, var promptReplacements = string. Join(\"\\\\n\", results. Select(d => \"-\" + d. Content));, port., ncreased, summary = await _openai.GetCompletion(\"crossdoc\", new Dictionary<string, string>, {\"SEARCHRESULTS\", promptReplacements }, s with sm, ar generat, return new SearchResults(results, summary);, record SearchDocument (string? metadata_storage_name, string content);, 2 references, public record SearchResult (string? Title, string Content);, 100 %, No issues found\\n[Transcript] Now we\\'ll unpack the core concepts for interacting with models using prompts and demonstrate how you can use Azure\\'s Open AI Studio to experiment with and test your models before bringing them into your code to deliver differentiated app experiences.'}, {'id': 3, 'start': '0:00:28.24', 'end': '0:00:32.298933', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] laptop\\n[OCR] Microsoft Azure, Add role assignment, Role, Members Review + assign, Cognitive Services OpenAl Contributor, Full access including the ability to fine-tune, deploy and generate text, Cognitive Services OpenAl User, Ability to view files, models, deployments. Readers can't make any changes. They can inference, Firewalls and virtual networks, Private endpoint connections, Overview, Activity log, Diagnose and solve problems, Allow access from, All networks, Selected Networks and Private Endpoints, Disabled, Resource Management, Configure network security for your cognitive service. Learn more, luding the ability to fine-tune, deploy and gen, Keys and Endpoint, Virtual networks, Model deployments, files, models, deployments. Readers can't mal, Secure your cognitive service with virtual networks. + Add existing virtual network, Pricing tier, Subnet, Networking, Identity, Firewall, Add IP ranges to allow access from the internet or your on-premises networks., Address range, Tags, Secure your cognitive service with virtual networks. + Add existing virtual network + Add new virtual network, IP address or CIDR\\n[Transcript] All with Azure's Enterprise Grade Security for your Apps Foundation and joining me today for an in depth look at Azure's Open AI Services.\"}, {'id': 4, 'start': '0:00:32.298933', 'end': '0:01:21.3813', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] bench, book, cup, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, man\\n[OCR] Pablo Castro, Microsoft Mechanics, Pa, Distinguished, Microsoft N, Jeremy Chapman, Director Microsoft 365, nics\\n[Transcript] Pablo Castro, Distinguished Engineer on Azure's AI team.\\nWelcome back to Mechanics.\\nIt's great to be back and in the studio this time.\\nThanks so much for joining us back on mechanics.\\nSo Open AI has really started to gain momentum over the past couple of years and it provides this really large, extremely rich set of base AI models that can accelerate the process of constructing natural language powered experiences.\\nThey can, for example, greatly improve how you interact with your apps and extract knowledge from data.\\nNow this is an exciting area that Microsoft has invested a lot in.\\nSo what are we solving for then, with Azure's Open AI service?\\nThere's definitely a ton of potential, and we see lots of applications powered by Azure Open AI today.\\nFrom a developer's perspective, the thing that excites me the most is how it helps build new experiences with several cutting edge models like GPT 3A large language model that generates content based on natural language input codecs which can translate natural language instructions directly into code and that lead to a new model that generates realistic images and art from natural language descriptions.\"}, {'id': 5, 'start': '0:01:21.3813', 'end': '0:01:42.936167', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] book, dining table, laptop\\n[Visual labels] display\\n[OCR] 95, 96, OpenAI, GPT-3, Codex, DALL-E 2, Let's start with your content, Back, Done, Transformer Decoders, Advanced code generation, mfu tvn > 1<, uhwxuq vxp, Natural language input, should, know, what, asking, it, to, do, Human-like language generation, sm, agree, hqt *ngv k ? 2= k > pwodgtu ngpivj k -- + }, grass field with trees|\\n[Transcript] And when we combine Open AI with Azure into the Azure Open AI service, you get the core building blocks you need for production grade applications.\"}, {'id': 6, 'start': '0:01:43.136367', 'end': '0:02:02.44', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] chair, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, display, software, electronics\\n[OCR] OpenAI, GPT-3, Codex, DALL-E 2, Cognitive Services, Azure OpenAl Studio - Preview, Models, Try it out, Create customized model, Management, Training data, Validation data, Review and train, Cancel, Base model, Base model type, O Advanced options, File Management, ada, babbage, curie, Nex, Playground, Deployments, Parameters, Examples, Regenerate, Undo, Presence penalty, Temperature, Summarize Text, Microsoft Mechanics, O Review and train, Get started with Azure OpenAI, Explore examples for prompt completion, Classify Text, Natural Language to SQL, Generate New Product Names, Experiment with prompt, Customize a model with fine-tuning, Manage deployments in your, Manage performance results, completions, resource, Azure OpenAI Studio - Preview, Product description:, Seed words:, Product description: A home milkshake maker, Tokens: 9, Seed words: fast, healthy, compact., Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\\n[Transcript] From how we host Open AI at scale for you in Azure to enterprise level security, we're also giving you a great environment to try things out and iterate.\\nThe Azure Open AI Studio let's you experiment and test your ideas with Open AI before bringing them into your code.\"}, {'id': 7, 'start': '0:02:02.44', 'end': '0:02:08.84', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] laptop\\n[Visual labels] computer, software\\n[OCR] 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, Examples, Regenerate, Undo, Tokens: 9, I Studio - Preview, EXPLORER, scription: A home milkshake maker, HttpRequest httpreq = HttpRequest.newBuilder(), : fast, healthy, compact., .header(\"Content-Type\", \"application/json\"), .header(\"api-key\", this.apikey), nes: HomeShaker, Fit Shaker, QuickShake, Shake, .build();, To, > TIMELINE, OPEN EDITORS, METADATA\\n[Transcript] And when you\\'re done experimenting and you know what you want, you can call the service from your code just like any other REST API.'}, {'id': 8, 'start': '0:02:08.84', 'end': '0:02:18.505033', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] laptop\\n[Visual labels] computer, software, multimedia\\n[OCR] 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 31, 32, 34, 35, 36, 11, Microsoft Azure, Add role assignment, Role, Full access including the ability to fine-tune, deploy and generate text, Cognitive Services OpenAl User, Ability to view files, models, deployments. Readers can\\'t make any changes. They can inference, Firewalls and virtual networks, Private endpoint connections, Overview, Activity log, Diagnose and solve problems, Allow access from, All networks, Selected Networks and Private Endpoints, Disabled, Resource Management, Configure network security for your cognitive service. Learn more, Keys and Endpoint, Virtual networks, Model deployments, Pricing tier, Subnet, Networking, Identity, Firewall, Add IP ranges to allow access from the internet or your on-premises networks., Address range, Tags, Secure your cognitive service with virtual networks. + Add existing virtual network + Add new virtual network, IP address or CIDR, HttpRequest httpreq = HttpRequest.newBuilder(), .header(\"Content-Type\", \"application/json\"), .header(\"api-key\", this.apikey), .build();, _client = new HttpClient(new TokenCredentialsHttpHandler(new DeafultAzureCredential(), \"https\". cognitiveservices.azure.com\"));, _url = new Uri($\"https://{name}.openai.azure.com/openai/deployments/{deploymentId}/completions?api-version=2022-06-01-preview\");, C#, Role Members, Review + assign, BuiltInRole, Members, connections, Connection name, Connection state, Private endpoint, Endpoints, earn more\\n[Transcript] And security is built in with capabilities spanning strong authentication, role based access control, and the ability to configure virtual networks private endpoints as you would for any Azure service.'}, {'id': 9, 'start': '0:02:18.505033', 'end': '0:02:54.541033', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] bench, cup, dining table, laptop\\n[Visual labels] lamp, human face, person, clothing, laptop, man, speech\\n[OCR] Microsoft Azure, Firewalls and virtual networks, Private endpoint connections, Overview, Activity log, Diagnose and solve problems, Keys and Endpoint, Model deployments, Pricing tier, Networking, Tags, Connection name, Connection state, Private endpoint, Access control (IAM), urce Management, Tell me top curse |, Tell me top curse words in America for 2022.|, The generated response does now follow, our content guidelines., Microsoft Mechanics, nics, aka.ms/Alprinciples, Microsoft M, Customers, Active, Cancelled, Expired\\n[Transcript] We've also incorporated tools for supporting responsible AI with content filtering in the service.\\nFor example, if your input includes inappropriate content, the Azure Open AI service would catch it, and this works for any generated output too.\\nYou can use this to detect and mitigate harmful use following responsible AI principles, which you can find more about at AKA Ms.\\nAI Principles.\\nSo can you give a few examples of how we're using Open AI and apps and services today?\\nSure.\\nOpen AI actually powers a number of experiences at Microsoft today.\\nFor example, Copilot in the Power platform enables users to author power FX commands or even reach automation flows.\"}, {'id': 10, 'start': '0:02:54.541033', 'end': '0:03:09.756233', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] cup, dining table, kite, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, electronics\\n[OCR] 13, Let\\'s start with your content, Microsoft Mechanics, Customers, Active, Cancelled, Expired, Show me the Customer, Show me the Customers from the U.S., whose subscription is expired, Question, Answers, Customers where Address 1:, Country/Region = \"U.S.\", And \\'Subscription\\' = \"Expired\"), Filter(Customers, \\'Address 1:, Search, t (preview), s words, Describe your flow in everyday words, Once a day, get all invoice rows from Dataverse. For those that have the status rejected-incomplete, send an email, and actions-like, n an Outlook email,, Generating suggestions for you ..., Microsoft Teams., Suggested flow:, Recurrence, Multiple actions, This is not what I want, Designer, PREVIEW, + New design, Share, Template, We use your content and Al to, Add text, generate designs for you, My medi, Your design headline, Visuals, Add your own images, Text, Let your travel adventures begin, From this device, From your phone, My media, Styles, Generate an image using a description, A bonsai tree as a watercolor, Skip and start from a blank canvas or recent designs, Share photos from your last adventure, med\\n[Transcript] Another great example is the design it Up we recently announced which uses the Daly 2 model to interpret your text descriptions to generate images and even artwork.'}, {'id': 11, 'start': '0:03:09.756233', 'end': '0:03:31', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] clock, cup, dining table, kite, laptop, sports ball\\n[Visual labels] software\\n[OCR] 52%, DALL-E 2, Let\\'s start with your content, Back, Done, Designer, PREVIEW, + New design, Share, Template, We use your content and Al to, Add text, generate designs for you, My medi, Your design headline, Visuals, Add your own images, Text, Let your travel adventures begin, From this device, From your phone, My media, Styles, Generate an image using a description, A bonsai tree as a watercolor, Skip and start from a blank canvas or recent designs, Share photos from your last adventure, Microsoft Designer, our travel adventures be, designer.microsoft.com, DALL, Generating designs ..., II, + Add a subheading, What image do you want to create?, Try an example, Color v, DALL-, ADD YOUR TEXT HERE, Customize design, E 2, grass field with-trees, Tip, Try adding your favorite illustration style, \"A double helix as comic book art, no, ... field with trees, in the style of post-impressionism, well preserved, ... field with trees, in the style of fauvism, detailed, ... field with trees, from above, in the style of saul leiter, tender, suns ..., ... field with trees, on canvas, in the style of georges seurat, epic, wo ..., ... field with trees, close up, in the style of impressionist, emo, oversa ..., File uploaded successfully\\n[Transcript] Let me show you I\\'m here in the designer preview.\\nYou can find this at designer.microsoft.com.\\nI\\'m going to add a design headline, say Daly Two and here I can generate an image from natural language.\\nI\\'ll type grass filled with trees and you\\'ll see designer gives me some nice recommendations to augment it.\\nBut I\\'ll keep things simple and just add cows.'}, {'id': 12, 'start': '0:03:31', 'end': '0:03:42.76', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] sports ball\\n[OCR] DALL-E 2, Let\\'s start with your content, Back, Done, Generating designs ..., DALL-, ADD YOUR TEXT HERE, Customize design, E 2, Tip, File uploaded successfully, grass field with trees and cows, DALL E 2, Try adding a time period to get a different, result, \"A gorilla drinking coffee from the, 1800\\'s\", ... trees and cows, in the style of pixel art, vivid, ... trees and cows, viewed from behind, in the style of pencil, ... trees and cows, as chinese watercolor, well preserved, ... trees and cows, viewed from behind, in the style of pencil sketch, ..., ... trees and cows, lens flare, in the\\\\\"yle of chinese watercolor, retro ..., ... trees and cows, panoramic, in the style of by Van Gogh, plain bac ..., Size v, grass field with trees and cows as a Iartoon, ... as a cartoon, as pixel art, look at that detail, ... as a cartoon, as post-impressionism, contest winner, E2, ... as a cartoon, extreme close-up view, in the style of pierre auguste ..., ... as a cartoon, aerial view, in the style of georges seurat, wormhole ..., ... as a cartoon, from above, in the style of pixel art, serene, zbrush, ..., Add your text, here, DALL-E, ADD YOUR, TEXT HERE, Id with trees and cows as a blocky video gamel, ... blocky video game, as vincent van gogh, ... blocky video game, as fauvism, detailed, ... blocky video game, midview, in the style of chinese watercolor, pe ..., ... blocky video game, panoramic, in the style of pointillism, grainy, t ..., ... blocky video game, through a periscope, in the style of saul leiter, ..., grass field with trees and cows as a blocky vi .. I\\n[Transcript] Now if I want to change the look, I can add as a cartoon and that\\'s starting to look closer what I want, but I\\'ll change the cartoon to a blocky video game and that\\'s exactly what I wanted.'}, {'id': 13, 'start': '0:03:42.76', 'end': '0:04:04.711133', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] clock, cup, dining table, laptop, sports ball\\n[OCR] DALL-E 2, Let's start with your content, Back, Done, Designer, PREVIEW, + New design, Share, Add text, Visuals, Add your own images, Text, My media, Styles, Skip and start from a blank canvas or recent designs, + Add a subheading, Customize design, DALL E 2, DALL-E, grass field with trees and cows as a blocky vi .. I, Add your, text here, Add your text here, Templates, Design ideas, Design 4, Zoom to 150%, Zoom to 100%, DAI, Zoom to fit, Zoom to fill\\n[Transcript] Now I just need to choose a design.\\nI'll pick this one and you'll see our progression of images.\\nAnd just to see a little bit more detail, I'll select the second one so I can zoom into the middle of the screen and now you can see more of a close up view.\\nThese were all generated on demand.\\nDesigner interpreted what I typed, and it created new images each time I iterated.\\nSo these hubs aren't just putting an app frame around the API open.\"}, {'id': 14, 'start': '0:04:04.711133', 'end': '0:04:37.410467', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] bench, clock, cup, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, speech\\n[OCR] Microsoft Mechanics, nics, Microsoft Me\\n[Transcript] AI models like this can help you build more dynamic, interactive and differentiated experiences.\\nAnd that's really an important point because we're not training models in the traditional AI machine learning sense for just interacting with them using text.\\nSo can you explain a bit more behind that concept?\\nYes, these types of models use text for interaction.\\nThey have been trained on huge amounts of written text and can use their accumulated knowledge to perform a number of tasks directly.\\nWe call the text input to a model a prompt.\"}, {'id': 15, 'start': '0:04:37.410467', 'end': '0:05:22.44', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning\\n[Detected objects] dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, display, software\\n[OCR] Cognitive Services, Azure OpenAl Studio - Preview, Models, Try it out, Management, File Management, Playground, Deployments, Parameters, Examples, Top probabilities, A more descriptive word for happy is joyful, A word to describe going fast into a turn is, Swerving, Regenerate, Undo, Zero Shot, Concisely answer questions about this paragraph:, Azure Cognitive Services are cloud-based artificial intelligence (Al) services that help developers build cognitive, intelligence into applications without having direct Al or data science skills or knowledge. They are available through, REST APIs and client library SDKs in popular development languages. Azure Cognitive Services enables developers to, easily add cognitive features into their applications with cognitive solutions that can see, hear, speak, and analyze., Q: Are Cognitive Services cloud-based?, A: Yes, Q: Do I need to have direct Al or data science skills to use Cognitive Service?, A: No, Q: Are Cognitive Services available through APIs?, Few Shot, Presence penalty, Microsoft Mechanics, Azure OpenAI Studio - Preview, A more descril, A more descriptive word for happy is, A word to descl, Can you restate \"winter is coming\" in a more, descriptive way?, Winter is coming soon. The days are getting, shorter and the temperature is getting colder., A more descriptive word for happy is jjoyfu, Q: Are Cognitive Services availab\\n[Transcript] The simplest form of this interaction pattern is to give a model a string of text or a prompt and ask it to complete it.\\nIt could be a partial statement, a question, or anything you want the model to add to.\\nIn cases where the model has enough context to generate good outputs, that might be all you need.\\nThis is sometimes called zero shot because the model responds with no additional input or training.\\nThat said, sometimes you do want to guide the model a bit more.\\nYou can do this by giving it examples of how you want it to behave, such as a question, answer, spare, and finishing the prompt with the one you want it to complete.\\nThis still requires no actual updates of the weights in the model.\\nThe examples are learned on the spot, so this is often referred to as few shot learning.'}, {'id': 16, 'start': '0:05:22.44', 'end': '0:05:41.7414', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Beginning-Middle\\n[Detected objects] bench, dining table, laptop\\n[Visual labels] software, human face, person, clothing, laptop, computer\\n[OCR] Cognitive Services, Azure OpenAl Studio - Preview, Models, Try it out, Management, File Management, Playground, Deployments, Parameters, Examples, Top probabilities, Regenerate, Undo, Concisely answer questions about this paragraph:, Azure Cognitive Services are cloud-based artificial intelligence (Al) services that help developers build cognitive, intelligence into applications without having direct Al or data science skills or knowledge. They are available through, REST APIs and client library SDKs in popular development languages. Azure Cognitive Services enables developers to, easily add cognitive features into their applications with cognitive solutions that can see, hear, speak, and analyze., Q: Are Cognitive Services cloud-based?, A: Yes, Q: Do I need to have direct Al or data science skills to use Cognitive Service?, A: No, Q: Are Cognitive Services available through APIs?, Few Shot, Microsoft Mechanics, Microsoft M\\n[Transcript] So how is it then to integrate Open AI as part of everyday apps?\\nOh, it's super straightforward.\\nOpen AI can be used to create many new experiences that were impossible before.\\nMy favorite example is how it can transform the entire surface area of an app into something you can converse with using natural language.\\nLet's use Minecraft the game as an example, and if you played Minecraft, you might be familiar with the text commands you can make during gameplay to manipulate the world around you.\"}, {'id': 17, 'start': '0:05:41.7414', 'end': '0:06:42.0016', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] dining table, laptop, remote\\n[Visual labels] sky, plant, tree, human face, person, clothing, laptop, computer, pc game\\n[OCR] /help, /tp -> teleport, /datapack (enableldisablellist), /stopsound <targets>, /tm -> teammsg, /forge (tpsltr acklentitylgener ateldimensionslsetdimensionlmods), /attribute <target> <attribute> (getlbaselmodifier), Microsoft Mechanics, Codex, summon creeper, on jure: make green monster, Summoned new Creeper\\n[Transcript] The challenge is that there is a lot of commands.\\nI'll run slash help to see all the possible commands here.\\nYou can see there are dozens of different commands that you would need to memorize, and this is in addition to the more than 1000 different materials and lots of different entities.\\nOne command is slash summon, which is used to spawn a new entity into the world from cows to ship and then slash field to build structures using blocks and so on.\\nThe nice thing about Minecraft is that it's extensible so you can write custom mods.\\nWhat I'm going to show you is how we created a mod called Conjure which acts as a plug in so that when you write a natural language message, it'll pass that command to the Open AI service in Azure using the codecs model.\\nThis interprets the intent and finally passes back the generated command that Minecraft can natively understand.\\nSo let's try this out for real.\\nIn Minecraft.\"}, {'id': 18, 'start': '0:06:42.0016', 'end': '0:07:51.003867', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] cup, dining table, laptop\\n[Visual labels] sky, plant, tree, pc game, human face, person, clothing, laptop, computer\\n[OCR] Summoned new Cou, con jure: make an, Con jure: make an animal that goes, Successfully filled 18 blocks, Microsoft Mechanics\\n[Transcript] I'll ask it to make a cow, and there's our cow.\\nIt converted my request to create one.\\nThat was pretty easy though, because it just converted my command to summon cow.\\nNow let's be less precise and type.\\nMake an animal that goes move and open eye who knows about the world.\\nAnd it was able to translate my intent using the sound that cow makes to tell Minecraft to summon a cow.\\nAnd beyond the summon command, it can also interpret other requests.\\nI'll ask it to make a transparent wall and there it is, even though transparent isn't part of a block name.\\nCodex knows that glass is transparent and it connected the dots for us.\\nIt executed the field command and as you can see it even came up with relative coordinates.\\nSo what we just saw is how we can use Open AI not only to let you interact using natural language with an application, but also how the model's knowledge of the world in general can really enrich the experience.\\nThis is really a great example of the possibilities for interacting with apps using natural language.\\nNow, aside from building the mod itself, was there anything else you had to do?\\nRemember that we discussed before the importance of prompts?\\nSo for this I needed a very simple prompt.\"}, {'id': 19, 'start': '0:07:51.003867', 'end': '0:08:27.340167', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] bowl, dining table, laptop\\n[OCR] 100%, ConjurePrompt.txt - Notepad, File, Edit, View, Translate English requests into Minecraft commands., build a 2x2 wall, build a 3x3 glass wall, build a hollow box, /fill ~ ~ ~ ~5 ~5 ~5 oak_planks hollow, give me a sword, /give @s stone_sword, give me a powerful sword, /give @s netherite_sword, make a fish, /summon tropical_fish, Ln 1, Col 1, Windows (CRLF), UTF-8, /summon tropical fish\\n[Transcript] This asks codecs to translate natural language into Minecraft commands.\\nAt the top I made a statement in English about my intent and then I gave it a few examples such as building walls using relative coordinates and summoning animals.\\nThis is like a few shot scenario.\\nYou'll see that even though I defined that make a fish in my case translates to slash summon tropical fish, it could take that tiny piece of information and generalize to all similar cases, even if the details vary a lot.\\nSo these prompts apply to all the animals in Minecraft, even the sounds they make or other indirect ways you might use to refer to them.\"}, {'id': 20, 'start': '0:08:27.340167', 'end': '0:08:46.559367', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] bench, cup, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, man, speech, spokesperson\\n[OCR] Microsoft Mechanics, nics, Microsoft M\\n[Transcript] This really shows the power of few shot learning with the Open AI service.\\nThis whole thing was just a few hours of work with my kids and.\\nIt really shows how just a little bit of guidance actually gets you the outcomes that you want using your prompt.\\nExactly, and these models are already in use out there.'}, {'id': 21, 'start': '0:08:46.559367', 'end': '0:08:48.72', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, software\\n[OCR] 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, EXPLORER, HttpRequest httpreq = HttpRequest.newBuilder(), .header(\"Content-Type\", \"application/json\"), .header(\"api-key\", this.apikey), .build();, UTF-8, File Edit Selection View Go Run Terminal Help, . ConjureMod.java - conjure - Visual Studio Code, Configure Java Runtime, V OPEN EDITORS 1 unsaved, src > main > java > com > conjuremod > J ConjureMod.java, private String translateCommand(String msg) throws IOException, InterruptedException {, J ConjureMod.java src\\\\main\\\\java\\\\com\\\\conj ..., LOGGER.info(\"Conjure - prompt - \" + msg);, CONJURE, .gradle, var req = new OpenAIRequest();, req.prompt = msg;, req.stop = \"\\\\n\";, req.temperature = 0.1;, req.max_tokens = 128;, String json = gson.toJson(req);, ConjureMod.java, .gitignore, build.gradle, gradlew, gradlew.bat, return null;, static class OpenAIRequest {, public String prompt;, public String stop;, public double temperature;, public int max_tokens;, static class OpenAIResponseChoice {, public String text;, static class OpenAIResponse {, public OpenAIResponseChoice[] choices;, TIMELINE, EV3DEV DEVICE BROWSER, Ln 128, Col 22, Spaces: 4\\n[Transcript] For example, Codex powers the GitHub Copilot capability that helps developers write code faster.\\nLet me show you.'}, {'id': 22, 'start': '0:08:48.72', 'end': '0:09:11.04', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[OCR] 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, EXPLORER, HttpRequest httpreq = HttpRequest.newBuilder(), .header(\"Content-Type\", \"application/json\"), .header(\"api-key\", this.apikey), .build();, Configure Java Runtime, V OPEN EDITORS 1 unsaved, src > main > java > com > conjuremod > J ConjureMod.java, private String translateCommand(String msg) throws IOException, InterruptedException {, J ConjureMod.java src\\\\main\\\\java\\\\com\\\\conj ..., LOGGER.info(\"Conjure - prompt - \" + msg);, CONJURE, .gradle, var req = new OpenAIRequest();, req.prompt = msg;, req.stop = \"\\\\n\";, req.temperature = 0.1;, req.max_tokens = 128;, String json = gson.toJson(req);, ConjureMod.java, .gitignore, build.gradle, gradlew.bat, return null;, static class OpenAIRequest {, public String prompt;, public String stop;, public double temperature;, public int max_tokens;, static class OpenAIResponseChoice {, public String text;, static class OpenAIResponse {, public OpenAIResponseChoice[] choices;, TIMELINE, EV3DEV DEVICE BROWSER, E gradlew, JAVA PROJECTS, RS 1 unsaved, figure Java Runtime, eMod.java, es, dle, 1 unsaved, re Java Runtime, conjuremod, od.java\\n[Transcript] Here\\'s a portion of the code behind my Minecraft mode in Visual Studio Code, and I have the GitHub Copilot extension enabled.\\nThe model is written in Java, and this is the part that takes the text that the player wrote and sends it to Open AI.\\nSo here I have the HTTP request that calls the Open AI service in Azure.\\nWhat\\'s missing is actually sending the request and then processing the response.'}, {'id': 23, 'start': '0:09:11.04', 'end': '0:09:28.96', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[OCR] 13, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, HttpRequest httpreq = HttpRequest.newBuilder(), .header(\"Content-Type\", \"application/json\"), .header(\"api-key\", this.apikey), .build();, src > main > java > com > conjuremod > J ConjureMod.java, private String translateCommand(String msg) throws IOException, InterruptedException {, J ConjureMod.java src\\\\main\\\\java\\\\com\\\\conj ..., LOGGER.info(\"Conjure - prompt - \" + msg);, var req = new OpenAIRequest();, req.prompt = msg;, req.stop = \"\\\\n\";, req.temperature = 0.1;, req.max_tokens = 128;, String json = gson.toJson(req);, return null;, static class OpenAIRequest {, public String prompt;, public String stop;, public double temperature;, public int max_tokens;, static class OpenAIResponseChoice {, public String text;, static class OpenAIResponse {, public OpenAIResponseChoice[] choices;, 1 unsaved, re Java Runtime, conjuremod, od.java, untime, od, HttpResponse<String> response = http. send(httpreq, BodyHandlers.ofString());, LOGGER.info(\"Conjure - response - \" + response.body());, var resp = gson.fromJson(response.body(), OpenAIResponse.class);, if (resp.choices != null && resp.choices.length > 0) {\\n[Transcript] But I can use Copilot to write that code here.\\nCopilot is predicting this entire line of code.\\nI just need to hit tab and I can keep going and just by hitting the tab key a few times, Copilot wrote all the code we needed from logging to parsing the Jason response and producing a result.'}, {'id': 24, 'start': '0:09:28.96', 'end': '0:09:57.730467', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] bench, cup, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer\\n[OCR] 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, HttpRequest httpreq = HttpRequest.newBuilder(), .header(\"Content-Type\", \"application/json\"), .header(\"api-key\", this.apikey), Microsoft M, var req = new OpenAIRequest();, req.prompt = msg;, req.stop = \"\\\\n\";, req.temperature = 0.1;, req.max_tokens = 128;, String json = gson.toJson(req);, return null;, static class OpenAIRequest {, public String prompt;, public String stop;, public double temperature;, public int max_tokens;, HttpResponse<String> response = http. send(httpreq, BodyHandlers.ofString());, LOGGER.info(\"Conjure - response - \" + response.body());, var resp = gson.fromJson(response.body(), OpenAIResponse.class);, if (resp.choices != null && resp.choices.length > 0) {, CC\\n[Transcript] And I gotta say, that\\'s super impressive just using the tab key in that case.\\nSo can you show us how you might put all this to work with an app that you might build from scratch?\\nIf you\\'re building a custom app, you just need a few lines of code to call the Open AI service.\\nLet\\'s take the example of a green energy company.\\nWe need to keep up to date on the latest trends and insights, which requires us to build an understanding across lots of unstructured data sitting in documents from various sources.\\nI\\'ll explain how we\\'ve architected our app first and then I\\'ll show you how it works and what we did.'}, {'id': 25, 'start': '0:09:57.730467', 'end': '0:10:31.764467', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] cup, dining table, laptop\\n[Visual labels] computer\\n[OCR] 20, 2017, 201, 2018, GPT-3, es, Azure OpenAl + Cognitive Search, Which years had highest e, Which years had highest expenditure for Wind compared t|, Al + Cognitive Search, ars had highest expenditure for Wind compared to Solar Power?, Azure Cognitive Search, Which years had highest expenditure for Wind compared to Solar Power?, Relecloud, Green Energy, ANNUAL REPORT, rgy, ergy, Futures, REPORT, er, Years 2017, 2021 and 2019 had the highest expenditure, based on increased government demand and the high cost off mairdenace, for Wind Turbines in the deep sea. Expenditure started to increase in 2017., Relecloud Green Energy for 2017, infrastructure which went up 20% YoY to 60%. While solar power decreased signiffcatly, and is 5x that of Solar power., Cross document summarization\\n[Transcript] In this example, the user can ask the app a question using natural language.\\nThe app then calls Azure Cognitive Search to discover text based documents related to that question.\\nThis ranks the documents and returns the top candidates.\\nBut to answer some types of questions, we'll need data across multiple of these top documents.\\nSo it passes the top candidates to the Azure Open AI service, which reads and understands these top documents, leveraging the GPT 3 model to generate an answer, which is then passed back to our application using information spread across these relevant documents.\\nSo let me show you this in action.\"}, {'id': 26, 'start': '0:10:31.764467', 'end': '0:10:38.671367', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle\\n[Detected objects] cup, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop\\n[OCR] Microsoft Mechanics, Search, Azure OpenAl + Cognitive Search, Search ...'}, {'id': 27, 'start': '0:10:38.671367', 'end': '0:12:08.761367', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] Middle-End\\n[Detected objects] cup, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, display, software, electronics, speech\\n[OCR] 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 11, 22%, 1/5, 3/, 3/5, 2/10, 3/10, 10, public SearchService(string serviceName, string indexName), _openai = new OpenAIClient(\"pablooai\", \"davinci2\");, 1 reference, public async Task<SearchResults> Search(string query), var response = await _client. SearchAsync<SearchDocument>(query, options);, string? summary = null;, if (results. Any()), {\"SEARCHRESULTS\", promptReplacements }, return new SearchResults(results, summary);, record SearchDocument (string? metadata_storage_name, string content);, public record SearchResult (string? Title, string Content);, 100 %, No issues found, Microsoft Mechanics, File, Edit, View, Renewable sources expected and actual in 2022 X, EIA expects renewables to account for 22% of U.S. electricity generation in 2022, and this is borne out by the data in the first half of the year with 24% of U.S. electricity, generation coming from renewable sources., · EIA expects renewables to account for 22% of U.S. electricity generation in 2022.pdf, Skip to sub-navigation Today in Energy August 16, 2022 EIA expects renewables to account for 22% of U.S. electricity generation in 2022 10% 12% 12% 12% 13% 13% 15% 17%'}, {'id': 28, 'start': '0:12:08.761367', 'end': '0:12:09.1284', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer\\n[Transcript] I'll start in the browser and you're looking at the simple app, but the real beauty is in the power of information it can bring to you.\\nI'll do a quick search for renewable sources, expected and actual for 2022 and it finds a few relevant documents each containing parts of the answer and at the top you'll see the generated answer that grabs elements from multiple documents like 22% from the first doc and first half of 2022 from the second doc.\\nNotice there is 1/3 unrelated doc that was not included in the summary because GPT 3 realized it was not related to the question.\\nNow I'll show you how easy it is to call Open AI and generate the cross document answer which you saw.\\nI'm in Visual Studio and here are a few lines of code that do all of the work.\\nFirst, I take the user query and send it to Azure Cognitive Search.\\nThen I take the results from Search, which are multiple documents, and I construct a prompt from them using a template.\\nFinally, I send this prompt to Open AI to generate the summary answer.\\nSo with very little code, we could leverage Open AI, and this same process could be used to very quickly solve other problems like entity extraction or summarization that otherwise would have required you to pick, train, and deploy custom machine learning models.\\nHere you more or less just build a prompt and you're done.\\nRight.\\nI can really see how useful this would be across any number of research based activities.\\nThat said though, in this case you actually knew what to do, so if someone's watching and needed to start from scratch, where would you even begin?\\nThe Azure Open AI Studio is a great place to start.\\nIt lets you experiment with prompts before you build them into your apps.\\nTo use the Open AI Studio, first you'll need an Azure subscription.\"}, {'id': 29, 'start': '0:12:09.1284', 'end': '0:12:14.767367', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] dining table, laptop\\n[OCR] 101, Pablo Castro, Microsoft Azure, Tools, MICROSOFT, Azure services, Create a, Virtual, Policy, Subscriptions, All resources, Azure Synapse, More services, Resources, machines, networks, Analytics, Recent, Favorite, Name, Type, mechanics191, Network interface, Mechanics-nsg, Network security group, See all, Navigate, Resource groups, Dashboard, Azure Monitor, Microsoft Defender for Cloud, Cost Management, Microsoft Learn, Analyze and optimize your, Learn Azure with free online, Monitor your apps and, Secure your apps and, training from Microsoft, infrastructure, cloud spend for free, Useful links, Azure mobile app, Request Access to Azure OpenAl Service, (Preview), * Required, Please read all instructions carefully and complete form as instructed, Thank you for your interest in the limited preview of Microsoft's Azure OpenAl Service. Please submit this form for 1), approval for a new experimentation scenario using Azure OpenAl's text and code models, including developing, new proofs of concept, 2) approval to move from development to production using Azure OpenAl's text and, code models, 3) expressing interest in the invite-only preview for DALL-E., Limited access public preview customers for text and code models: Azure OpenAI Service requires registration, and is currently only available to Microsoft managed customers and partners during the preview period. Learn more, about limited access to Azure OpenAI Service during the preview period here. Invite-only preview for DALL-E: DALL-, E is currently invite only. We welcome you to share your use case information for future consideration. We cannot, guarantee onboarding or offer information about timelines. Limited access scenarios: When evaluating which, scenarios to onboard, we consider who will directly interact with the application, who will see the output of the, aka.ms/oai/access, the\"}, {'id': 30, 'start': '0:12:14.767367', 'end': '0:12:16.96', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[OCR] and is currently only available to Microsoft managed customers and partners during the preview period. Learn more, E is currently invite only. We welcome you to share your use case information for future consideration. We cannot, guarantee onboarding or offer information about timelines. Limited access scenarios: When evaluating which, about limited access to Azure OpenAl Service during the preview period here. Invite-only preview for DALL-E: DALL-, application, whether the application will be used in a high-stakes domain (e.g ., medical), and the extent to which the, application's capabilities are tightly scoped. In general, applications in high stakes domains will require additional, mitigations and are more likely to be approved for applications with internal-only users and internal-only audiences., Applications with broad possible uses, including content generation capabilities, are more likely to be approved if 1) the, domain is not high stakes and users are authenticated or 2) in the case of high stakes domains, anyone who views or, interacts with the content is internal to your company., Possible causes for a denied application:, 1) You are not a managed customer. Learn more here, 2) The application was for more than one use case. For example, expressing interest in DALL-E while also applying for, a separate text-to-code scenario., 3) Application submitted with personal email (Example: @gmail.com, @yahoo.com, @hotmail.com, etc.), 4) A scenario we are not yet supporting because of higher potential for misuse, such as a content creation application, open to unauthenticated users who can generate unconstrained content on any topic, 5) Vague or incomplete answers, or not responding to a request for more information, Updates about your onboarding request: If you applied for Azure OpenAl text or code models, we will reply within, approximately 10 business days. We are unable to offer timelines for DALL-E. Please add maccount@microsoft.com, and csgate@microsoft.com to your Safe Senders list to ensure you receive all correspondence pertaining to your, application, including the verification email from Microsoft Vetting Service. For questions about the application process,, please contact our team at csgate@microsoft.com, 1. Your First Name *, Enter your answer\\n[Transcript] You'll need to apply for access to Azure Open AI using this format AKA dot Ms.\"}, {'id': 31, 'start': '0:12:16.96', 'end': '0:12:18.96', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[OCR] E is currently invite only. We welcome you to share your use case information for future consideration. We cannot, guarantee onboarding or offer information about timelines. Limited access scenarios: When evaluating which, about limited access to Azure OpenAl Service during the preview period here. Invite-only preview for DALL-E: DALL-, application, whether the application will be used in a high-stakes domain (e.g ., medical), and the extent to which the, application's capabilities are tightly scoped. In general, applications in high stakes domains will require additional, mitigations and are more likely to be approved for applications with internal-only users and internal-only audiences., Applications with broad possible uses, including content generation capabilities, are more likely to be approved if 1) the, domain is not high stakes and users are authenticated or 2) in the case of high stakes domains, anyone who views or, interacts with the content is internal to your company., Possible causes for a denied application:, 1) You are not a managed customer. Learn more here, 2) The application was for more than one use case. For example, expressing interest in DALL-E while also applying for, a separate text-to-code scenario., 3) Application submitted with personal email (Example: @gmail.com, @yahoo.com, @hotmail.com, etc.), 4) A scenario we are not yet supporting because of higher potential for misuse, such as a content creation application, open to unauthenticated users who can generate unconstrained content on any topic, 5) Vague or incomplete answers, or not responding to a request for more information, Updates about your onboarding request: If you applied for Azure OpenAl text or code models, we will reply within, approximately 10 business days. We are unable to offer timelines for DALL-E. Please add maccount@microsoft.com, and csgate@microsoft.com to your Safe Senders list to ensure you receive all correspondence pertaining to your, application, including the verification email from Microsoft Vetting Service. For questions about the application process,, please contact our team at csgate@microsoft.com, 1. Your First Name *, Enter your answer\\n[Transcript] slash OAI slash access.\"}, {'id': 32, 'start': '0:12:18.96', 'end': '0:12:20.773367', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[OCR] Pablo Castro, Microsoft Azure, Overview, Activity log, Diagnose and solve problems, Resource Management, Keys and Endpoint, Model deployments, Pricing tier, Identity, Search, MICROSOFT, pablooai, Go to Azure OpenAl Studio, View Cost, JSON View, Resource group (move) : PabloOpenAl, API type, Status, : Active, Pricing tier : Standard, Location, : South Central US, Endpoint, : https://pablooai.openai.azure.com/, Subscription (move), : Azure Search DEV Search Engine (beloh, jlembicz), Manage keys : Click here to manage keys, Subscription ID, : 829a4cbc-1cb1-4783-be15-75b2d8345e5c, Tags (edit), : Click here to add tags, Get Started, Develop, Deploy a model and start generating completions, Apply the Azure OpenAI APIs for semantic search, summarization and more! The Azure OpenAl, models can adapt to your task with only a few examples or by specifying your task in English, Cost analysis, Explore, Deploy, Get started now with the new Azure OpenAl Studio., Learn the basics and check out our sample code, Deploy a model and get started making API calls, Monitoring, Alerts, Diagnostic settings, Automation'}, {'id': 33, 'start': '0:12:20.773367', 'end': '0:12:27.4', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[OCR] Try it out, Management, File Management, Deployments, Cognitive Services | Azure OpenAl Studio - Preview, pablooai (South Central US, SO), PC, davinci2, Summarize Text, Get started with Azure OpenAI, Explore examples for prompt completion, Classify Text, Natural Language to SQL, Generate New Product Names, Experiment with prompt, Customize a model with fine-tuning, Manage deployments in your, Manage performance results, completions, resource, Azure OpenAl Studio, Perform a wide variety of naturg, Perform a wide variety of natural language tasks with Azure OpenAl, including copywriting, summarization, parsing unstructured text, classification, and translation., Summarize text by adding a 'tl;dr:' to the, Classify items into categories provided at, Translate natural language to SQL queries., Create product names from examples, end of a text passage., inference time., words., Learn more, Manage your deployments and models, Fine-tune a custom model to increase, Upload datasets to use when creating, Try out the completions endpoint by writing, reliability for a wide variety of use cases, Create deployments to explore the model, custom models, and view performance and, a prompt and generating a response. Set, while decreasing costs and speeding up, fine-tune results from training and, processing times., capabilities., different parameters values to adjust how, validation data., the model responds., Go to playground, Start fine-tuning a custom model, Go to Deployments, Go to File management, plore the model, Azure OpenAl Studio > Deployments, Deployments enable you to make completions and search calls against a provided base model or your fine-tuned model. You can also scale up and down your deployments easily by modifying the scale unit., + Create new deployment, Deployment name, Model name, Scale type, Scale size, Created at, Created by, codex, code-cushman-001, Standard, 10/12/2022 1 ... pablocas ..., text-davinci-002, 10/12/2022 1 ..., pablocas ...\\n[Transcript] After you create an Open AI resource, it'll link you directly to the Open AI Studio.\\nIn the studio, you'll start by creating a deployment.\"}, {'id': 34, 'start': '0:12:27.4', 'end': '0:12:44.88', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[OCR] Try it out, Management, Cancel, File Management, Deployments, Cognitive Services | Azure OpenAl Studio - Preview, pablooai (South Central US, SO), PC, davinci2, Azure OpenAl Studio > Deployments, Deployments enable you to make completions and search calls against a provided base model or your fine-tuned model. You can also scale up and down your deployments easily by modifying the scale unit., + Create new deployment, Deployment name, Model name, Scale type, Scale size, Created at, Created by, codex, code-cushman-001, Standard, 10/12/2022 1 ... pablocas ..., 10/12/2022 1 ..., pablocas ..., Deploy model, Set up a deployment to make API calls against a provided base model or a custom, model. Finished deployments are available for use. Your deployment status will move, to succeeded when the deployment is complete and ready for use., Only one deployment is permitted per model. The models with existing deployments are, disabled., Select a model, Create, Base models, code-search-ada-code-001, Deployments enable you to make completions and search calls against a provided base model or your fine-tuned model. You code-search-babbage-code ..., ployments easily by modifying the scale unit., code-search-babbage-text -..., text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-search-curie-query-001, text-search-babbage-doc-0 ..., text-search-babbage-query ..., text-search-davinci-doc-001, text-search-davinci-query-0 ..., text-similarity-babbage-001, text-similarity-davinci-001, Already deployed, code-search-babbage-code ...\\n[Transcript] You can think of a deployment as your own service endpoint for Open AI that your application calls.\\nIn my case, I I'll use an existing deployment, so we'll skip that.\\nYou can see I already have two deployments, 1 using Davinci 2 for natural language generation and another one using codecs for code generation.\"}, {'id': 35, 'start': '0:12:44.88', 'end': '0:13:02.64', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Visual labels] display\\n[OCR] 100, 0.5, Try it out, Management, File Management, Playground, Deployments, Parameters, Examples, Top probabilities, Regenerate, Undo, Frequency penalty, Presence penalty, Pre-response text, Post-response text, Cognitive Services | Azure OpenAl Studio - Preview, pablooai (South Central US, SO), PC, Azure OpenAl Studio > Playground, davinci2, Summarize Text, Code View, Max length (tokens), Stop sequences, Best of, Enter text, Get started with Azure OpenAI, Explore examples for prompt completion, Classify Text, Natural Language to SQL, Generate New Product Names, Experiment with prompt, Customize a model with fine-tuning, Manage deployments in your, Manage performance results, completions, resource, Azure OpenAl Studio, Summarize text by adding a 'tl;dr:' to the, Classify items into categories provided at, Translate natural language to SQL queries., Create product names from examples, end of a text passage., inference time., words., Learn more, Manage your deployments and models, Fine-tune a custom model to increase, Upload datasets to use when creating, Try out the completions endpoint by writing, reliability for a wide variety of use cases, Create deployments to explore the model, custom models, and view performance and, a prompt and generating a response. Set, while decreasing costs and speeding up, fine-tune results from training and, processing times., capabilities., different parameters values to adjust how, validation data., the model responds., Go to playground, Start fine-tuning a custom model, Go to Deployments, Go to File management, codex, Perform a wide variety of natural language tasks with Azure OpenAI, including copywriting, summarization, parsing unstructured text, classification, and translation., Load an example, Language V, Tokens: 69, Start typing here\\n[Transcript] Going back to the home screen, you'll see the studio offers a few examples to get started, summarize text, classified text, natural language to SQL and generating new product names.\\nIn the studio.\\nWe also have the playground, so let me head over there first.\\nI'll choose one of the deployments I created.\\nI'll choose Davinci 2.\"}, {'id': 36, 'start': '0:13:02.64', 'end': '0:13:30.9101', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] dining table, laptop\\n[Visual labels] display\\n[OCR] 100, 0.5, Try it out, Management, File Management, Playground, Deployments, Parameters, Examples, Top probabilities, Regenerate, Undo, Frequency penalty, Presence penalty, Pre-response text, Post-response text, Cognitive Services | Azure OpenAl Studio - Preview, pablooai (South Central US, SO), PC, Azure OpenAl Studio > Playground, davinci2, Summarize Text, Code View, Max length (tokens), Stop sequences, Best of, Enter text, Classify Text, Natural Language to SQL, Generate New Product Names, Product description: A home milkshake maker, Seed words: fast, healthy, compact., Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker, Load an example, Tokens: 69, Start typing here, English to French, Parse unstructured data, Classification, Product description: A pair of shoes that can fit any foot size., Seed words: adaptable, fit, omni-fit., Product names: Omni-Fit shoes, Perfect-Fit shoes, All-Fit shoes.\\n[Transcript] Then I'll select an example from the drop down menu and I'll start with this example for generating a new product name.\\nYou can see the example uses a few shot prompt with a product description and seed words to help it understand and generate results.\\nIn this example we're looking to name shoes that can fit any food size with seed words like Adaptable fit and Omnifit.\\nWhen I generate the results, it comes up with Omnifit shoes, Perfect fit shoes and all fit shoes and so you can see how this can be used to help people with their creative process.\"}, {'id': 37, 'start': '0:13:30.9101', 'end': '0:13:45.825', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] bench, dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, display, man\\n[OCR] 100, 0.5, Models, Try it out, Management, File Management, Playground, Deployments, Parameters, Examples, Top probabilities, Regenerate, Undo, Frequency penalty, Presence penalty, Pre-response text, Post-response text, Pablo Castro, pablooai (South Central US, SO), PC, Azure OpenAl Studio > Playground, Temperature, davinci2, Summarize Text, Code View, Max length (tokens), Stop sequences, Enter text, Microsoft Mechanics, nics, Microsoft M, Learn more, Cognitive Services | Azure OpenAI Studio - Preview, Privacy & cookies [2, A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more, if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical, white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4, solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core, past white dwarf star density to that of atomic nuclei., Tokens: 90\\n[Transcript] How would you then go from these examples that you just showed to maybe the Open AI search app that you showed before?\\nWell, the playground helps you test and iterate on your prompts before you incorporate them into your custom application.'}, {'id': 38, 'start': '0:13:45.825', 'end': '0:14:19.24', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] bench\\n[Visual labels] display, electronics\\n[OCR] 100, 0.5, 10, 0., Azure OpenAl Studio - Preview, Models, Try it out, Management, File Management, Playground, Deployments, Parameters, Examples, Top probabilities, Regenerate, Undo, ments, Frequency penalty, Presence penalty, Pre-response text, Post-response text, Pablo Castro, pablooai (South Central US, SO), PC, Azure OpenAl Studio > Playground, Temperature, davinci2, Summarize Text, Code View, Max length (tokens), Stop sequences, Enter text, Learn more, Cognitive Services | Azure OpenAI Studio - Preview, Privacy & cookies [2, A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more, if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical, white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4, solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core, past white dwarf star density to that of atomic nuclei., Tokens: 90, Tl;dr, gnitive Services, lanagement, it out, odels, out, ate, anagement, und, Tokens: 144, incredibly dense objects have a diameter of about 10 kilometers and a mass of 1.4 solar masses.\\n[Transcript] Let's use the summarized text example as a starting point.\\nThis example prompt is about neutron stars.\\nBy looking at the structure of the prompt, I can see the pattern they follow and learn from it.\\nWe're using the Davinci model, which is the largest of the GPT 3 models and optimized for text understanding.\\nNow you can see this long paragraph of text as input.\\nThe example adds TLDR at the end to suggest to the model that this should be completed with a short summary.\\nLet's run it and here you see the generated output about the neutron star.\"}, {'id': 39, 'start': '0:14:19.24', 'end': '0:14:23.24', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[OCR] Deployments, Examples, Top probabilities, Regenerate, Undo, ments, Frequency penalty, Presence penalty, Pre-response text, Post-response text, Temperature, davinci2, Summarize Text, Code View, Max length (tokens), Stop sequences, Enter text, Learn more, A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more, if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical, white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4, solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core, past white dwarf star density to that of atomic nuclei., Tl;dr, anagement, Tokens: 144, incredibly dense objects have a diameter of about 10 kilometers and a mass of 1.4 solar masses., nA, Le\\n[Transcript] So I'm giving Open AI instructions for what to do in plain English.\"}, {'id': 40, 'start': '0:14:23.24', 'end': '0:14:29.902367', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer\\n[OCR] Deployments, Examples, Top probabilities, Regenerate, Undo, ments, Frequency penalty, Presence penalty, Pre-response text, Post-response text, Temperature, davinci2, Summarize Text, Code View, Max length (tokens), Stop sequences, Enter text, Microsoft Mechanics, A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more, if the star was especially metal-rich.[1] Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical, white holes, quark stars, and strange stars.[2] Neutron stars have a radius on the order of 10 kilometres (6.2 mi) and a mass of about 1.4, solar masses.[3] They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core, past white dwarf star density to that of atomic nuclei., Tl;dr, anagement, Tokens: 144, incredibly dense objects have a diameter of about 10 kilometers and a mass of 1.4 solar masses., Le'}, {'id': 41, 'start': '0:14:29.902367', 'end': '0:15:50.983367', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] dining table, laptop\\n[Visual labels] human face, person, clothing, laptop, computer, display, software, electronics\\n[OCR] 0.50, 100, 0.5, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 11, 10, Models, Try it out, Create customized model, Management, Cancel, Base model, Base model type, Next, O Advanced options, File Management, ada, babbage, curie, Playground, Deployments, Parameters, Examples, Regenerate, Undo, Presence penalty, Cognitive Services | Azure OpenAl Studio - Preview, Pablo Castro, pablooai (South Central US, SO), PC, Azure OpenAl Studio > Playground, Temperature, davinci2, Summarize Text, Code View, Max length (tokens), Answer each question by extracting key facts from as many statements as possible., Statements:, - The West Census Region, where 8.9% of single-family homes (2020 data) had small-scale solar generation, had the highest percentage of, Stop sequences, homes with small-scale solar generation, mostly in California., - The Northeast Census Region had the second-largest percentage of homes with small-scale solar generation at 4.7% (2020 data), - U.S. shipments of solar photovoltaic (PV) modules (solar panels) rose to a record electricity-generating capacity of 28.8 million peak, kilowatts (kW) in 2021, from 21.8 million peak kW in 2020, based on data from our Annual Photovoltaic Module Shipments Report., - The United States added 13.2 gigawatts (GW) of utility-scale solar capacity in 2021, an annual record and 25% more than the 10.6 GW, added in 2020, according to our Annual Electric Generator Report.'}, {'id': 42, 'start': '0:15:50.983367', 'end': '0:15:50.983367', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Visual labels] computer, electronics\\n[OCR] Status, Model name, pablooai (South Central US, S0), Azure OpenAl Studio > Models, data for better performance and more accurate results., Customized models, Provided models, Privacy & cooki, yground, gement, els, Azure OpenAl is powered by models with different capabilities and price points. Deploy one of the provided base models to try it out in Playground or train a custom model to your specific use case, Learn more about the different types of provided models [2, Delete, Creat ..., ft-d37981cb0df240e8addd6633c92d237f, 11/18 ... curie\\n[Transcript] Coming back to the Cross document summarization example, let's guide the model to read a set of statements so it can answer a question based on those statements, just like a human would.\\nIf I go to Visual Studio and grab my prompt and paste it here, you'll see our prompt including the placeholders our code replaced at runtime in the app.\\nThis is real content, so it's pretty long and hard to follow.\\nSo instead of running that one, let me just paste a shorter version that has the same structure so we canmore easily see what's going on.\\nI'll ask it to complete the task, which is to answer the question about the percentage of homes with solar, and you'll see a detailed answer with a few regions called out.\\nThe answer was accurate, but I'd like to see a shorter version of the answer, and this is where I can provide an example answer to further refine the results for the style that I'm looking for.\\nSo first, I'll undo the last run to clear the answer.\\nNow, I'll paste in the same statement from before, but you'll see that this is a different question with a very concise answer.\\nFrom there, I can generate a new result for the first question and you'll see that it learned from our example and returned a very concise answer.\\nSo the playground in the Azure Open AI Studio helps you experiment and iterate.\\nYou can test your prompts for accuracy, refine them, and then bring them into your custom applications.\"}, {'id': 43, 'start': '0:15:50.983367', 'end': '0:16:32.5916', 'content': \"[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End\\n[Detected objects] chair, dining table, laptop\\n[Visual labels] computer, electronics, human face, person, clothing, laptop, man, speech\\n[OCR] Status, Model name, pablooai (South Central US, S0), Azure OpenAl Studio > Models, data for better performance and more accurate results., Customized models, Provided models, Privacy & cooki, yground, gement, els, Azure OpenAl is powered by models with different capabilities and price points. Deploy one of the provided base models to try it out in Playground or train a custom model to your specific use case, Learn more about the different types of provided models [2, Delete, Creat ..., ft-d37981cb0df240e8addd6633c92d237f, 11/18 ... curie, Microsoft Mechanics, Microsoft M, Microsoft Me, aka.ms/oai/access, www.microsoft.com/mechanics, CS\\n[Transcript] And if that isn't precise enough, one thing that I didn't have time to demonstrate today is Azure Open AI's support for fine tuning, where you can specialize one of the base models with additional data that you provide so you have the full spectrum from serial shot to creating your own fine-tuned models.\\nAnd all of what you've shown today, to be clear, is completely grounded in security and responsible AI.\\nSo now that you've got hands on with Azure's open AI service, can you explain for everyone watching how they might get started and learn more?\\nThe best way to get started is by trying it out.\\nWe've made it easy to sign up for the service at AKA dot Ms.\\nslash OAI slash access and from there you can start using Azure's Open AI Studio.\\nThanks so much Paul for joining us today and showing us some highlights of what Azure Open AI can do.\\nOf course, keep checking back to Microsoft Mechanics for all the latest update.\\nSubscribe to our channel if you haven't already.\\nAnd as always, thank you for watching.\"}, {'id': 44, 'start': '0:16:32.5916', 'end': '0:16:39.2983', 'content': '[Video title] ChatGPT OpenAI powering your apps OpenAI Studio in Microsoft Azure\\n[Tags] End-Rolling credits\\n[Visual labels] circle\\n[OCR] Microsoft Mechanics, www.microsoft.com/mechanics'}]}\n",
      "Processing Completed\n"
     ]
    }
   ],
   "source": [
    "accessToken = getAccessToken()\n",
    "while True:\n",
    "    promptContent = getPromptContent(videoId, accessToken)\n",
    "    if not \"ErrorType\" in promptContent:\n",
    "        print(promptContent)\n",
    "        print('Processing Completed')\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(20)\n",
    "        print('Processing...')\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3 - Summarize the video for each sections iteratively and create the final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = promptContent['sections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummary(sections):\n",
    "    preText = f'You are given a video including its visual, audio and text insights: ' \\\n",
    "    '[Transcript] is the text that is spoken in the video ' \\\n",
    "    '[OCR] is the visual text in the video, ' \\\n",
    "    '[Known People] are the people that appear in the video, ' \\\n",
    "    '[Audio effects] are the sounds in the video, ' \\\n",
    "    '[Detected objects] are the objects that appear in the video, ' \\\n",
    "    '[Visual labels] are the objects that appear in the video. ' \\\n",
    "    'Use these insights as part of the video\\'s content, but don\\'t use their initials (written in []) as-is'\n",
    "    postText = \"Generate informative and detailed summary:\"\n",
    "    finalText = postText\n",
    "    summary = ''\n",
    "    combineSummary = ''\n",
    "    for i, section in enumerate(sections):\n",
    "        if len(summary) > 0:\n",
    "            preTextExtra = f'In the previous parts of the video the following ideas were discussed : \\n {sectionAnswer}\\n'\n",
    "        else:\n",
    "            preTextExtra = ''\n",
    "        \n",
    "        summaryText = f'{preText} \\n {preTextExtra}'\n",
    "        if i == 0:\n",
    "            partOfVideo = f'Given the first part of the video:'\n",
    "\n",
    "        if i > 0:\n",
    "            partOfVideo = f'Given the next part of the video:'\n",
    "\n",
    "        if i == len(sections) - 1:\n",
    "            partOfVideo = f'Given the last part of the video:'\n",
    "            postText = finalText\n",
    "\n",
    "        summaryText += f'\\n {partOfVideo}'\n",
    "        summaryText += \"\"\"\n",
    "        {text}\n",
    "        \"\"\"\n",
    "        summaryText += f'\\n {postText}'\n",
    "        customPrompt = PromptTemplate(template=summaryText, input_variables=[\"text\"])\n",
    "        chainType = \"map_reduce\"\n",
    "        docs = [Document(page_content=section[\"content\"], metadata={\"id\": '', \"source\": ''})]\n",
    "        summaryChain = load_summarize_chain(llm, chain_type=chainType, map_prompt=customPrompt, combine_prompt=customPrompt)\n",
    "        sectionSummary = summaryChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "        sectionAnswer = sectionSummary['output_text']\n",
    "        combineSummary += sectionAnswer\n",
    "        summary = sectionAnswer\n",
    "    return summary, combineSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4 - For each section create indexed document in cognitive search with embedded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoInsights = []\n",
    "summary, combineSummary = getSummary(sections)\n",
    "for section in sections:\n",
    "    videoInsights.append({\n",
    "        'sectionId': section['id'],\n",
    "        'sectionStart': section['start'],\n",
    "        'sectionEnd': section['end'],\n",
    "        'sectionContent': section['content'],\n",
    "        'sectionVector': '',\n",
    "        'videoId': videoId,\n",
    "        'videoName': promptContent['name'],\n",
    "        'summary': combineSummary,\n",
    "        'sourcefile': str(section['start']) + ' ' + str(section['end'])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this part of the video, the speaker focuses on the usage of the ChatGPT OpenAI model in Azure OpenAI Studio for cross-document summarization. They explain that the model can read a set of statements and answer questions based on them. The speaker provides an example prompt in Visual Studio and demonstrates how the code replaces placeholders at runtime. They ask the model a question about the percentage of homes with solar, and the model provides a detailed answer with specific regions mentioned. However, the speaker desires a shorter answer and provides an example answer to refine the results. They input a different question, and the model generates a concise answer, learning from the example. The speaker highlights the playground in Azure Open AI Studio, which allows users to experiment, iterate, test prompts, refine them, and integrate them into custom applications. The video also mentions the support for fine-tuning in Azure Open AI, where users can specialize one of the base models with additional data. The speaker assures that security and responsible AI are prioritized in all the demonstrated features. To get started with Azure Open AI, viewers are encouraged to sign up for the service and start using Azure's Open AI Studio. The video concludes with gratitude towards the presenter and a reminder to stay updated with Microsoft Mechanics.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def createVideoSearchIndex(SearchService, SearchKey, indexName):\n",
    "    indexClient = SearchIndexClient(endpoint=f\"https://{SearchService}.search.windows.net/\",\n",
    "            credential=AzureKeyCredential(SearchKey))\n",
    "    if indexName not in indexClient.list_index_names():\n",
    "        index = SearchIndex(\n",
    "            name=indexName,\n",
    "            fields=[\n",
    "                        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "                        SimpleField(name=\"sectionId\", type=SearchFieldDataType.String),\n",
    "                        SimpleField(name=\"sectionStart\", type=SearchFieldDataType.String),\n",
    "                        SimpleField(name=\"sectionEnd\", type=SearchFieldDataType.String),\n",
    "                        SimpleField(name=\"videoId\", type=SearchFieldDataType.String),\n",
    "                        SearchableField(name=\"videoName\", type=SearchFieldDataType.String,\n",
    "                                        searchable=True, retrievable=True, analyzer_name=\"en.microsoft\"),\n",
    "                        SearchableField(name=\"content\", type=SearchFieldDataType.String,\n",
    "                                        searchable=True, retrievable=True, analyzer_name=\"en.microsoft\"),\n",
    "                        SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                                    searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"vectorConfig\"),\n",
    "                        SearchableField(name=\"summary\", type=SearchFieldDataType.String,\n",
    "                                        searchable=True, retrievable=True, analyzer_name=\"en.microsoft\"),\n",
    "                        SimpleField(name=\"sourcefile\", type=\"Edm.String\", filterable=True, retrievable=True),\n",
    "            ],\n",
    "            vector_search = VectorSearch(\n",
    "                algorithm_configurations=[\n",
    "                    HnswVectorSearchAlgorithmConfiguration(\n",
    "                        name=\"vectorConfig\",\n",
    "                        kind=\"hnsw\",\n",
    "                        parameters={\n",
    "                            \"m\": 4,\n",
    "                            \"efConstruction\": 400,\n",
    "                            \"efSearch\": 500,\n",
    "                            \"metric\": \"cosine\"\n",
    "                        }\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            semantic_settings=SemanticSettings(\n",
    "                configurations=[SemanticConfiguration(\n",
    "                    name='semanticConfig',\n",
    "                    prioritized_fields=PrioritizedFields(\n",
    "                        title_field=SemanticField(field_name=\"content\"), prioritized_content_fields=[SemanticField(field_name='content')]))],\n",
    "                        prioritized_keywords_fields=[SemanticField(field_name='sourcefile')])\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            print(f\"Creating {indexName} search index\")\n",
    "            indexClient.create_index(index)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(f\"Search index {indexName} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index msmechanics already exists\n"
     ]
    }
   ],
   "source": [
    "createVideoSearchIndex(SearchService, SearchKey, \"msmechanics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generateKbEmbeddings(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, OpenAiEmbedding, embeddingModelType, text):\n",
    "    if (embeddingModelType == 'azureopenai'):\n",
    "        try:\n",
    "            client = AzureOpenAI(\n",
    "                        api_key = OpenAiKey,  \n",
    "                        api_version = OpenAiVersion,\n",
    "                        azure_endpoint = OpenAiEndPoint\n",
    "                        )\n",
    "\n",
    "            response = client.embeddings.create(\n",
    "                input=text, model=OpenAiEmbedding)\n",
    "            embeddings = response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "\n",
    "    elif embeddingModelType == \"openai\":\n",
    "        try:\n",
    "            client = OpenAI(api_key=OpenAiApiKey)\n",
    "            response = client.embeddings.create(\n",
    "                    input=text, model=\"text-embedding-ada-002\", api_key = OpenAiApiKey)\n",
    "            embeddings = response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            logging.info(e)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "def createSections(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, embeddingModelType, OpenAiEmbedding, docs):\n",
    "    counter = 1\n",
    "    for i in docs:\n",
    "        yield {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"sectionId\": str(i.get('sectionId')),\n",
    "            \"sectionStart\": i.get('sectionStart'),\n",
    "            \"sectionEnd\": i.get('sectionEnd'),\n",
    "            \"content\": i.get('sectionContent'),\n",
    "            \"contentVector\": generateKbEmbeddings(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, OpenAiEmbedding, embeddingModelType,  i.get('sectionContent')),\n",
    "            \"videoId\": i.get('videoId'),\n",
    "            \"videoName\": i.get('videoName'),\n",
    "            \"summary\": i.get('summary'),\n",
    "            \"sourcefile\": i.get('sourcefile')\n",
    "        }\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "def indexSections(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, embeddingModelType, OpenAiEmbedding, SearchService, SearchKey, indexName, docs):\n",
    "    print(\"Total docs: \" + str(len(docs)))\n",
    "    sections = createSections(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, embeddingModelType, OpenAiEmbedding, docs)\n",
    "    searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net/\",\n",
    "                                    index_name=indexName,\n",
    "                                    credential=AzureKeyCredential(SearchKey))\n",
    "    i = 0\n",
    "    batch = []\n",
    "    for s in sections:\n",
    "        batch.append(s)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            results = searchClient.index_documents(batch=batch)\n",
    "            succeeded = sum([1 for r in results if r.succeeded])\n",
    "            print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        results = searchClient.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 4\n",
      "\tIndexed 4 sections, 4 succeeded\n"
     ]
    }
   ],
   "source": [
    "indexSections(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, embeddingModelType, OpenAiEmbedding, SearchService, SearchKey, \"videos\", videoInsights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 5 - Run RAG Pattern on indexed video Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import Vector\n",
    "def performVideoSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, question, indexName, k, returnFields=[\"sectionId\", \"sectionContent\", \"sectionStart\", \"sectionEnd\"] ):\n",
    "    searchClient = SearchClient(endpoint=f\"https://{SearchService}.search.windows.net\",\n",
    "        index_name=indexName,\n",
    "        credential=AzureKeyCredential(SearchKey))\n",
    "    try:\n",
    "        r = searchClient.search(  \n",
    "            search_text=question,  \n",
    "            vectors=[Vector(value=generateKbEmbeddings(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, embeddingModelType, OpenAiEmbedding, \n",
    "                                                     question), k=k, fields=\"contentVector\")],  \n",
    "            select=returnFields,\n",
    "            query_type=\"semantic\", \n",
    "            query_language=\"en-us\", \n",
    "            semantic_configuration_name='semanticConfig', \n",
    "            query_caption=\"extractive\", \n",
    "            query_answer=\"extractive\",\n",
    "            include_total_count=True,\n",
    "            top=k\n",
    "        )\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askQuestion(query):\n",
    "    chainType = \"stuff\"\n",
    "    topK = 3\n",
    "\n",
    "    # Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "    #r = performVideoSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, OpenAiEmbedding, embeddingModelType,\n",
    "    #                    query, \"videos\", topK, returnFields=[\"sectionId\", \"sectionContent\", \"sectionStart\", \"sectionEnd\", \"summary\"])\n",
    "    r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding,\n",
    "                        query, \"videos\", topK, returnFields=[\"id\", \"sectionId\", \"content\", \"sectionStart\", \"sectionEnd\", \"summary\", \"sourcefile\"])\n",
    "\n",
    "    if r == None:\n",
    "        docs = [Document(page_content=\"No results found\")]\n",
    "    else :\n",
    "        docs = [\n",
    "            Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "            for doc in r\n",
    "            ]\n",
    "\n",
    "    qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "    answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "    outputAnswer = answer['output_text']\n",
    "    return outputAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple\\'s event is scheduled for October 30th and it is called the \"Scary Fast\" event. The products planned to be announced at the event include new MacBook Pros with 14-inch and 16-inch models featuring M3 Pro and M3 Max chips, as well as an iMac update with the standard M3 chip. The M3 chip is significant as it will be based on a three nanometer production process, offering faster performance, improved power efficiency, and better battery life. The event will be online-only with no in-person component.\\nSOURCES: 0:00:46.279567 0:01:16.3763, 0:01:16.3763 0:02:30.984167, 0:02:30.984167 0:02:38.9588'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When is Apple's event scheduled and what is it called?. What are the products planned to be announced at the event?\"\n",
    "askQuestion(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The speakers are Mark Gurman and another unidentified person.\\nSOURCES: 0:01:16.3763 0:02:30.984167, 0:00:46.279567 0:01:16.3763, 0:02:30.984167 0:02:38.9588'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who are the speakers?\"\n",
    "askQuestion(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The video discusses Apple\\'s upcoming product launch event on October 30th, which is called \"Scaryfast.\" It is expected that new Mac chips and new Macs will be announced. The event will be online-only, with no in-person component. Possible products to be introduced include new MacBook Pros with M3 Pro and M3 Max chips, as well as an iMac with the standard M3 chip. The M3 chip is significant as it will be based on a three nanometer production process, offering faster performance, better power efficiency, and improved battery life. The video also mentions the logo morphing into the Finder icon, indicating that the event is Mac-centric. The timing of the event at 5:00 PM and the dark theme of the invitation suggest a Halloween theme. The discussion highlights the anticipation for these product launches and the potential for significant advancements in technology. \\nSOURCES: 0:00:00 0:00:46.279567, 0:01:16.3763 0:02:30.984167'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can you summarize the discussion in the video?\"\n",
    "askQuestion(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is no mention of the price of the new products.\\nSOURCES:'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Is there a price mentioned of the new products?\"\n",
    "askQuestion(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The CEO is not mentioned in the provided content.\\nSOURCES:'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is the CEO?\"\n",
    "askQuestion(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Time Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanicsVideosList = [\n",
    "    {\n",
    "        \"name\": \"Interview_Farooq\",\n",
    "        \"id\": \"48ca2e67e3\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Interview_JensLottner\",\n",
    "        \"id\": \"de586609ec\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Interview_BusinessWeek\",\n",
    "        \"id\": \"12e65e4bdb\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Interview_BeyondTheBell\",\n",
    "        \"id\": \"17561131a2\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Interview_JacquelineRong\",\n",
    "        \"id\": \"ae98aab761\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanicsVideoInsights = []\n",
    "for video in mechanicsVideosList:\n",
    "    videoId = video['id']\n",
    "    videoName = video['name']\n",
    "    print(\"Processing video : \" + videoId)\n",
    "    # Create Prompt Content for the Video\n",
    "    accessToken = getAccessToken()\n",
    "    promptContentResp = createPromptContent(videoId, accessToken)\n",
    "    accessToken = getAccessToken()\n",
    "    while True:\n",
    "        promptContent = getPromptContent(videoId, accessToken)\n",
    "        if not \"ErrorType\" in promptContent:\n",
    "            print(promptContent)\n",
    "            print('Processing Completed')\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(20)\n",
    "            print('Processing Content...')\n",
    "            continue\n",
    "    # Get list of all sections from Prompts\n",
    "    sections = promptContent['sections']\n",
    "    # Get summary for all sections\n",
    "    summary, combineSummary = getSummary(sections)\n",
    "    print(\"Got Summary for videoId : \" + videoId)\n",
    "    # Append data for each Video\n",
    "    for section in sections:\n",
    "        mechanicsVideoInsights.append({\n",
    "            'sectionId': section['id'],\n",
    "            'sectionStart': section['start'],\n",
    "            'sectionEnd': section['end'],\n",
    "            'sectionContent': section['content'],\n",
    "            'sectionVector': '',\n",
    "            'videoId': videoId,\n",
    "            'videoName': promptContent['name'],\n",
    "            'summary': combineSummary,\n",
    "            'sourcefile': str(section['start']) + ' ' + str(section['end'])\n",
    "        })\n",
    "\n",
    "# Create Index\n",
    "createVideoSearchIndex(SearchService, SearchKey, \"bloombergint\")\n",
    "# Index Sections\n",
    "indexSections(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, embeddingModelType, OpenAiEmbedding, SearchService, SearchKey, \"bloombergint\", mechanicsVideoInsights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobClient\n",
    "from Utilities.azureBlob import upsertMetadata\n",
    "\n",
    "for video in mechanicsVideosList:\n",
    "    videoName = video['name']\n",
    "    videoId = video['id']\n",
    "    blobName = videoName + '.mp4'\n",
    "    blob = BlobClient.from_connection_string(conn_str=OpenAiDocConnStr, container_name=OpenAiVideoContainer, blob_name=blobName)\n",
    "    if blob.exists():\n",
    "        metadata = {'embedded': 'true', 'namespace': \"bloombergint\", 'indexType': \"cogsearchvs\", \n",
    "                                    \"indexName\": \"Bloomberg Interview\".replace(\"-\", \"_\"),\n",
    "                                    \"textSplitterType\": \"recursive\", \n",
    "                                    \"chunkSize\": \"8000\", \"chunkOverlap\": \"1000\",\n",
    "                                    \"promptType\": \"generic\", \"videoId\": videoId, \"videoName\": videoName}\n",
    "        upsertMetadata(OpenAiDocConnStr, OpenAiVideoContainer, blobName, metadata)\n",
    "    else:\n",
    "        print(blobName + \" not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
