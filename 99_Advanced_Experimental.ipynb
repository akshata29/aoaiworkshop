{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "import pytesseract\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI\n",
    "import uuid\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure environment variables  \n",
    "load_dotenv(dotenv_path='./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getenv('PATH'))\n",
    "#os.environ[\"PATH\"] = os.getenv('PATH') + ';' + 'C:\\\\Program Files\\\\poppler-24.02.0\\\\Library\\\\bin\\\\'\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bofa2018 = \"./Data/BOFA/BofA 2018.pdf\"\n",
    "# Get elements\n",
    "rawPdfElements = partition_pdf(\n",
    "    filename=bofa2018,\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "398\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "text_elements = []\n",
    "table_elements = []\n",
    "image_elements = []\n",
    "\n",
    "# Function to encode images\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "for element in rawPdfElements:\n",
    "    if 'CompositeElement' in str(type(element)):\n",
    "        text_elements.append(element)\n",
    "    elif 'Table' in str(type(element)):\n",
    "        table_elements.append(element)\n",
    "\n",
    "table_elements = [i.text for i in table_elements]\n",
    "text_elements = [i.text for i in text_elements]\n",
    "\n",
    "# Tables\n",
    "print(len(table_elements))\n",
    "\n",
    "# Text\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_file in os.listdir(output_path):\n",
    "#     if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#         image_path = os.path.join(output_path, image_file)\n",
    "#         encoded_image = encode_image(image_path)\n",
    "#         image_elements.append(encoded_image)\n",
    "# print(len(image_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv('OpenAiWestUsKey'),  \n",
    "    api_version=os.getenv('OpenAiGpt4vVersion'),\n",
    "    #base_url=f\"{os.getenv('OpenAiWestUsEp')}openai/deployments/{os.getenv('OpenAiGpt4v')}/extensions\",\n",
    "    azure_endpoint=os.getenv('OpenAiWestUsEp'),\n",
    ")\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.getenv('OpenAiEndPoint'), azure_deployment=os.getenv('OpenAiEmbedding'), \n",
    "                                   api_key=os.getenv('OpenAiKey'), openai_api_type=\"azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\astalati\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chat_models\\azure_openai.py:156: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://dataaiapim.azure-api.net to https://dataaiapim.azure-api.net/openai.\n",
      "  warnings.warn(\n",
      "C:\\Users\\astalati\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chat_models\\azure_openai.py:163: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\astalati\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\chat_models\\azure_openai.py:171: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://dataaiapim.azure-api.net to https://dataaiapim.azure-api.net/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "                openai_api_base=os.getenv('OpenAiEndPoint'),\n",
    "                openai_api_version=os.getenv('OpenAiVersion'),\n",
    "                deployment_name=os.getenv('OpenAiChat'),\n",
    "                openai_api_key=os.getenv('OpenAiKey'),\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeText(text):\n",
    "    prompt = f\"Summarize the following text:\\n\\n{text}\\n\\nSummary:\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function for table summaries\n",
    "def summarizeTable(table):\n",
    "    prompt = f\"Summarize the following table:\\n\\n{table}\\n\\nSummary:\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Function to add documents to the retriever\n",
    "def addDocumentToRetriever(retriever, idKey, summaries, originalContents):\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, metadata={idKey: doc_ids[i]})\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, originalContents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th element of texts processed.\n",
      "2th element of texts processed.\n",
      "3th element of texts processed.\n",
      "4th element of texts processed.\n",
      "5th element of texts processed.\n",
      "6th element of texts processed.\n",
      "7th element of texts processed.\n",
      "8th element of texts processed.\n",
      "9th element of texts processed.\n",
      "10th element of texts processed.\n",
      "11th element of texts processed.\n",
      "12th element of texts processed.\n",
      "13th element of texts processed.\n",
      "14th element of texts processed.\n",
      "15th element of texts processed.\n",
      "16th element of texts processed.\n",
      "17th element of texts processed.\n",
      "18th element of texts processed.\n",
      "19th element of texts processed.\n",
      "20th element of texts processed.\n",
      "21th element of texts processed.\n",
      "22th element of texts processed.\n",
      "23th element of texts processed.\n",
      "24th element of texts processed.\n",
      "25th element of texts processed.\n",
      "26th element of texts processed.\n",
      "27th element of texts processed.\n",
      "28th element of texts processed.\n",
      "29th element of texts processed.\n",
      "30th element of texts processed.\n",
      "31th element of texts processed.\n",
      "32th element of texts processed.\n",
      "33th element of texts processed.\n",
      "34th element of texts processed.\n",
      "35th element of texts processed.\n",
      "36th element of texts processed.\n",
      "37th element of texts processed.\n",
      "38th element of texts processed.\n",
      "39th element of texts processed.\n",
      "40th element of texts processed.\n",
      "41th element of texts processed.\n",
      "42th element of texts processed.\n",
      "43th element of texts processed.\n",
      "44th element of texts processed.\n",
      "45th element of texts processed.\n",
      "46th element of texts processed.\n",
      "47th element of texts processed.\n",
      "48th element of texts processed.\n",
      "49th element of texts processed.\n",
      "50th element of texts processed.\n",
      "51th element of texts processed.\n",
      "52th element of texts processed.\n",
      "53th element of texts processed.\n",
      "54th element of texts processed.\n",
      "55th element of texts processed.\n",
      "56th element of texts processed.\n",
      "57th element of texts processed.\n",
      "58th element of texts processed.\n",
      "59th element of texts processed.\n",
      "60th element of texts processed.\n",
      "61th element of texts processed.\n",
      "62th element of texts processed.\n",
      "63th element of texts processed.\n",
      "64th element of texts processed.\n",
      "65th element of texts processed.\n",
      "66th element of texts processed.\n",
      "67th element of texts processed.\n",
      "68th element of texts processed.\n",
      "69th element of texts processed.\n",
      "70th element of texts processed.\n",
      "71th element of texts processed.\n",
      "72th element of texts processed.\n",
      "73th element of texts processed.\n",
      "74th element of texts processed.\n",
      "75th element of texts processed.\n",
      "76th element of texts processed.\n",
      "77th element of texts processed.\n",
      "78th element of texts processed.\n",
      "79th element of texts processed.\n",
      "80th element of texts processed.\n",
      "81th element of texts processed.\n",
      "82th element of texts processed.\n",
      "83th element of texts processed.\n",
      "84th element of texts processed.\n",
      "85th element of texts processed.\n",
      "86th element of texts processed.\n",
      "87th element of texts processed.\n",
      "88th element of texts processed.\n",
      "89th element of texts processed.\n",
      "90th element of texts processed.\n",
      "91th element of texts processed.\n",
      "92th element of texts processed.\n",
      "93th element of texts processed.\n",
      "94th element of texts processed.\n",
      "95th element of texts processed.\n",
      "96th element of texts processed.\n",
      "97th element of texts processed.\n",
      "98th element of texts processed.\n",
      "99th element of texts processed.\n",
      "100th element of texts processed.\n",
      "101th element of texts processed.\n",
      "102th element of texts processed.\n",
      "103th element of texts processed.\n",
      "104th element of texts processed.\n",
      "105th element of texts processed.\n",
      "106th element of texts processed.\n",
      "107th element of texts processed.\n",
      "108th element of texts processed.\n",
      "109th element of texts processed.\n",
      "110th element of texts processed.\n",
      "111th element of texts processed.\n",
      "112th element of texts processed.\n",
      "113th element of texts processed.\n",
      "114th element of texts processed.\n",
      "115th element of texts processed.\n",
      "116th element of texts processed.\n",
      "117th element of texts processed.\n",
      "118th element of texts processed.\n",
      "119th element of texts processed.\n",
      "120th element of texts processed.\n",
      "121th element of texts processed.\n",
      "122th element of texts processed.\n",
      "123th element of texts processed.\n",
      "124th element of texts processed.\n",
      "125th element of texts processed.\n",
      "126th element of texts processed.\n",
      "127th element of texts processed.\n",
      "128th element of texts processed.\n",
      "129th element of texts processed.\n",
      "130th element of texts processed.\n",
      "131th element of texts processed.\n",
      "132th element of texts processed.\n",
      "133th element of texts processed.\n",
      "134th element of texts processed.\n",
      "135th element of texts processed.\n",
      "136th element of texts processed.\n",
      "137th element of texts processed.\n",
      "138th element of texts processed.\n",
      "139th element of texts processed.\n",
      "140th element of texts processed.\n",
      "141th element of texts processed.\n",
      "142th element of texts processed.\n",
      "143th element of texts processed.\n",
      "144th element of texts processed.\n",
      "145th element of texts processed.\n",
      "146th element of texts processed.\n",
      "147th element of texts processed.\n",
      "148th element of texts processed.\n",
      "149th element of texts processed.\n",
      "150th element of texts processed.\n",
      "151th element of texts processed.\n",
      "152th element of texts processed.\n",
      "153th element of texts processed.\n",
      "154th element of texts processed.\n",
      "155th element of texts processed.\n",
      "156th element of texts processed.\n",
      "157th element of texts processed.\n",
      "158th element of texts processed.\n",
      "159th element of texts processed.\n",
      "160th element of texts processed.\n",
      "161th element of texts processed.\n",
      "162th element of texts processed.\n",
      "163th element of texts processed.\n",
      "164th element of texts processed.\n",
      "165th element of texts processed.\n",
      "166th element of texts processed.\n",
      "167th element of texts processed.\n",
      "168th element of texts processed.\n",
      "169th element of texts processed.\n",
      "170th element of texts processed.\n",
      "171th element of texts processed.\n",
      "172th element of texts processed.\n",
      "173th element of texts processed.\n",
      "174th element of texts processed.\n",
      "175th element of texts processed.\n",
      "176th element of texts processed.\n",
      "177th element of texts processed.\n",
      "178th element of texts processed.\n",
      "179th element of texts processed.\n",
      "180th element of texts processed.\n",
      "181th element of texts processed.\n",
      "182th element of texts processed.\n",
      "183th element of texts processed.\n",
      "184th element of texts processed.\n",
      "185th element of texts processed.\n",
      "186th element of texts processed.\n",
      "187th element of texts processed.\n",
      "188th element of texts processed.\n",
      "189th element of texts processed.\n",
      "190th element of texts processed.\n",
      "191th element of texts processed.\n",
      "192th element of texts processed.\n",
      "193th element of texts processed.\n",
      "194th element of texts processed.\n",
      "195th element of texts processed.\n",
      "196th element of texts processed.\n",
      "197th element of texts processed.\n",
      "198th element of texts processed.\n",
      "199th element of texts processed.\n",
      "200th element of texts processed.\n",
      "201th element of texts processed.\n",
      "202th element of texts processed.\n",
      "203th element of texts processed.\n",
      "204th element of texts processed.\n",
      "205th element of texts processed.\n",
      "206th element of texts processed.\n",
      "207th element of texts processed.\n",
      "208th element of texts processed.\n",
      "209th element of texts processed.\n",
      "210th element of texts processed.\n",
      "211th element of texts processed.\n",
      "212th element of texts processed.\n",
      "213th element of texts processed.\n",
      "214th element of texts processed.\n",
      "215th element of texts processed.\n",
      "216th element of texts processed.\n",
      "217th element of texts processed.\n",
      "218th element of texts processed.\n",
      "219th element of texts processed.\n",
      "220th element of texts processed.\n",
      "221th element of texts processed.\n",
      "222th element of texts processed.\n",
      "223th element of texts processed.\n",
      "224th element of texts processed.\n",
      "225th element of texts processed.\n",
      "226th element of texts processed.\n",
      "227th element of texts processed.\n",
      "228th element of texts processed.\n",
      "229th element of texts processed.\n",
      "230th element of texts processed.\n",
      "231th element of texts processed.\n",
      "232th element of texts processed.\n",
      "233th element of texts processed.\n",
      "234th element of texts processed.\n",
      "235th element of texts processed.\n",
      "236th element of texts processed.\n",
      "237th element of texts processed.\n",
      "238th element of texts processed.\n",
      "239th element of texts processed.\n",
      "240th element of texts processed.\n",
      "241th element of texts processed.\n",
      "242th element of texts processed.\n",
      "243th element of texts processed.\n",
      "244th element of texts processed.\n",
      "245th element of texts processed.\n",
      "246th element of texts processed.\n",
      "247th element of texts processed.\n",
      "248th element of texts processed.\n",
      "249th element of texts processed.\n",
      "250th element of texts processed.\n",
      "251th element of texts processed.\n",
      "252th element of texts processed.\n",
      "253th element of texts processed.\n",
      "254th element of texts processed.\n",
      "255th element of texts processed.\n",
      "256th element of texts processed.\n",
      "257th element of texts processed.\n",
      "258th element of texts processed.\n",
      "259th element of texts processed.\n",
      "260th element of texts processed.\n",
      "261th element of texts processed.\n",
      "262th element of texts processed.\n",
      "263th element of texts processed.\n",
      "264th element of texts processed.\n",
      "265th element of texts processed.\n",
      "266th element of texts processed.\n",
      "267th element of texts processed.\n",
      "268th element of texts processed.\n",
      "269th element of texts processed.\n",
      "270th element of texts processed.\n",
      "271th element of texts processed.\n",
      "272th element of texts processed.\n",
      "273th element of texts processed.\n",
      "274th element of texts processed.\n",
      "275th element of texts processed.\n",
      "276th element of texts processed.\n",
      "277th element of texts processed.\n",
      "278th element of texts processed.\n",
      "279th element of texts processed.\n",
      "280th element of texts processed.\n",
      "281th element of texts processed.\n",
      "282th element of texts processed.\n",
      "283th element of texts processed.\n",
      "284th element of texts processed.\n",
      "285th element of texts processed.\n",
      "286th element of texts processed.\n",
      "287th element of texts processed.\n",
      "288th element of texts processed.\n",
      "289th element of texts processed.\n",
      "290th element of texts processed.\n",
      "291th element of texts processed.\n",
      "292th element of texts processed.\n",
      "293th element of texts processed.\n",
      "294th element of texts processed.\n",
      "295th element of texts processed.\n",
      "296th element of texts processed.\n",
      "297th element of texts processed.\n",
      "298th element of texts processed.\n",
      "299th element of texts processed.\n",
      "300th element of texts processed.\n",
      "301th element of texts processed.\n",
      "302th element of texts processed.\n",
      "303th element of texts processed.\n",
      "304th element of texts processed.\n",
      "305th element of texts processed.\n",
      "306th element of texts processed.\n",
      "307th element of texts processed.\n",
      "308th element of texts processed.\n",
      "309th element of texts processed.\n",
      "310th element of texts processed.\n",
      "311th element of texts processed.\n",
      "312th element of texts processed.\n",
      "313th element of texts processed.\n",
      "314th element of texts processed.\n",
      "315th element of texts processed.\n",
      "316th element of texts processed.\n",
      "317th element of texts processed.\n",
      "318th element of texts processed.\n",
      "319th element of texts processed.\n",
      "320th element of texts processed.\n",
      "321th element of texts processed.\n",
      "322th element of texts processed.\n",
      "323th element of texts processed.\n",
      "324th element of texts processed.\n",
      "325th element of texts processed.\n",
      "326th element of texts processed.\n",
      "327th element of texts processed.\n",
      "328th element of texts processed.\n",
      "329th element of texts processed.\n",
      "330th element of texts processed.\n",
      "331th element of texts processed.\n",
      "332th element of texts processed.\n",
      "333th element of texts processed.\n",
      "334th element of texts processed.\n",
      "335th element of texts processed.\n",
      "336th element of texts processed.\n",
      "337th element of texts processed.\n",
      "338th element of texts processed.\n",
      "339th element of texts processed.\n",
      "340th element of texts processed.\n",
      "341th element of texts processed.\n",
      "342th element of texts processed.\n",
      "343th element of texts processed.\n",
      "344th element of texts processed.\n",
      "345th element of texts processed.\n",
      "346th element of texts processed.\n",
      "347th element of texts processed.\n",
      "348th element of texts processed.\n",
      "349th element of texts processed.\n",
      "350th element of texts processed.\n",
      "351th element of texts processed.\n",
      "352th element of texts processed.\n",
      "353th element of texts processed.\n",
      "354th element of texts processed.\n",
      "355th element of texts processed.\n",
      "356th element of texts processed.\n",
      "357th element of texts processed.\n",
      "358th element of texts processed.\n",
      "359th element of texts processed.\n",
      "360th element of texts processed.\n",
      "361th element of texts processed.\n",
      "362th element of texts processed.\n",
      "363th element of texts processed.\n",
      "364th element of texts processed.\n",
      "365th element of texts processed.\n",
      "366th element of texts processed.\n",
      "367th element of texts processed.\n",
      "368th element of texts processed.\n",
      "369th element of texts processed.\n",
      "370th element of texts processed.\n",
      "371th element of texts processed.\n",
      "372th element of texts processed.\n",
      "373th element of texts processed.\n",
      "374th element of texts processed.\n",
      "375th element of texts processed.\n",
      "376th element of texts processed.\n",
      "377th element of texts processed.\n",
      "378th element of texts processed.\n",
      "379th element of texts processed.\n",
      "380th element of texts processed.\n",
      "381th element of texts processed.\n",
      "382th element of texts processed.\n",
      "383th element of texts processed.\n",
      "384th element of texts processed.\n",
      "385th element of texts processed.\n",
      "386th element of texts processed.\n",
      "387th element of texts processed.\n",
      "388th element of texts processed.\n",
      "389th element of texts processed.\n",
      "390th element of texts processed.\n",
      "391th element of texts processed.\n",
      "392th element of texts processed.\n",
      "393th element of texts processed.\n",
      "394th element of texts processed.\n",
      "395th element of texts processed.\n",
      "396th element of texts processed.\n",
      "397th element of texts processed.\n",
      "398th element of texts processed.\n"
     ]
    }
   ],
   "source": [
    "# Processing text elements with feedback and sleep\n",
    "textSummaries = []\n",
    "for i, te in enumerate(text_elements):\n",
    "    summary = summarizeText(te)\n",
    "    textSummaries.append(summary)\n",
    "    print(f\"{i + 1}th element of texts processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th element of tables processed.\n",
      "2th element of tables processed.\n",
      "3th element of tables processed.\n",
      "4th element of tables processed.\n",
      "5th element of tables processed.\n",
      "6th element of tables processed.\n",
      "7th element of tables processed.\n",
      "8th element of tables processed.\n",
      "9th element of tables processed.\n",
      "10th element of tables processed.\n",
      "11th element of tables processed.\n",
      "12th element of tables processed.\n",
      "13th element of tables processed.\n",
      "14th element of tables processed.\n",
      "15th element of tables processed.\n",
      "16th element of tables processed.\n",
      "17th element of tables processed.\n",
      "18th element of tables processed.\n",
      "19th element of tables processed.\n",
      "20th element of tables processed.\n",
      "21th element of tables processed.\n",
      "22th element of tables processed.\n",
      "23th element of tables processed.\n",
      "24th element of tables processed.\n",
      "25th element of tables processed.\n",
      "26th element of tables processed.\n",
      "27th element of tables processed.\n",
      "28th element of tables processed.\n",
      "29th element of tables processed.\n",
      "30th element of tables processed.\n",
      "31th element of tables processed.\n",
      "32th element of tables processed.\n",
      "33th element of tables processed.\n",
      "34th element of tables processed.\n",
      "35th element of tables processed.\n",
      "36th element of tables processed.\n",
      "37th element of tables processed.\n",
      "38th element of tables processed.\n",
      "39th element of tables processed.\n",
      "40th element of tables processed.\n",
      "41th element of tables processed.\n",
      "42th element of tables processed.\n",
      "43th element of tables processed.\n",
      "44th element of tables processed.\n",
      "45th element of tables processed.\n",
      "46th element of tables processed.\n",
      "47th element of tables processed.\n",
      "48th element of tables processed.\n",
      "49th element of tables processed.\n",
      "50th element of tables processed.\n",
      "51th element of tables processed.\n",
      "52th element of tables processed.\n",
      "53th element of tables processed.\n",
      "54th element of tables processed.\n",
      "55th element of tables processed.\n",
      "56th element of tables processed.\n",
      "57th element of tables processed.\n",
      "58th element of tables processed.\n",
      "59th element of tables processed.\n",
      "60th element of tables processed.\n",
      "61th element of tables processed.\n",
      "62th element of tables processed.\n",
      "63th element of tables processed.\n",
      "64th element of tables processed.\n",
      "65th element of tables processed.\n",
      "66th element of tables processed.\n",
      "67th element of tables processed.\n",
      "68th element of tables processed.\n",
      "69th element of tables processed.\n",
      "70th element of tables processed.\n",
      "71th element of tables processed.\n",
      "72th element of tables processed.\n",
      "73th element of tables processed.\n",
      "74th element of tables processed.\n",
      "75th element of tables processed.\n",
      "76th element of tables processed.\n",
      "77th element of tables processed.\n",
      "78th element of tables processed.\n",
      "79th element of tables processed.\n",
      "80th element of tables processed.\n",
      "81th element of tables processed.\n",
      "82th element of tables processed.\n",
      "83th element of tables processed.\n",
      "84th element of tables processed.\n",
      "85th element of tables processed.\n",
      "86th element of tables processed.\n",
      "87th element of tables processed.\n",
      "88th element of tables processed.\n",
      "89th element of tables processed.\n",
      "90th element of tables processed.\n",
      "91th element of tables processed.\n",
      "92th element of tables processed.\n",
      "93th element of tables processed.\n",
      "94th element of tables processed.\n",
      "95th element of tables processed.\n",
      "96th element of tables processed.\n",
      "97th element of tables processed.\n",
      "98th element of tables processed.\n",
      "99th element of tables processed.\n",
      "100th element of tables processed.\n",
      "101th element of tables processed.\n",
      "102th element of tables processed.\n",
      "103th element of tables processed.\n",
      "104th element of tables processed.\n",
      "105th element of tables processed.\n",
      "106th element of tables processed.\n",
      "107th element of tables processed.\n",
      "108th element of tables processed.\n",
      "109th element of tables processed.\n",
      "110th element of tables processed.\n",
      "111th element of tables processed.\n",
      "112th element of tables processed.\n",
      "113th element of tables processed.\n",
      "114th element of tables processed.\n",
      "115th element of tables processed.\n",
      "116th element of tables processed.\n",
      "117th element of tables processed.\n",
      "118th element of tables processed.\n",
      "119th element of tables processed.\n",
      "120th element of tables processed.\n",
      "121th element of tables processed.\n",
      "122th element of tables processed.\n",
      "123th element of tables processed.\n",
      "124th element of tables processed.\n",
      "125th element of tables processed.\n",
      "126th element of tables processed.\n",
      "127th element of tables processed.\n",
      "128th element of tables processed.\n",
      "129th element of tables processed.\n",
      "130th element of tables processed.\n",
      "131th element of tables processed.\n",
      "132th element of tables processed.\n",
      "133th element of tables processed.\n",
      "134th element of tables processed.\n",
      "135th element of tables processed.\n",
      "136th element of tables processed.\n",
      "137th element of tables processed.\n",
      "138th element of tables processed.\n",
      "139th element of tables processed.\n",
      "140th element of tables processed.\n",
      "141th element of tables processed.\n",
      "142th element of tables processed.\n",
      "143th element of tables processed.\n",
      "144th element of tables processed.\n",
      "145th element of tables processed.\n",
      "146th element of tables processed.\n",
      "147th element of tables processed.\n",
      "148th element of tables processed.\n",
      "149th element of tables processed.\n",
      "150th element of tables processed.\n",
      "151th element of tables processed.\n",
      "152th element of tables processed.\n",
      "153th element of tables processed.\n",
      "154th element of tables processed.\n",
      "155th element of tables processed.\n",
      "156th element of tables processed.\n",
      "157th element of tables processed.\n",
      "158th element of tables processed.\n",
      "159th element of tables processed.\n",
      "160th element of tables processed.\n",
      "161th element of tables processed.\n",
      "162th element of tables processed.\n",
      "163th element of tables processed.\n",
      "164th element of tables processed.\n",
      "165th element of tables processed.\n",
      "166th element of tables processed.\n",
      "167th element of tables processed.\n",
      "168th element of tables processed.\n",
      "169th element of tables processed.\n",
      "170th element of tables processed.\n",
      "171th element of tables processed.\n",
      "172th element of tables processed.\n",
      "173th element of tables processed.\n",
      "174th element of tables processed.\n",
      "175th element of tables processed.\n",
      "176th element of tables processed.\n",
      "177th element of tables processed.\n",
      "178th element of tables processed.\n",
      "179th element of tables processed.\n",
      "180th element of tables processed.\n",
      "181th element of tables processed.\n",
      "182th element of tables processed.\n",
      "183th element of tables processed.\n",
      "184th element of tables processed.\n",
      "185th element of tables processed.\n",
      "186th element of tables processed.\n",
      "187th element of tables processed.\n",
      "188th element of tables processed.\n",
      "189th element of tables processed.\n",
      "190th element of tables processed.\n",
      "191th element of tables processed.\n",
      "192th element of tables processed.\n",
      "193th element of tables processed.\n",
      "194th element of tables processed.\n",
      "195th element of tables processed.\n",
      "196th element of tables processed.\n",
      "197th element of tables processed.\n",
      "198th element of tables processed.\n",
      "199th element of tables processed.\n",
      "200th element of tables processed.\n",
      "201th element of tables processed.\n",
      "202th element of tables processed.\n",
      "203th element of tables processed.\n",
      "204th element of tables processed.\n",
      "205th element of tables processed.\n",
      "206th element of tables processed.\n",
      "207th element of tables processed.\n",
      "208th element of tables processed.\n",
      "209th element of tables processed.\n",
      "210th element of tables processed.\n",
      "211th element of tables processed.\n",
      "212th element of tables processed.\n",
      "213th element of tables processed.\n",
      "214th element of tables processed.\n",
      "215th element of tables processed.\n",
      "216th element of tables processed.\n",
      "217th element of tables processed.\n",
      "218th element of tables processed.\n",
      "219th element of tables processed.\n",
      "220th element of tables processed.\n",
      "221th element of tables processed.\n",
      "222th element of tables processed.\n",
      "223th element of tables processed.\n",
      "224th element of tables processed.\n",
      "225th element of tables processed.\n",
      "226th element of tables processed.\n",
      "227th element of tables processed.\n",
      "228th element of tables processed.\n",
      "229th element of tables processed.\n",
      "230th element of tables processed.\n",
      "231th element of tables processed.\n",
      "232th element of tables processed.\n",
      "233th element of tables processed.\n",
      "234th element of tables processed.\n",
      "235th element of tables processed.\n",
      "236th element of tables processed.\n",
      "237th element of tables processed.\n",
      "238th element of tables processed.\n",
      "239th element of tables processed.\n",
      "240th element of tables processed.\n",
      "241th element of tables processed.\n",
      "242th element of tables processed.\n",
      "243th element of tables processed.\n",
      "244th element of tables processed.\n",
      "245th element of tables processed.\n",
      "246th element of tables processed.\n",
      "247th element of tables processed.\n"
     ]
    }
   ],
   "source": [
    "# Processing table elements with feedback and sleep\n",
    "tableSummaries = []\n",
    "for i, te in enumerate(table_elements):\n",
    "    summary = summarizeTable(te)\n",
    "    tableSummaries.append(summary)\n",
    "    print(f\"{i + 1}th element of tables processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001FB133FB310>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001FADBCFB310>, model='text-embedding-ada-002', deployment='embedding', openai_api_version='2023-05-15', openai_api_base=None, openai_api_type='azure', openai_proxy='', embedding_ctx_length=8191, openai_api_key='1361532858cb428c8da412a73105de78', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=16, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, azure_endpoint='https://dataaiapim.azure-api.net', azure_ad_token=None, azure_ad_token_provider=None, validate_base_url=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vector store and storage layer\n",
    "vectorStore = Chroma(collection_name=\"summaries\", embedding_function=AzureOpenAIEmbeddings(azure_endpoint=os.getenv('OpenAiEndPoint'), azure_deployment=os.getenv('OpenAiEmbedding'), \n",
    "                                   api_key=os.getenv('OpenAiKey'), openai_api_type=\"azure\"))\n",
    "store = InMemoryStore()\n",
    "idKey = \"doc_id\"\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorStore, docstore=store, id_key=idKey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text summaries\n",
    "addDocumentToRetriever(retriever, idKey, textSummaries, text_elements)\n",
    "# Add table summaries\n",
    "addDocumentToRetriever(retriever, idKey, tableSummaries, table_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(Dollars in millions) Sales and trading revenue 2018 2017 Fixed-income, currencies and commodities $ 8,186 $ Equities 4,876 Total sales and trading revenue $ 13,062 $ Sales and trading revenue, excluding net DVA (3) Fixed-income, currencies and commodities $ 8,328 $ Equities 4,896 Total sales and trading revenue, excluding net DVA $ 13,224 $ 8,657 4,120 12,777 9,051 4,154 13,205',\n",
       " 'Sales and Trading Revenue (1, 2)\\n\\nThe return on average allocated capital was11 percent, up from 9 percent, reflecting higher net income. For more information on capital allocated to the business segments, see Business Segment Operations on page 30.\\n\\nSales and Trading Revenue Sales and trading revenue includes unrealized and realized gains and losses on trading and other assets, net interest income, and fees primarily from commissions on equity securities. Sales and trading revenue is segregated into fixed-income (government debt obligations, investment and non-investment grade corporate debt obligations, commercial MBS, residential mortgage-backed securities, collateralized loan obligations, interest rate and credit derivative contracts), currencies (interest rate and foreign exchange contracts), commodities (primarily futures, forwards, swaps and options) and equities (equity-linked derivatives and cash equity activity). The following table and related discussion present sales and trading revenue, substantially all of which is in Global Markets, with the remainder in Global Banking. In addition, the following table and related discussion present sales and trading revenue, excluding net DVA, which is a non-GAAP financial measure. For more information on net DVA, see Supplemental Financial Data on page 24.',\n",
       " '(1) Includes FTE adjustments of $249 million and $236 million for 2018 and 2017. For more information on sales and trading revenue, see Note 3 – Derivativesto the Consolidated Financial Statements.\\n\\n(2) Includes Global Banking sales and trading revenue of $430 million and $236 million for 2018 and 2017.\\n\\n2017.\\n\\n(3) FICC and Equities sales and trading revenue, excluding net DVA, is a non-GAAP financial measure. FICC net DVAlosses were $142 million and $394 million for 2018 and 2017. Equities net DVA losses were $20 million and $34 million for 2018 and 2017.\\n\\nThe following explanations for year-over-year changes in sales and trading, FICC and Equities revenue exclude net DVA, but would be the same whether net DVA was included or excluded. FICC revenue decreased $723 million in 2018 primarily due to lower activity and a less favorable market in credit-related products. The decline in FICC revenue was also impacted by higher funding costs, which were driven by increases in market interest rates. Equities revenue increased $742 million in 2018 driven by strength in client financing and derivatives.\\n\\nAll Other',\n",
       " 'Trading Account Profits Net Interest Income Other (1) Total (Dollars in millions) 2018 Interest rate risk $ 1,180 $ 1,292 $ 220 $ 2,692 Foreign exchange risk 1,503 (7 ) 6 1,502 Equity risk 3,994 (781 ) 1,619 4,832 Credit risk 1,063 1,853 552 3,468 Other risk 189 64 66 319 Total sales and trading revenue $ 7,929 $ 2,421 $ 2,463 $ 12,813 2017 Interest rate risk $ 712 $ 1,560 $ 249 $ 2,521 Foreign exchange risk 1,417 (1 ) 7 1,423 Equity risk 2,689 (517 ) 1,903 4,075 Credit risk 1,685 1,937 576 4,198 Other risk 203 45 76 324 Total sales and trading revenue $ 6,706 $ 3,024 $ 2,811 $ 12,541 2016 Interest rate risk $ 1,189 $ 2,002 $ 145 $ 3,336 Foreign exchange risk 1,360 (10 ) 5 1,355 Equity risk 1,917 28 2,074 4,019 Credit risk 1,674 1,956 424 4,054 Other risk 407 (7 ) 39 439 Total sales and trading revenue $ 6,547 $ 3,969 $ 2,687 $ 13,203']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can retrieve this table\n",
    "retriever.get_relevant_documents(\n",
    "    \"What is the trading revenue from sales and equities?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text, images and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The trading revenue from sales and equities is $4,876 million.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "     \"What is the trading revenue from sales and equities?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The percentage change in cash and cash equivalents from 2017 to 2018 is 13%.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the percentage change in cash and cash equivalents from 2017 and 2018?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To determine whether trading account liabilities were positive or negative in 2017 and 2018, we need to analyze the information provided. Unfortunately, the given context does not include any specific data or figures related to trading account liabilities. Therefore, we cannot answer the question based on the provided context.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Was trading account liabilities positive or negative from 2017 and 2018 and by how much?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The five segments for operations are:\\n\\n- Consumer Banking: Offers a range of credit, banking, and investment products and services to consumers and small businesses. Product offerings include savings accounts, CDs, checking accounts, residential mortgages, and loans.\\n- Global Wealth & Investment Management (GWIM): Provides tailored financial solutions to clients with over $250,000 in total investable assets. Services include wealth management, brokerage, banking, retirement products, and specialty asset management.\\n- Global Banking: Offers lending-related products and services, working capital management, treasury solutions, and investment banking products. Clients include middle-market companies, commercial real estate firms, global corporations, financial institutions, and mid-sized U.S.-based businesses.\\n- Global Markets: Provides sales and trading services, research services, market-making, financing, and risk management products to institutional clients. Covers fixed-income, credit, currency, commodity, and equity businesses.\\n- All Other: Consists of ALM activities, equity investments, non-core mortgage loans, servicing activities, liquidating businesses, and residual expense allocations. Also includes certain revenue and expense methodologies that are utilized to determine net income.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are the five segments for operations, give details in bulleted format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The net interest income from deposits in 2018 for the Consumer Banking business is not provided in the given context.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the net interest income from deposits in 2018 for Consumer banking business?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Langchain AzSearch Native Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits: 30\n"
     ]
    }
   ],
   "source": [
    "# Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "loader = AzureAIDocumentIntelligenceLoader(file_path=\"./Data/PDF/Bert.pdf\", \n",
    "                                           api_key = os.getenv(\"FormRecognizerKey\"), api_endpoint = os.getenv(\"FormRecognizerEndPoint\"), \n",
    "                                           api_model=\"prebuilt-layout\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = docs[0].page_content\n",
    "splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(splits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n===  \\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language {jacobdevlin, mingweichang, kentonl, kristout}@google.com'),\n",
       " Document(page_content='We introduce a new language representa- tion model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language repre- sentation models (Peters et al., 2018a; Rad- ford et al., 2018), BERT is designed to pre- train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a re- sult, the pre-trained BERT model can be fine- tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task- specific architecture modifications.  \\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQUAD v1.1 question answer- ing Test F1 to 93.2 (1.5 point absolute im- provement) and SQUAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', metadata={'Header 1': 'Abstract'}),\n",
       " Document(page_content='Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the re- lationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).  \\nThere are two existing strategies for apply- ing pre-trained language representations to down- stream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as addi- tional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre- trained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.  \\nWe argue that current techniques restrict the power of the pre-trained representations, espe- cially for the fine-tuning approaches. The ma- jor limitation is that standard language models are unidirectional, and this limits the choice of archi- tectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-to- right architecture, where every token can only at- tend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017). Such re- strictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fine- tuning based approaches to token-level tasks such as question answering, where it is crucial to incor- porate context from both directions.  \\nIn this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidi- rectionality constraint by using a \"masked lan- guage model\" (MLM) pre-training objective, in- spired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked  \\narXiv:1810.04805v2 [cs.CL] 24 May 2019\\n:selected: :selected: :unselected: :unselected: :selected: :selected: :unselected: :unselected: :unselected: :selected: :unselected: :selected: :selected: :selected: :selected:\\nword based only on its context. Unlike left-to- right language model pre-training, the MLM ob- jective enables the representation to fuse the left and the right context, which allows us to pre- train a deep bidirectional Transformer. In addi- tion to the masked language model, we also use a \"next sentence prediction\" task that jointly pre- trains text-pair representations. The contributions of our paper are as follows:  \\n· We demonstrate the importance of bidirectional pre-training for language representations. Un- like Radford et al. (2018), which uses unidirec- tional language models for pre-training, BERT uses masked language models to enable pre- trained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.  \\n· We show that pre-trained representations reduce the need for many heavily-engineered task- specific architectures. BERT is the first fine- tuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outper- forming many task-specific architectures.  \\n· BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert.', metadata={'Header 1': '1 Introduction'}),\n",
       " Document(page_content='There is a long history of pre-training general lan- guage representations, and we briefly review the most widely-used approaches in this section.', metadata={'Header 1': '2 Related Work'}),\n",
       " Document(page_content='Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, of- fering significant improvements over embeddings learned from scratch (Turian et al., 2010). To pre- train word embedding vectors, left-to-right lan- guage modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to dis- criminate correct from incorrect words in left and right context (Mikolov et al., 2013).  \\nThese approaches have been generalized to coarser granularities, such as sentence embed- dings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sen- tence words given a representation of the previous sentence (Kiros et al., 2015), or denoising auto- encoder derived objectives (Hill et al., 2016).  \\nELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding re- search along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual rep- resentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including ques- tion answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. (2016) proposed learning contextual representations through a task to pre- dict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation mod- els.', metadata={'Header 1': '2 Related Work', 'Header 2': '2.1 Unsupervised Feature-based Approaches'}),\n",
       " Document(page_content='As with the feature-based approaches, the first works in this direction only pre-trained word em- bedding parameters from unlabeled text (Col- lobert and Weston, 2008).  \\nMore recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model-\\n<figure>  \\n![](figures/0)  \\n<!-- FigureContent=\"NSP Mask LM Mask LM MNLI NER SQUAD 1 Start/End Span C T. T. C T. ’ T. ISEP T,\\' T ... \\'M T. ... T. ISEP T.\\' T .. .. BERT . BERT E CLS] E, EN FISEPT E,\\' EM ECLS E, EN [SEP] E, ’ ... [CLS] Tok 1 Tok N [SEP Tok TokM CLS] Tok 1 Tok N SEP Tok 1 TokM ... ... ... ... Masked Sentence A Masked Sentence B Question Paragraph Unlabeled Sentence A and B Pair Question Answer Pair Pre-training Fine-Tuning\" -->  \\n<figcaption>  \\nFigure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).  \\n</figcaption>  \\n</figure>  \\ning and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).', metadata={'Header 1': '2 Related Work', 'Header 2': '2.2 Unsupervised Fine-tuning Approaches'}),\n",
       " Document(page_content='There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014).', metadata={'Header 1': '2 Related Work', 'Header 2': '2.3 Transfer Learning from Supervised Data'}),\n",
       " Document(page_content='We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and fine-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine- tuning, the BERT model is first initialized with the pre-trained parameters, and all of the param- eters are fine-tuned using labeled data from the downstream tasks. Each downstream task has sep- arate fine-tuned models, even though they are ini- tialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section.  \\nA distinctive feature of BERT is its unified ar- chitecture across different tasks. There is mini-  \\nmal difference between the pre-trained architec- ture and the final downstream architecture.  \\nModel Architecture BERT\\'s model architec- ture is a multi-layer bidirectional Transformer en- coder based on the original implementation de- scribed in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our im- plementation is almost identical to the original, we will omit an exhaustive background descrip- tion of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2  \\nIn this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Param- eters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M).  \\nBERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Trans- former uses constrained self-attention where every token can only attend to context to its left.4  \\n<!-- Footnote=\"1https://github.com/tensorflow/tensor2tensor\" -->  \\n<!-- Footnote=\"2http://nlp.seas.harvard.edu/2018/04/03/attention.html\" -->  \\n<!-- Footnote=\"3In all cases we set the feed-forward/filter size to be 4H,\" -->  \\n<!-- Footnote=\"i.e., 3072 for the H = 768 and 4096 for the H = 1024.\" -->  \\n<!-- Footnote=\"4We note that in the literature the bidirectional Trans-\" -->  \\nInput/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ( Question, Answer )) in one token sequence. Throughout this work, a \"sentence\" can be an arbi- trary span of contiguous text, rather than an actual linguistic sentence. A \"sequence\" refers to the in- put token sequence to BERT, which may be a sin- gle sentence or two sentences packed together.  \\nWe use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special clas- sification token ([CLS]). The final hidden state corresponding to this token is used as the ag- gregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ( [ SEP ] ). Second, we add a learned embed- ding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS ] token as C E RH, and the final hidden vector for the ith input token as Ti ERH.  \\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiza- tion of this construction can be seen in Figure 2.', metadata={'Header 1': '3 BERT'}),\n",
       " Document(page_content='Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsuper- vised tasks, described in this section. This step is presented in the left part of Figure 1.  \\nTask #1: Masked LM Intuitively, it is reason- able to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-to- right and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirec- tional conditioning would allow each word to in- directly \"see itself\", and the model could trivially predict the target word in a multi-layered context.  \\nIn order to train a deep bidirectional representa- tion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \"masked LM\" (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece to- kens in each sequence at random. In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than recon- structing the entire input.  \\nAlthough this allows us to obtain a bidirec- tional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not ap- pear during fine-tuning. To mitigate this, we do not always replace \"masked\" words with the ac- tual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2.  \\nTask #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Ques- tion Answering (QA) and Natural Language Infer- ence (NLI) are based on understanding the rela- tionship between two sentences, which is not di- rectly captured by language modeling. In order to train a model that understands sentence rela- tionships, we pre-train for a binarized next sen- tence prediction task that can be trivially gener- ated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pre- training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). As we show in Figure 1, C is used for next sentence predic- tion (NSP).5 Despite its simplicity, we demon- strate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6  \\n<!-- Footnote=\"former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"Transformer decoder\" since it can be used for text generation.\" -->  \\n<!-- Footnote=\"5The final model achieves 97%-98% accuracy on NSP. 6The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP.\" -->  \\n| Input | [CLS] | my | dog | is | cute | [SEP] | he | likes | play | ##ing | [SEP] |\\n| - | - | - | - | - | - | - | - | - | - | - | - |\\n| | |||||||||||\\n| Token Embeddings | EICLS] | E | -dog | E. \"is | E. cute | E, \"[SEPT | Ehe | Elikes | E. play | Eating | ErSEP1 |\\n| | + | + | + | + | + | | + | + | + :selected: | + | + |\\n| Segment Embeddings | E -A | A | EA | | EA | A | ER | ER \"B | EB | ER | ER |\\n| | + | + | + | + | + | + | + | + | + | + | + |\\n| Position Embeddings | E. | E1 | E2 | E3 | EA | E5 | E6 | E- | E8 | Eg | E10 |  \\n<figure>  \\n![](figures/1)  \\n</figure>  \\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta- tion embeddings and the position embeddings.  \\nThe NSP task is closely related to representation- learning objectives used in Jernite et al. (2017) and Logeswaran and Lee (2018). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all pa- rameters to initialize end-task model parameters.  \\nPre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is criti- cal to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark (Chelba et al., 2013) in order to extract long contiguous sequences.', metadata={'Header 1': '3 BERT', 'Header 2': '3.1 Pre-training BERT'}),\n",
       " Document(page_content='Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks- - whether they involve single text or text pairs-by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs be- fore applying bidirectional cross attention, such as Parikh et al. (2016); Seo et al. (2017). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidi- rectional cross attention between two sentences.  \\nFor each task, we simply plug in the task- specific inputs and outputs into BERT and fine- tune all the parameters end-to-end. At the in- put, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphras- ing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and  \\n(4) a degenerate text-Ø pair in text classification or sequence tagging. At the output, the token rep- resentations are fed into an output layer for token- level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as en- tailment or sentiment analysis.  \\nCompared to pre-training, fine-tuning is rela- tively inexpensive. All of the results in the pa- per can be replicated in at most 1 hour on a sin- gle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We de- scribe the task-specific details in the correspond- ing subsections of Section 4. More details can be found in Appendix A.5.', metadata={'Header 1': '3 BERT', 'Header 2': '3.2 Fine-tuning BERT'}),\n",
       " Document(page_content='In this section, we present BERT fine-tuning re- sults on 11 NLP tasks.', metadata={'Header 1': '4 Experiments'}),\n",
       " Document(page_content='The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018a) is a col- lection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1.  \\nTo fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hid- den vector C E RH corresponding to the first input token ( [CLS ]) as the aggregate representa- tion. The only new parameters introduced during fine-tuning are classification layer weights W E RKXH, where K is the number of labels. We com- pute a standard classification loss with C and W, i.e., log(softmax(CWT)).  \\n<!-- Footnote=\"7For example, the BERT SQUAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.\" -->  \\n<!-- Footnote=\"8See (10) in https://gluebenchmark.com/faq.\" -->  \\n| System | MNLI-(m/mm) 392k | QQP 363k | QNLI 108k | SST-2 67k | CoLA 8.5k | STS-B 5.7k | MRPC 3.5k | RTE 2.5k | Average - |\\n| - | - | - | - | - | - | - | - | - | - |\\n| Pre-OpenAI SOTA | 80.6/80.1 | 66.1 | 82.3 | 93.2 | 35.0 | 81.0 | 86.0 | 61.7 | 74.0 |\\n| BİLSTM+ELMo+Attn | 76.4/76.1 | 64.8 | 79.8 | 90.4 | 36.0 | 73.3 | 84.9 | 56.8 | 71.0 |\\n| OpenAI GPT | 82.1/81.4 | 70.3 | 87.4 | 91.3 | 45.4 | 80.0 | 82.3 | 56.0 | 75.1 |\\n| BERTBASE | 84.6/83.4 | 71.2 | 90.5 | 93.5 | 52.1 | 85.8 | 88.9 | 66.4 | 79.6 |\\n| BERTLARGE | 86.7/85.9 | 72.1 | 92.7 | 94.9 | 60.5 | 86.5 | 89.3 | 70.1 | 82.1 |  \\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard). The number below each task denotes the number of training examples. The \"Average\" column is slightly different than the official GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single- model, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.  \\nWe use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that fine- tuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but per- form different fine-tuning data shuffling and clas- sifier layer initialization.9  \\nResults are presented in Table 1. Both BERTBASE and BERTLARGE outperform all sys- tems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy im- provement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the at- tention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing.  \\nWe find that BERTLARGE significantly outper- forms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2.', metadata={'Header 1': '4 Experiments', 'Header 2': '4.1 GLUE'}),\n",
       " Document(page_content='The Stanford Question Answering Dataset (SQUAD v1.1) is a collection of 100k crowd- sourced question/answer pairs (Rajpurkar et al., 2016). Given a question and a passage from  \\nWikipedia containing the answer, the task is to predict the answer text span in the passage.  \\nAs shown in Figure 1, in the question answer- ing task, we represent the input question and pas- sage as a single packed sequence, with the ques- tion using the A embedding and the passage using the B embedding. We only introduce a start vec- tor S E RRH and an end vector E E RH during fine-tuning. The probability of word i being the start of the answer span is computed as a dot prod- uct between T; and S followed by a softmax over all of the words in the paragraph: Pi = Ejes·Tj. eS.Ti  \\nThe analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as S.Ti + E.Tj, and the maximum scoring span where j ≥ i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.  \\nTable 2 shows top leaderboard entries as well as results from top published systems (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018). The top results from the SQUAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA (Joshi et al., 2017) befor fine-tuning on SQUAD.  \\nOur best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sys- tem in terms of F1 score. Without TriviaQA fine-  \\n<!-- Footnote=\"9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE.\" -->  \\n<!-- Footnote=\"10https://gluebenchmark.com/leaderboard\" -->  \\n<!-- Footnote=\"11QANet is described in Yu et al. (2018), but the system has improved substantially after publication.\" -->  \\n| System | Dev || Test ||\\n||| EM F1  | EM | F1 |\\n| - | - | - | - | - |\\n|| Top Leaderboard Systems (Dec 10th, 2018) || | |\\n| Human | - | - || 82.3 91.2  |\\n| #1 Ensemble - nlnet | - | - || 86.0 91.7  |\\n| #2 Ensemble - QANet | - | - || 84.5 90.5  |\\n| Published ||| | |\\n| BiDAF+ELMo (Single) | - | 85.6 | - | 85.8 |\\n| R.M. Reader (Ensemble) |||| 81.2 87.9 82.3 88.5    |\\n| Ours |||||\\n| BERTBASE (Single) || 80.8 88.5  | - | - |\\n| BERTLARGE (Single) || 84.1 90.9  | - | - |\\n| BERTLARGE (Ensemble) || 85.8 91.8  | - | - |\\n| BERTLARGE (Sgl.+TriviaQA) |||| 84.2 91.1 85.1 91.8    |\\n||||| BERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2     |  \\nTable 2: SQUAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training check- points and fine-tuning seeds.  \\n| System | Dev || Test ||\\n||||| EM F1 EM F1    |\\n| - | - | - | - | - |\\n|| Top Leaderboard Systems (Dec 10th, 2018) || | |\\n| Human |||| 86.3 89.0 86.9 89.5    |\\n| #1 Single - MIR-MRC (F-Net) | - | - || 74.8 78.0  |\\n| #2 Single - nlnet | - | - || 74.2 77.1  |\\n| Published ||| | |\\n| unet (Ensemble) | - | - || 71.4 74.9  |\\n| SLQA+ (Single) | - | || 71.4 74.4  |\\n| Ours BERTLARGE (Single)\\nTable 3: SQUAD 2.0 results. We exclude entries that use BERT as one of their components.  \\ntuning data, we only lose 0.1-0.4 F1, still outper- forming all existing systems by a wide margin.12', metadata={'Header 1': '4 Experiments', 'Header 2': '4.2 SQUAD v1.1'}),\n",
       " Document(page_content='The SQUAD 2.0 task extends the SQUAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided para- graph, making the problem more realistic.  \\nWe use a simple approach to extend the SQUAD v1.1 BERT model for this task. We treat ques- tions that do not have an answer as having an an- swer span with start and end at the [CLS] to- ken. The probability space for the start and end answer span positions is extended to include the position of the [CLS ] token. For prediction, we compare the score of the no-answer span: Snull = S.C + E.C to the score of the best non-null span  \\n| System || Dev Test  |\\n| - | - | - |\\n| ESIM+Glo Ve || 51.9 52.7  |\\n| ESIM+ELMo || 59.1 59.2  |\\n| OpenAI GPT | - | 78.0 |\\n| BERTBASE | 81.6 | - |\\n| BERTLARGE || 86.6 86.3  |\\n| Human (expert)+ | - | 85.0 |\\n| Human (5 annotations)+ | - | 88.0 |  \\nTable 4: SWAG Dev and Test accuracies. Human per- formance is measured with 100 samples, as reported in the SWAG paper.  \\nSî,j = maxj>¿S.T; + E.Tj. We predict a non-null answer when sij > Snull + T, where the thresh- old T is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.  \\nThe results compared to prior leaderboard en- tries and top published work (Sun et al., 2018; Wang et al., 2018b) are shown in Table 3, exclud- ing systems that use BERT as one of their com- ponents. We observe a +5.1 F1 improvement over the previous best system.', metadata={'Header 1': '4 Experiments', 'Header 2': '4.3 SQUAD v2.0'}),\n",
       " Document(page_content=\"The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair com- pletion examples that evaluate grounded common- sense inference (Zellers et al., 2018). Given a sen- tence, the task is to choose the most plausible con- tinuation among four choices.  \\nWhen fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-specific parameters introduced is a vec- tor whose dot product with the [CLS ] token rep- resentation C denotes a score for each choice which is normalized with a softmax layer.  \\nWe fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Re- sults are presented in Table 4. BERTLARGE out- performs the authors' baseline ESIM+ELMo sys- tem by +27.1% and OpenAI GPT by 8.3%.\", metadata={'Header 1': '4 Experiments', 'Header 2': '4.4 SWAG'}),\n",
       " Document(page_content='In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional  \\n<!-- Footnote=\"12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.\" -->\\n|||| 78.7 81.9 80.0 83.1    |  \\n| Tasks | (Acc)| (Acc)| Dev Set (Acc)| (Acc)| (F1)\\n| - | - | - | - | - | - |\\n| BERTBASE | 84.4 | 88.4 | 86.7 | 92.7 | 88.5 |\\n| No NSP | 83.9 | 84.9 | 86.5 | 92.6 | 87.9 |\\n| LTR & No NSP | 82.1 | 84.3 | 77.5 | 92.1 | 77.8 |\\n| + BİLSTM | 82.1 | 84.1 | 75.7 | 91.6 | 84.9 |  \\nTable 5: Ablation over the pre-training tasks using the BERTBASE architecture. \"No NSP\" is trained without the next sentence prediction task. \"LTR & No NSP\" is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. \"+ BiLSTM\" adds a ran- domly initialized BiLSTM on top of the \"LTR + No NSP\" model during fine-tuning.  \\nablation studies can be found in Appendix C.', metadata={'Header 1': '5 Ablation Studies'}),\n",
       " Document(page_content='We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using exactly the same pre- training data, fine-tuning scheme, and hyperpa- rameters as BERTBASE:  \\nNo NSP: A bidirectional model which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.  \\nLTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input repre- sentation, and our fine-tuning scheme.  \\nWe first examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQUAD 1.1. Next, we evaluate the impact of training bidirectional representations by com- paring \"No NSP\" to \"LTR & No NSP\". The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQUAD.  \\nFor SQUAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no right- side context. In order to make a good faith at- tempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQUAD, but the  \\nresults are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks.  \\nWe recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two mod- els, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer.', metadata={'Header 1': '5 Ablation Studies', 'Header 2': '5.1 Effect of Pre-training Tasks'}),\n",
       " Document(page_content='In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training pro- cedure as described previously.  \\nResults on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict ac- curacy improvement across all four datasets, even for MRPC which only has 3,600 labeled train- ing examples, and is substantially different from the pre-training tasks. It is also perhaps surpris- ing that we are able to achieve such significant improvements on top of models which are al- ready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. (2017) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters (Al-Rfou et al., 2018). By contrast, BERTBASE contains 110M parameters and BERTLARGE con- tains 340M parameters.  \\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the first work to demonstrate convinc- ingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been suffi- ciently pre-trained. Peters et al. (2018b) presented\\nMNLI-m QNLI MRPC SST-2 SQUAD     |\\nmixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. (2016) men- tioned in passing that increasing hidden dimen- sion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improve- ments. Both of these prior works used a feature- based approach - we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of ran- domly initialized additional parameters, the task- specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.', metadata={'Header 1': '5 Ablation Studies', 'Header 2': '5.2 Effect of Model Size'}),\n",
       " Document(page_content='All of the BERT results presented so far have used the fine-tuning approach, where a simple classifi- cation layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a down- stream task. However, the feature-based approach, where fixed features are extracted from the pre- trained model, has certain advantages. First, not all tasks can be easily represented by a Trans- former encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation.  \\nIn this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang and De Meulder, 2003). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we for- mulate this as a tagging task but do not use a CRF  \\n| Hyperparams |||| Dev Set Accuracy |||\\n| #L |||||| #H #A LM (ppl) MNLI-m MRPC SST-2      |\\n| - | - | - | - | - | - | - |\\n| 3 | 768 | 12 | 5.84 | 77.9 | 79.8 | 88.4 |\\n| 6 | 768 | 3 | 5.24 | 80.6 | 82.2 | 90.7 |\\n| 6 | 768 | 12 | 4.68 | 81.9 | 84.8 | 91.3 |\\n| 12 | 768 | 12 | 3.99 | 84.4 | 86.7 | 92.9 |\\n|| 12 1024  | 16 | 3.54 | 85.7 | 86.9 | 93.3 |\\n|| 24 1024  | 16 | 3.23 | 86.6 | 87.8 | 93.7 |  \\nTable 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of at- tention heads. \"LM (ppl)\" is the masked LM perplexity of held-out training data.  \\n| System || Dev F1 Test F1  |\\n| - | - | - |\\n| ELMo (Peters et al., 2018a) | 95.7 | 92.2 |\\n| CVT (Clark et al., 2018) | - | 92.6 |\\n| CSE (Akbik et al., 2018) | - | 93.1 |\\n| Fine-tuning approach | | |\\n| BERTLARGE | 96.6 | 92.8 |\\n| BERT BASE | 96.4 | 92.4 |\\n| Feature-based approach (BERTBASE) | | |\\n| Embeddings | 91.0 | - |\\n| Second-to-Last Hidden | 95.6 | - |\\n| Last Hidden | 94.9 | - |\\n| Weighted Sum Last Four Hidden | 95.9 | - |\\n| Concat Last Four Hidden | 96.1 | - |\\n| Weighted Sum All 12 Layers | 95.5 | - |  \\nTable 7: CoNLL-2003 Named Entity Recognition re- sults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters.  \\nlayer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.  \\nTo ablate the fine-tuning approach, we apply the feature-based approach by extracting the activa- tions from one or more layers without fine-tuning any parameters of BERT. These contextual em- beddings are used as input to a randomly initial- ized two-layer 768-dimensional BiLSTM before the classification layer.  \\nResults are presented in Table 7. BERTLARGE performs competitively with state-of-the-art meth- ods. The best performing method concatenates the token representations from the top four hidden lay- ers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fine- tuning and feature-based approaches.', metadata={'Header 1': '5 Ablation Studies', 'Header 2': '5.3 Feature-based Approach with BERT'}),\n",
       " Document(page_content='Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architec- tures. Our major contribution is further general- izing these findings to deep bidirectional architec- tures, allowing the same pre-trained model to suc- cessfully tackle a broad set of NLP tasks.', metadata={'Header 1': '6 Conclusion'}),\n",
       " Document(page_content=\"Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638-1649.  \\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. arXiv preprint arXiv: 1808.04444.  \\nRie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817-1853.  \\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST.  \\nJohn Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In Proceedings of the 2006 confer- ence on empirical methods in natural language pro- cessing, pages 120-128. Association for Computa- tional Linguistics.  \\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Association for Computational Linguis- tics.  \\nPeter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467-479.  \\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez- Gazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1-14, Vancou- ver, Canada. Association for Computational Lin- guistics.  \\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robin- son. 2013. One billion word benchmark for measur- ing progress in statistical language modeling. arXiv preprint arXiv: 1312.3005.  \\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora question pairs.  \\nChristopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL.  \\nKevin Clark, Minh-Thang Luong, Christopher D Man- ning, and Quoc Le. 2018. Semi-supervised se- quence modeling with cross-view training. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 1914- 1925.  \\nRonan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Pro- ceedings of the 25th international conference on Machine learning, pages 160-167. ACM.  \\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Nat- ural Language Processing, pages 670-680, Copen- hagen, Denmark. Association for Computational Linguistics.  \\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural informa- tion processing systems, pages 3079-3087.  \\nJ. Deng, W. Dong, R. Socher, L .- J. Li, K. Li, and L. Fei- Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.  \\nWilliam B Dolan and Chris Brockett. 2005. Automati- cally constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).  \\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the \\\\_. arXiv preprint arXiv: 1801.07736.  \\nDan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaus- sian error linear units. CoRR, abs/1606.08415.  \\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics.  \\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In ACL. Association for Computational Linguistics.  \\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehen- sion. In IJCAI.  \\nYacine Jernite, Samuel R. Bowman, and David Son- tag. 2017. Discourse-based objectives for fast un- supervised sentence representation learning. CoRR, abs/1705.00557.\\n:selected:\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In ACL.  \\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294-3302.  \\nQuoc Le and Tomas Mikolov. 2014. Distributed rep- resentations of sentences and documents. In Inter- national Conference on Machine Learning, pages 1188-1196.  \\nHector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.  \\nLajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence represen- tations. In International Conference on Learning Representations.  \\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Con- textualized word vectors. In NIPS.  \\nOren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context em- bedding with bidirectional LSTM. In CoNLL.  \\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in Neural Information Processing Systems 26, pages 3111-3119. Curran Associates, Inc.  \\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal- able hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot- tou, editors, Advances in Neural Information Pro- cessing Systems 21, pages 1081-1088. Curran As- sociates, Inc.  \\nAnkur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP.  \\nJeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532- 1543.  \\nMatthew Peters, Waleed Ammar, Chandra Bhagavat- ula, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In ACL.  \\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word rep- resentations. In NAACL.  \\nMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 1499-1509.  \\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under- standing with unsupervised learning. Technical re- port, OpenAI.  \\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Nat- ural Language Processing, pages 2383-2392.  \\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.  \\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642.  \\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv: 1810.06638.  \\nWilson L Taylor. 1953. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):415-433.  \\nErik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.  \\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Compu- tational Linguistics, ACL '10, pages 384-394.  \\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000-6010.  \\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings of the 25th international conference on Machine learning, pages 1096-1103. ACM.  \\nAlex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform  \\nfor natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: An- alyzing and Interpreting Neural Networks for NLP, pages 353-355.  \\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi- granularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers). Association for Computational Lin- guistics.  \\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow- man. 2018. Neural network acceptability judg- ments. arXiv preprint arXiv: 1805.12471.  \\nAdina Williams, Nikita Nangia, and Samuel R Bow- man. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.  \\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv: 1609.08144.  \\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320-3328.  \\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehen- sion. In ICLR.  \\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceed- ings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).  \\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27.\", metadata={'Header 1': 'References'}),\n",
       " Document(page_content='We organize the appendix into three sections:  \\n· Additional implementation details for BERT are presented in Appendix A;  \\n· Additional details for our experiments are presented in Appendix B; and  \\n· Additional ablation studies are presented in Appendix C.  \\nWe present additional ablation studies for BERT including:  \\n\\\\- Effect of Number of Training Steps; and  \\n\\\\- Ablation for Different Masking Proce- dures.', metadata={'Header 1': 'Appendix for \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"'}),\n",
       " Document(page_content='We provide examples of the pre-training tasks in the following.  \\nMasked LM and the Masking Procedure As- suming the unlabeled sentence is my dog is hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further il- lustrated by  \\n. 80% of the time: Replace the word with the [MASK] token, e.g., my dog is hairy -> my dog is [MASK]  \\n. 10% of the time: Replace the word with a random word, e.g., my dog is hairy -> my dog is apple  \\n· 10% of the time: Keep the word un- changed, e.g., my dog is hairy -> my dog is hairy. The purpose of this is to bias the representation towards the actual observed word.  \\nThe advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been re- placed by random words, so it is forced to keep a distributional contextual representation of ev- ery input token. Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model\\'s language understanding capability. In Section C.2, we evaluate the impact this proce- dure.  \\nCompared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model\\n:unselected: :unselected:<figure>  \\n![](figures/2)  \\n<!-- FigureContent=\"BERT (Ours) OpenAI GPT ELMo T. T, --- T, T. ... TN T. T, TA Trm Trm --- Trm Trn Trm Trm Lstm Lstm Lstm Lstm Lstm + Lstm 4 Trm Trm ... Trm Trm Trm ... Trm Lstm Lstm Lstm Lstm Lstm Lstm E, E2 EN E, E2 EN ... E, E ...\" -->  \\n<figcaption>  \\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\\nOpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.  \\n</figcaption>  \\n</figure>  \\nto converge. In Section C.1 we demonstrate that MLM does converge marginally slower than a left- to-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost.  \\nNext Sentence Prediction The next sentence prediction task can be illustrated in the following examples.  \\nInput = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext  \\nInput = [CLS] the man [MASK] to the store [SEP]  \\npenguin [MASK] are flight ##less birds [SEP] Label = NotNext', metadata={'Header 1': 'A Additional Details for BERT', 'Header 2': 'A.1 Illustration of the Pre-training Tasks'}),\n",
       " Document(page_content='To generate each training input sequence, we sam- ple two spans of text from the corpus, which we refer to as \"sentences\" even though they are typ- ically much longer than single sentences (but can be shorter also). The first sentence receives the A embedding and the second receives the B embed- ding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the \"next sentence pre- diction\" task. They are sampled such that the com- bined length is ≤ 512 tokens. The LM masking is applied after WordPiece tokenization with a uni- form masking rate of 15%, and no special consid- eration given to partial word pieces.  \\nWe train with batch size of 256 sequences (256 sequences \\\\* 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40  \\nepochs over the 3.3 billion word corpus. We use Adam with learning rate of le-4, 31 = 0.9, 32 = 0.999, L2 weight decay of 0.01, learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We use a dropout prob- ability of 0.1 on all layers. We use a gelu acti- vation (Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.  \\nTraining of BERTBASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre- training took 4 days to complete.  \\nLonger sequences are disproportionately expen- sive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.', metadata={'Header 1': 'A Additional Details for BERT', 'Header 2': 'A.2 Pre-training Procedure'}),\n",
       " Document(page_content='For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of train- ing epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-specific, but we found the following range of possible values to work well across all tasks:  \\n· Batch size: 16, 32  \\n<!-- Footnote=\"13https://cloudplatform.googleblog.com/2018/06/Cloud- TPU-now-offers-preemptible-pricing-and-global- availability.html\" -->\\n:selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected: :selected:\\n· Learning rate (Adam): 5e-5, 3e-5, 2e-5  \\n· Number of epochs: 2, 3, 4  \\nWe also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets. Fine-tuning is typically very fast, so it is rea- sonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.', metadata={'Header 1': 'A Additional Details for BERT', 'Header 2': 'A.3 Fine-tuning Procedure'}),\n",
       " Document(page_content='Here we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons be- tween the model architectures are shown visually in Figure 3. Note that in addition to the architec- ture differences, BERT and OpenAI GPT are fine- tuning approaches, while ELMo is a feature-based approach.  \\nThe most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text cor- pus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained:  \\n. GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words).  \\n· GPT uses a sentence separator ( [ SEP ] ) and classifier token ( [CLS]) which are only in- troduced at fine-tuning time; BERT learns [SEP ], [CLS] and sentence A/B embed- dings during pre-training.  \\n· GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.  \\n· GPT used the same learning rate of 5e-5 for all fine-tuning experiments; BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set.  \\nTo isolate the effect of these differences, we per- form ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable.', metadata={'Header 1': 'A Additional Details for BERT', 'Header 2': 'A.3 Fine-tuning Procedure', 'Header 3': 'A.4 Comparison of BERT, ELMo ,and OpenAI GPT'}),\n",
       " Document(page_content='The illustration of fine-tuning BERT on different tasks can be seen in Figure 4. Our task-specific models are formed by incorporating BERT with one additional output layer, so a minimal num- ber of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks. In the figure, E represents the input embedding, Ti represents the contextual representation of token i, [CLS] is the special symbol for classification out- put, and [SEP] is the special symbol to separate non-consecutive token sequences.', metadata={'Header 1': 'A Additional Details for BERT', 'Header 2': 'A.3 Fine-tuning Procedure', 'Header 3': 'A.5 Illustrations of Fine-tuning on Different Tasks'}),\n",
       " Document(page_content='Our GLUE results in Table1 are obtained from https://gluebenchmark.com/ leaderboard and https://blog. openai.com/language-unsupervised. The GLUE benchmark includes the following datasets, the descriptions of which were originally summarized in Wang et al. (2018a):  \\nMNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classifi- cation task (Williams et al., 2018). Given a pair of sentences, the goal is to predict whether the sec- ond sentence is an entailment, contradiction, or neutral with respect to the first one.  \\nQQP Quora Question Pairs is a binary classifi- cation task where the goal is to determine if two questions asked on Quora are semantically equiv- alent (Chen et al., 2018).  \\nQNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset (Rajpurkar et al., 2016) which has been converted to a binary classification task (Wang et al., 2018a). The positive examples are (ques- tion, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer.\\n<figure>  \\n![](figures/3)  \\n<!-- FigureContent=\"Class Class Label Label C T. ... TA T, [SEP T,\\' ... T ... IM C T, :unselected: :unselected: BERT BERT ... E[CLS] E, ... EN E. [SEP] E,\\' E.\\' M ... ETCLS] E, E2 ... EN [CLS] Tok Tok Tok Tok [CLS] Tok 1 Tok N 1 ... N [SEP] 1 ... M Tok 2 ... Sentence 1 Sentence 2 Single Sentence (a) Sentence Pair Classification Tasks: MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG (b) Single Sentence Classification Tasks: SST-2, CoLA Start/End Span O :unselected: B-PER O C ’ T, ... TN TISEPT T,\\' T. IM C T. T. 2 :unselected: :unselected: BERT BERT E CLS] E, ... EN EISEP E, E.\\' ... E[CLS] E, E2 EN [CLS] Tok Tok SEP] Tok Tok M [CLS] Tok 1 Tok 2 Tok N 1 ... N 1 ... Question Paragraph Single Sentence (c) Question Answering Tasks: (d) Single Sentence Tagging Tasks: CONLL-2003 NER SQUAD v1.1\" -->  \\n<figcaption>  \\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.  \\n</figcaption>  \\n</figure>  \\nSST-2 The Stanford Sentiment Treebank is a binary single-sentence classification task consist- ing of sentences extracted from movie reviews with human annotations of their sentiment (Socher et al., 2013).  \\nCoLA The Corpus of Linguistic Acceptability is a binary single-sentence classification task, where the goal is to predict whether an English sentence is linguistically \"acceptable\" or not (Warstadt et al., 2018).  \\nSTS-B The Semantic Textual Similarity Bench- mark is a collection of sentence pairs drawn from news headlines and other sources (Cer et al., 2017). They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning.  \\nMRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations  \\nfor whether the sentences in the pair are semanti- cally equivalent (Dolan and Brockett, 2005).  \\nRTE Recognizing Textual Entailment is a bi- nary entailment task similar to MNLI, but with much less training data (Bentivogli et al., 2009).14  \\nWNLI Winograd NLI is a small natural lan- guage inference dataset (Levesque et al., 2011). The GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\\'s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore ex- clude this set to be fair to OpenAI GPT. For our GLUE submission, we always predicted the ma-  \\n<!-- Footnote=\"14 Note that we only report single-task fine-tuning results in this paper. A multitask fine-tuning approach could poten- tially push the performance even further. For example, we did observe substantial improvements on RTE from multi- task training with MNLI.\" -->  \\n<!-- Footnote=\"15https://gluebenchmark.com/faq\" -->  \\njority class.', metadata={'Header 1': 'B Detailed Experimental Setup', 'Header 2': 'B.1 Detailed Descriptions for the GLUE Benchmark Experiments.'}),\n",
       " Document(page_content='Figure 5 presents MNLI Dev accuracy after fine- tuning from a checkpoint that has been pre-trained for k steps. This allows us to answer the following questions:  \\n1\\\\. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch \\\\* 1,000,000 steps) to achieve high fine-tuning accuracy?  \\nAnswer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps.  \\n2\\\\. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word?  \\nAnswer: The MLM model does converge slightly slower than the LTR model. How- ever, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately.', metadata={'Header 1': 'C Additional Ablation Studies', 'Header 2': 'C.1 Effect of Number of Training Steps'}),\n",
       " Document(page_content='In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies.  \\n<figure>  \\n![](figures/4)  \\n<!-- FigureContent=\"84 MNLI Dev Accuracy 82 80 X 1 78 A-BERTBASE (Masked LM) 76 X-BERTBASE (Left-to-Right) 200 400 600 800 1,000 Pre-training Steps (Thousands)\" -->  \\n<figcaption>  \\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after fine-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.  \\n</figcaption>  \\n</figure>  \\nNote that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never ap- pears during the fine-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both fine-tuning and feature-based ap- proaches, as we expect the mismatch will be am- plified for the feature-based approach as the model will not have the chance to adjust the representa- tions.  \\n| Masking Rates ||| Dev Set Results |||\\n|| MASK SAME  | RND | MNLI| NER ||\\n|||||| Fine-tune Fine-tune Feature-based   |\\n| - | - | - | - | - | - |\\n| 80% | 10% | 10% | 84.2 | 95.4 | 94.9 |\\n| 100% | 0% | 0% | 84.3 | 94.9 | 94.0 |\\n| 80% | 0% | 20% | 84.1 | 95.2 | 94.6 |\\n| 80% | 20% | 0% | 84.4 | 95.2 | 94.7 |\\n| 0% | 20% | 80% | 83.7 | 94.8 | 94.6 |\\n| 0% || 0% 100%  | 83.6 | 94.9 | 94.6 |  \\nTable 8: Ablation over different masking strategies.  \\nThe results are presented in Table 8. In the table, MASK means that we replace the target token with the [MASK] symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token.  \\nThe numbers in the left part of the table repre- sent the probabilities of the specific strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3.  \\nFrom the table it can be seen that fine-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strat- egy was problematic when applying the feature- based approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.\\n:selected: :selected: :selected: :selected: :selected: :unselected: :unselected: :unselected: :unselected: :unselected:', metadata={'Header 1': 'C Additional Ablation Studies', 'Header 2': 'C.2 Ablation for Different Masking Procedures'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_embeddings = AzureOpenAIEmbeddings(azure_endpoint=os.getenv(\"OpenAiEndPoint\"), \n",
    "                      azure_deployment=os.getenv(\"OpenAiEmbedding\"), openai_api_version=os.getenv(\"OpenAiVersion\"),\n",
    "                      api_key=os.getenv(\"OpenAiKey\"))\n",
    "\n",
    "SearchService = os.getenv(\"SearchService\")\n",
    "vector_store_address: str = f\"https://{SearchService}.search.windows.net\"\n",
    "vector_store_password: str = os.getenv(\"SearchKey\")\n",
    "\n",
    "index_name: str = \"4a85d6d20874476ea5c599615cafbc26\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    ")\n",
    "#vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant chunks based on the question\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "# Use a prompt for RAG that is checked into the LangChain prompt hub (https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=989ad331-949f-4bac-9694-660074a208a7)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"OpenAiEndPoint\"),\n",
    "    api_version=os.getenv(\"OpenAiVersion\"),\n",
    "    azure_deployment=os.getenv(\"OpenAiChat\"),\n",
    "    api_key=os.getenv(\"OpenAiKey\"),\n",
    "    temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copilot Studio allows users to create, test, and deploy conversational copilots. It includes advanced capabilities of Azure Copilot Framework natively. Users can build custom copilots, extend Microsoft Copilot, and create Al-powered apps and solutions beyond copilot.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a question about the document\n",
    "\n",
    "rag_chain.invoke(\"What are some features of Copilot Studio?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the retrieved documents or certain source metadata from the documents\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableMap(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'Header 1': '· Azure Machine Learning',\n",
       "   'Header 2': 'Copilot building studio'},\n",
       "  {'Header 1': '· Azure Machine Learning', 'Header 2': 'Copilot Studio'},\n",
       "  {}],\n",
       " 'answer': 'Copilot Studio allows users to create, test, and deploy conversational copilots. It includes advanced capabilities of Azure Copilot Framework natively. Users can build their own Copilot with a comprehensive end-to-end AI toolchain.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a question about the document\n",
    "\n",
    "answer = rag_chain_with_source.invoke(\"What are some features of Copilot Studio?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copilot Studio allows users to create, test, and deploy conversational copilots. It includes advanced capabilities of Azure Copilot Framework natively. Users can build their own Copilot with the most comprehensive end-to-end AI toolchain.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
