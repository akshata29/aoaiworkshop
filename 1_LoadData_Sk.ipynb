{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Index Data using Semantic Kernel - Vector Store\n",
    "\n",
    "We will use [Azure Cognitive Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) to load and index the data.  Azure Cognitive Search is a cloud search service with built-in AI capabilities that enrich all types of information to easily identify and explore relevant content at scale. It uses the same integrated Microsoft natural language stack that Bing and Office have used for more than a decade, and AI services across vision, language, and speech, to deliver knowledge from structured and unstructured data.\n",
    "\n",
    "Cognitive search enabled the vector search feature! When done correctly, vector search is a proven technique for significantly increasing the semantic relevance of search results.  It is a technique that uses machine learning to embed text into a vector space, where the distance between vectors is a measure of semantic similarity.  This allows for the use of vector similarity search to find relevant results.  [Sign up]\n",
    "(https://aka.ms/VectorSearchSignUp) for Private Preview of Vector Search.\n",
    "\n",
    "Cognitive Search can index and store vectors, but it doesn't generate them out of the box. The documents that you push to your search service must contain vectors within the payload. Alternatively, you can use the Indexer to pull vectors from your data sources such as Blob Storage JSON files or CSVs. You can also use a Custom Skill to generate embeddings as part of the AI Enrichment process.\n",
    "\n",
    "\n",
    "[Sample repo](https://github.com/Azure/cognitive-search-vector-pr) to get started with vector search. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisite:\n",
    "- To run the code, install the following packages from local Wheel file. Alternatively, install azure-search-documents==11.4.0a20230509004 from the Dev Feed. For instructions on how to connect to the dev feed, please visit Azure-Python-SDK Azure Search Documents [Dev Feed](https://dev.azure.com/azure-sdk/public/_artifacts/feed/azure-sdk-for-python/connect/pip).\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/).\n",
    "- An Azure Cognitive Search service (any tier, any region). [Create a service](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) or find an [existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ./azure_search_documents-11.4.0b4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install semenatic kernel\n",
    "#%pip install semantic-kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = \"skindex\"\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"{OpenAiEndPoint}\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "openai.api_base = openAiEndPoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding, AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureTextCompletion,\n",
    "    AzureTextEmbedding,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import (\n",
    "    AzureCognitiveSearchMemoryStore,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the PDF, create the chunk and push to Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexibility to change the call to OpenAI or Azure OpenAI\n",
    "embeddingModelType = \"azureopenai\"\n",
    "#AZURE_COGNITIVE_SEARCH_ENDPOINT = SearchService\n",
    "#AZURE_COGNITIVE_SEARCH_ADMIN_KEY = SearchKey\n",
    "#AZURE_OPENAI_API_KEY = OpenAiKey\n",
    "#AZURE_OPENAI_ENDPOINT = OpenAiEndPoint\n",
    "#AZURE_OPENAI_DEPLOYMENT_NAME = OpenAiChat\n",
    "vectorSize = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if embeddingModelType == \"azureopenai\":\n",
    "    #deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat_completion\", AzureChatCompletion(OpenAiChat, OpenAiEndPoint, OpenAiKey))\n",
    "    # next line assumes embeddings deployment name is \"text-embedding-ada-002\", adjust this if  appropriate \n",
    "    kernel.add_text_embedding_generation_service(\"ada\", AzureTextEmbedding(deployment_name=OpenAiEmbedding,\n",
    "            endpoint=OpenAiEndPoint,\n",
    "            api_key=OpenAiKey))\n",
    "    kernel.add_text_completion_service(\n",
    "        \"dv\",\n",
    "        AzureTextCompletion(\n",
    "            deployment_name=OpenAiEmbedding,\n",
    "            endpoint=OpenAiEndPoint,\n",
    "            api_key=OpenAiKey,\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    #api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", OpenAiApiKey, \"\"))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", OpenAiApiKey, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "# kernel.import_skill(sk.core_skills.TextMemorySkill())\n",
    "connector = AzureCognitiveSearchMemoryStore(\n",
    "        vector_size=vectorSize, search_endpoint=f\"https://{SearchService}.search.windows.net\", admin_key=SearchKey\n",
    "    )\n",
    "# Register the memory store with the kernel\n",
    "kernel.register_memory_store(memory_store=connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file name and the namespace for the index\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.utils import open_filename\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PDFMinerLoader\n",
    ")\n",
    "\n",
    "# Change the code below to use it as \"Semantic Kernel way\"\n",
    "fileName = \"Fabric Get Started.pdf\"\n",
    "fabricGetStartedPath = \"Data/PDF/\" + fileName\n",
    "loader = PDFMinerLoader(fabricGetStartedPath)\n",
    "rawDocs = loader.load()\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "docs = textSplitter.split_documents(rawDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate thru the pandas dataframe and embed each row and save into memory (Chroma)\n",
    "async def populateCogSearch(kernel: sk.Kernel, docs, fileName) -> None:\n",
    "    # Add some documents to the semantic memory using save_information_async\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        await kernel.memory.save_information_async(\n",
    "        collection=indexName, \n",
    "        id=f\"{fileName}-{counter}\".replace(\".\", \"_\").replace(\" \", \"_\").replace(\":\", \"_\").replace(\"/\", \"_\").replace(\",\", \"_\").replace(\"&\", \"_\"),\n",
    "        text=doc.page_content)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hnsw_parameters is not a known attribute of class <class 'azure.search.documents.indexes._generated.models._models_py3.HnswVectorSearchAlgorithmConfiguration'> and will be ignored\n"
     ]
    }
   ],
   "source": [
    "await populateCogSearch(kernel, docs, fileName)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def searchCogDb(kernel: sk.Kernel, question, k, relevanceScore):\n",
    "    result = await kernel.memory.search_async(collection=indexName, query=question, limit=k, min_relevance_score=relevanceScore)\n",
    "    return {result[0].text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: {\"Fabric allows creators to concentrate on producing their best work, freeing them from\\n\\nthe need to integrate, manage, or understand the underlying infrastructure that\\n\\nsupports the experience.\\n\\nComponents of Microsoft Fabric\\n\\nMicrosoft Fabric offers the comprehensive set of analytics experiences designed to work\\n\\ntogether seamlessly. Each experience is tailored to a specific persona and a specific task.\\n\\nFabric includes industry-leading experiences in the following categories for an end-to-\\n\\nend analytical need.\\n\\nData Engineering - Data Engineering experience provides a world class Spark\\n\\nplatform with great authoring experiences, enabling data engineers to perform\\n\\nlarge scale data transformation and democratize data through the lakehouse.\\n\\nMicrosoft Fabric Spark's integration with Data Factory enables notebooks and\\n\\nspark jobs to be scheduled and orchestrated. For more information, see What is\\n\\nData engineering in Microsoft Fabric?\\n\\n\\x0cData Factory - Azure Data Factory combines the simplicity of Power Query with the\\n\\nscale and power of Azure Data Factory. You can use more than 200 native\\n\\nconnectors to connect to data sources on-premises and in the cloud. For more\\n\\ninformation, see What is Data Factory in Microsoft Fabric?\\n\\nData Science - Data Science experience enables you to build, deploy, and\\n\\noperationalize machine learning models seamlessly within your Fabric experience.\\n\\nIt integrates with Azure Machine Learning to provide built-in experiment tracking\"}\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"What is Microsoft Fabric\"  \n",
    "results = await searchCogDb(kernel, query, 3, 0.3)\n",
    "print(f\"Id: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: {\"Tell us about your PDF experience.\\n\\nMicrosoft Fabric get started\\ndocumentation\\n\\nMicrosoft Fabric is a unified platform that can meet your organization's data and\\nanalytics needs. Discover the Fabric shared and platform documentation from this page.\\n\\nAbout Microsoft Fabric\\n\\nｅ OVERVIEW\\n\\nWhat is Fabric?\\n\\nFabric terminology\\n\\nｂ GET STARTED\\n\\nStart a Fabric trial\\n\\nFabric home navigation\\n\\nEnd-to-end tutorials\\n\\nContext sensitive Help pane\\n\\nGet started with Fabric items\\n\\nｐ CONCEPT\\n\\nFind items in OneLake data hub\\n\\nPromote and certify items\\n\\nｃ HOW-TO GUIDE\\n\\nApply sensitivity labels\\n\\nWorkspaces\\n\\nｐ CONCEPT\\n\\nFabric workspace\\n\\n\\x0cWorkspace roles\\n\\nｂ GET STARTED\\n\\nCreate a workspace\\n\\nｃ HOW-TO GUIDE\\n\\nWorkspace access control\\n\\n\\x0cWhat is Microsoft Fabric?\\n\\nArticle • 05/23/2023\\n\\nMicrosoft Fabric is an all-in-one analytics solution for enterprises that covers everything\\n\\nfrom data movement to data science, Real-Time Analytics, and business intelligence. It\\n\\noffers a comprehensive suite of services, including data lake, data engineering, and data\\n\\nintegration, all in one place.\\n\\nWith Fabric, you don't need to piece together different services from multiple vendors.\\n\\nInstead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is\\n\\ndesigned to simplify your analytics needs.\\n\\nThe platform is built on a foundation of Software as a Service (SaaS), which takes\\n\\nsimplicity and integration to a whole new level.\\n\\n） Important\"}\n"
     ]
    }
   ],
   "source": [
    "# Vector Search with Multi-language support\n",
    "query = \"¿Qué es Microsoft Fabric?\"\n",
    "results = await searchCogDb(kernel, query, 3, 0.6)\n",
    "print(f\"Id: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
