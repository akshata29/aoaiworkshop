{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Kernel\n",
    "[Semantic Kernel](https://github.com/microsoft/semantic-kernel/tree/main)  is an SDK that integrates Large Language Models (LLMs) like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. Semantic Kernel achieves this by allowing you to define plugins that can be chained together in just a few lines of code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering over the docs/index\n",
    "Question answering in this context refers to question answering over your document data.  For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = \"skindex\"\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"{OpenAiEndPoint}\"\n",
    "assert openAiEndPoint, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "openai.api_base = openAiEndPoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answer for a question from the document we already indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding, AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureTextCompletion,\n",
    "    AzureTextEmbedding,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import (\n",
    "    AzureCognitiveSearchMemoryStore,\n",
    ")\n",
    "embeddingModelType = \"azureopenai\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if embeddingModelType == \"azureopenai\":\n",
    "    #deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat_completion\", AzureChatCompletion(OpenAiChat, OpenAiEndPoint, OpenAiKey))\n",
    "    # next line assumes embeddings deployment name is \"text-embedding-ada-002\", adjust this if  appropriate \n",
    "    kernel.add_text_embedding_generation_service(\"ada\", AzureTextEmbedding(deployment_name=OpenAiEmbedding,\n",
    "            endpoint=OpenAiEndPoint,\n",
    "            api_key=OpenAiKey))\n",
    "    kernel.add_text_completion_service(\n",
    "        \"dv\",\n",
    "        AzureTextCompletion(\n",
    "            deployment_name=OpenAiEmbedding,\n",
    "            endpoint=OpenAiEndPoint,\n",
    "            api_key=OpenAiKey,\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    #api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", OpenAiApiKey, \"\"))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", OpenAiApiKey, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "# kernel.import_skill(sk.core_skills.TextMemorySkill())\n",
    "vectorSize = 1536\n",
    "connector = AzureCognitiveSearchMemoryStore(\n",
    "        vector_size=vectorSize, search_endpoint=f\"https://{SearchService}.search.windows.net\", admin_key=SearchKey\n",
    "    )\n",
    "# Register the memory store with the kernel\n",
    "kernel.register_memory_store(memory_store=connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets augment the LLM query with retrieval from the local vector DB with the RAG (Retrieval Augmented Generation) pattern\n",
    "# The prompt below should be self explanatory of what we are trying to do with this RAG pattern\n",
    "async def RagQnA(kernel, query, limit, relevanceScore, promptOverride=None) -> str:\n",
    "    # Step1: Retrieval: Get list of documents from local DB matching the query\n",
    "    docs = await kernel.memory.search_async(collection=indexName, query=query, limit=limit, min_relevance_score=relevanceScore)\n",
    "    # Step2: Augment: Construct the augmented prompt from the retrieved document. Retrieved docs separated by triple backticks to make it easy for LLM to instruct\n",
    "    qdocs = \"\\n```\\n\".join([docs[i].text for i in range(len(docs))])\n",
    "    \n",
    "    if promptOverride is None:\n",
    "        prompt = \"\"\"{{ $qdocs}} \n",
    "        \n",
    "        Question: Please query above documents delimited by triple backticks for {{ $query }} \n",
    "        and respond back with answer only from the above documents delimited by triple backticks.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = promptOverride\n",
    "    \n",
    "    # Step3: Generation: Generate a summary and markdown formatted output as requested in the prompt from the LLM API\n",
    "    summarize = kernel.create_semantic_function(prompt, temperature=0.0)\n",
    "    context_variables = sk.ContextVariables(variables={\n",
    "        \"qdocs\": qdocs,\n",
    "        \"query\": query\n",
    "    })\n",
    "    response = summarize(variables=context_variables)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. With Fabric, you don't need to piece together different services from multiple vendors. Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is designed to simplify your analytics needs. The platform is built on a foundation of Software as a Service (SaaS), which takes simplicity and integration to a whole new level.\n"
     ]
    }
   ],
   "source": [
    "# We already created our index and loaded the data, so we can skip that part. Let's try to ask a question:\n",
    "# Question answering involves fetching multiple documents, and then asking a question of them. \n",
    "# The LLM response will contain the answer to your question, based on the content of the documents.\n",
    "\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "result = await RagQnA(kernel, query,topK, 0.3)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. It is built on a foundation of Software as a Service (SaaS) and is designed to simplify analytics needs by providing a highly integrated and easy-to-use product.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "            If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "            QUESTION: {{ $query }}\n",
    "            =========\n",
    "            {{ $qdocs}}\n",
    "            =========\n",
    "            \"\"\"\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "result = await RagQnA(kernel, query,topK, 0.3, template)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<What are the components of Microsoft Fabric?>\n",
      "<What are the advantages of using Microsoft Fabric's SaaS foundation?>\n",
      "<Can you explain the PDF experience in Microsoft Fabric?>\n"
     ]
    }
   ],
   "source": [
    "# Let's generate followup questions\n",
    "followupTemplate = \"\"\"\n",
    "    Generate three very brief questions that the user would likely ask next.\n",
    "    Use double angle brackets to reference the questions, e.g. <What is Azure?>.\n",
    "    Try not to repeat questions that have already been asked.  Don't include the context in the answer.\n",
    "\n",
    "    Return the questions in the following format:\n",
    "    <>\n",
    "   \n",
    "    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n",
    "\n",
    "    {{ $qdocs}}\n",
    "    \"\"\"\n",
    "\n",
    "# We already created our index and loaded the data, so we can skip that part. Let's try to ask a question:\n",
    "# Question answering involves fetching multiple documents, and then asking a question of them. \n",
    "# The LLM response will contain the answer to your question, based on the content of the documents.\n",
    "\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "result = await RagQnA(kernel, query,topK, 0.3, followupTemplate)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about we ask a question for which the answer is not in the document we have indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I am an AI language model and I cannot generate jokes. However, I can assist you with any questions or information you may need.\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"Tell me a Joke\"\n",
    "result = await RagQnA(kernel, query,topK, 0.3)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we don't want to have LLM answer the question outside of the document we have indexed in Vector Store. We can use the custom prompt to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CEO of Microsoft is not mentioned in the provided documents.\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"Who is the CEO of Microsoft\"\n",
    "\n",
    "result = await RagQnA(kernel, query,topK, 0.3)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
