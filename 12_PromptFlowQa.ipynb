{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering - test, evaluation and experimentation\n",
    "\n",
    "We will walk you through how to use prompt flow Python SDK to test, evaluate and experiment with the \"Chat with PDF\" flow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create connections\n",
    "Connection in prompt flow is for managing settings of your application behaviors incl. how to talk to different services (Azure OpenAI for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fpdoaoaice (AzureOpenAI)\n",
      "dataaioaicg (CognitiveSearch)\n",
      "chatpdf (Custom)\n",
      "aoai (AzureOpenAI)\n",
      "aoaicg (CognitiveSearch)\n",
      "entaoai (Custom)\n",
      "llmops (Custom)\n"
     ]
    }
   ],
   "source": [
    "import promptflow\n",
    "\n",
    "pf = promptflow.PFClient()\n",
    "\n",
    "# List all the available connections\n",
    "for c in pf.connections.list():\n",
    "    print(c.name + \" (\" + c.type + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to have a connection named `entaoai` to run Q&A PromptFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"entaoai\",\n",
      "    \"module\": \"promptflow.connections\",\n",
      "    \"created_date\": \"2023-09-01T18:42:17.454175\",\n",
      "    \"last_modified_date\": \"2023-09-21T13:07:15.332505\",\n",
      "    \"type\": \"custom\",\n",
      "    \"configs\": {\n",
      "        \"OpenAiEmbedding\": \"embedding\",\n",
      "        \"OpenAiVersion\": \"2023-07-01-preview\",\n",
      "        \"OpenAiChat\": \"chat\",\n",
      "        \"OpenAiChat16k\": \"chat16k\",\n",
      "        \"OpenAiEndPoint\": \"https://dataaiapim.azure-api.net\",\n",
      "        \"CosmosEndpoint\": \"https://dataaichatgpt.documents.azure.com:443/\",\n",
      "        \"CosmosDatabase\": \"aoai\",\n",
      "        \"CosmosContainer\": \"chatgpt\",\n",
      "        \"PineconeEnv\": \"us-east-1-aws\",\n",
      "        \"VsIndexName\": \"oaiembed\",\n",
      "        \"RedisAddress\": \"dataairedis.southcentralus.azurecontainer.io\",\n",
      "        \"RedisPort\": \"6379\",\n",
      "        \"KbIndexName\": \"aoaikb\",\n",
      "        \"SearchService\": \"dataaioaicg\",\n",
      "        \"SynapseName\": \"dataaiazuresql.database.windows.net,1433\",\n",
      "        \"SynapsePool\": \"northwind\",\n",
      "        \"SynapseUser\": \"azureadmin\"\n",
      "    },\n",
      "    \"secrets\": {\n",
      "        \"OpenAiKey\": \"******\",\n",
      "        \"CosmosKey\": \"******\",\n",
      "        \"SearchKey\": \"******\",\n",
      "        \"PineconeKey\": \"******\",\n",
      "        \"RedisPassword\": \"******\",\n",
      "        \"SynapsePassword\": \"******\"\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-21 13:07:15,331][promptflow][WARNING] - Connection with name entaoai already exists. Updating it.\n"
     ]
    }
   ],
   "source": [
    "# Create the Cognitive search connection using CLI\n",
    "!pf connection create -f \"./promptflow/entaoai.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing connection\n",
      "name: entaoai\n",
      "module: promptflow.connections\n",
      "created_date: '2023-09-01T18:42:17.454175'\n",
      "last_modified_date: '2023-09-03T17:21:44.729985'\n",
      "type: custom\n",
      "configs:\n",
      "  OpenAiEmbedding: embedding\n",
      "  OpenAiVersion: 2023-07-01-preview\n",
      "  OpenAiChat: chat\n",
      "  OpenAiChat16k: chat16k\n",
      "  OpenAiEndPoint: https://dataaiapim.azure-api.net\n",
      "  CosmosEndpoint: https://dataaichatgpt.documents.azure.com:443/\n",
      "  CosmosDatabase: aoai\n",
      "  CosmosContainer: chatgpt\n",
      "  PineconeEnv: us-east-1-aws\n",
      "  VsIndexName: oaiembed\n",
      "  RedisAddress: dataairedis.southcentralus.azurecontainer.io\n",
      "  RedisPort: '6379'\n",
      "  KbIndexName: aoaikb\n",
      "  SearchService: dataaioaicg\n",
      "secrets:\n",
      "  OpenAiKey: '******'\n",
      "  CosmosKey: '******'\n",
      "  SearchKey: '******'\n",
      "  PineconeKey: '******'\n",
      "  RedisPassword: '******'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create needed connection\n",
    "from promptflow.entities import AzureOpenAIConnection, OpenAIConnection\n",
    "\n",
    "try:\n",
    "    conn_name = \"entaoai\"\n",
    "    conn = pf.connections.get(name=conn_name)\n",
    "    print(\"using existing connection\")\n",
    "except:\n",
    "    print(\"Create connection by uncommenting previous cell\")\n",
    "\n",
    "print(conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "Unknown input(s) of flow: {'answer': 'BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-15 16:26:39 -0500   35760 execution          INFO     Start to run 8 nodes with concurrency level 16.\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     INFO     Executing node parse_postBody. node run id: ddb7db5d-bcdc-4c28-917e-a5d86ae36a23_parse_postBody_0\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     INFO     Node parse_postBody completes.\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     INFO     Executing node create_llm. node run id: ddb7db5d-bcdc-4c28-917e-a5d86ae36a23_create_llm_0\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     INFO     Executing node embed_the_question. node run id: ddb7db5d-bcdc-4c28-917e-a5d86ae36a23_embed_the_question_0\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     WARNING  Output of create_llm is not json serializable, use str to store it.\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     INFO     Node create_llm completes.\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     INFO     Node embed_the_question completes.\n",
      "2023-09-15 16:26:39 -0500   35760 execution.flow     INFO     Executing node check_cache_answer. node run id: ddb7db5d-bcdc-4c28-917e-a5d86ae36a23_check_cache_answer_0\n",
      "2023-09-15 16:26:41 -0500   35760 execution.flow     INFO     [check_cache_answer in line 0 (index starts from 0)] stdout> Search index aoaikb already exists\n",
      "2023-09-15 16:26:41 -0500   35760 execution.flow     INFO     [check_cache_answer in line 0 (index starts from 0)] stdout> KB Search Count: 1\n",
      "2023-09-15 16:26:41 -0500   35760 execution.flow     INFO     Node check_cache_answer completes.\n",
      "2023-09-15 16:26:41 -0500   35760 execution.flow     INFO     Executing node followup_questions. node run id: ddb7db5d-bcdc-4c28-917e-a5d86ae36a23_followup_questions_0\n",
      "2023-09-15 16:26:41 -0500   35760 execution.flow     WARNING  Input 'llm' of followup_questions is not json serializable, use str to store it.\n",
      "2023-09-15 16:26:41 -0500   35760 execution.flow     INFO     Node followup_questions completes.\n",
      "{'output': {'values': [{'recordId': 0, 'data': {'data_points': ['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\n\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\n\\n9\\n1\\n0\\n2\\n\\ny\\na\\nM\\n4\\n2\\n\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n\\n2\\nv\\n5\\n0\\n8\\n4\\n0\\n.\\n0\\n1\\n8\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nAbstract\\n\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture modiﬁcations.\\n\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n\\n1\\n\\nIntroduction\\n\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce ﬁne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\n\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and ﬁne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciﬁc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The ﬁne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciﬁc parameters, and is trained on the\\ndownstream tasks by simply ﬁne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\n\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\n\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations\\nfrom Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a “masked lan-\\nguage model” (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\n\\n \\n \\n \\n \\n \\n \\n\\x0cword based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\nIn addi-\\ntrain a deep bidirectional Transformer.\\ntion to the masked language model, we also use\\na “next sentence prediction” task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n\\n• We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n\\n• We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciﬁc architectures.\\n\\n• BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n\\n2 Related Work\\n\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\nmost widely-used approaches in this section.\\n\\n2.1 Unsupervised Feature-based Approaches\\n\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods.\\nPre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiﬁcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\n\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\n\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n\\n2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).', 'Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n\\nFeature-based approach (BERTBASE)\\n\\nEmbeddings\\nSecond-to-Last Hidden\\nLast Hidden\\nWeighted Sum Last Four Hidden\\nConcat Last Four Hidden\\nWeighted Sum All 12 Layers\\n\\n95.7\\n-\\n-\\n\\n96.6\\n96.4\\n\\n91.0\\n95.6\\n94.9\\n95.9\\n96.1\\n95.5\\n\\n92.2\\n92.6\\n93.1\\n\\n92.8\\n92.4\\n\\n-\\n-\\n-\\n-\\n-\\n-\\n\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\n\\nlayer in the output. We use the representation of\\nthe ﬁrst sub-token as the input to the token-level\\nclassiﬁer over the NER label set.\\n\\nTo ablate the ﬁne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without ﬁne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classiﬁcation layer.\\n\\nResults are presented in Table 7. BERTLARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind ﬁne-tuning the entire model. This\\ndemonstrates that BERT is effective for both ﬁne-\\ntuning and feature-based approaches.\\n\\n6 Conclusion\\n\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to beneﬁt from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these ﬁndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.\\n\\n\\x0cReferences\\n\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\n2018. Contextual string embeddings for sequence\\nIn Proceedings of the 27th International\\nlabeling.\\nConference on Computational Linguistics, pages\\n1638–1649.\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817–1853.\\n\\nLuisa Bentivogli, Bernardo Magnini,\\n\\nIdo Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe ﬁfth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\n\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120–128. Association for Computa-\\ntional Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\n\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural\\nlanguage.\\nComputational linguistics, 18(4):467–479.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\nIn Proceedings\\ncrosslingual focused evaluation.\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017), pages 1–14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\n\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\n\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\n\\nQuora question pairs.\\n\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\n\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nSemi-supervised se-\\nning, and Quoc Le. 2018.\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing, pages 1914–\\n1925.\\n\\nRonan Collobert and Jason Weston. 2008. A uniﬁed\\narchitecture for natural language processing: Deep\\nIn Pro-\\nneural networks with multitask learning.\\nceedings of the 25th international conference on\\nMachine learning, pages 160–167. ACM.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670–680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\n\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079–3087.\\n\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nImageNet: A Large-Scale Hierarchical\\n\\nFei. 2009.\\nImage Database. In CVPR09.\\n\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\n\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via ﬁlling in\\nthe . arXiv preprint arXiv:1801.07736.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\n\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nIn Proceedings of the 2016\\nfrom unlabelled data.\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\n\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model ﬁne-tuning for text classiﬁcation. In\\nACL. Association for Computational Linguistics.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nReinforced\\nFuru Wei, and Ming Zhou. 2018.\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\n\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.\\n\\n\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\n\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors.\\nIn\\nAdvances in neural information processing systems,\\npages 3294–3302.\\n\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning, pages\\n1188–1196.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge.\\nIn\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\n\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nefﬁcient framework for learning sentence represen-\\nIn International Conference on Learning\\ntations.\\nRepresentations.\\n\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\n\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26, pages 3111–3119. Curran Associates,\\nInc.', '2.2 Unsupervised Fine-tuning Approaches\\n\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\n(Col-\\nbedding parameters from unlabeled text\\nlobert and Weston, 2008).\\n\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nﬁne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\nlanguage model-\\nLeft-to-right\\net al., 2018a).\\n\\n\\x0cFigure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\n\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n\\n2.3 Transfer Learning from Supervised Data\\n\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to ﬁne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n\\n3 BERT\\n\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and ﬁne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\n\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-\\n\\nmal difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\n\\nModel Architecture BERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\n\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\n\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n\\n1https://github.com/tensorﬂow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\n\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\n\\n4We note that in the literature the bidirectional Trans-\\n\\nBERTBERTE[CLS]E1 E[SEP]...ENE1’...EM’CT1T[SEP]...TNT1’...TM’[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1’...EM’CT1T[SEP]...TNT1’...TM’[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\x0cInput/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\nThroughout this work, a “sentence” can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A “sequence” refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\n\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The ﬁrst\\ntoken of every sequence is always a special clas-\\nsiﬁcation token ([CLS]). The ﬁnal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiﬁcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the ﬁnal hidden\\nvector of the special [CLS] token as C ∈ RH ,\\nand the ﬁnal hidden vector for the ith input token\\nas Ti ∈ RH .\\n\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n\\n3.1 Pre-training BERT\\n\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\n\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly “see itself”, and the model could trivially\\npredict the target word in a multi-layered context.\\n\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a “masked\\nLM” (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the ﬁnal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.'], 'answer': 'The main difference between BERT and previous language representation models is that BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. This allows BERT to capture more contextual information and improve performance on a wide range of tasks.', 'thoughts': '<br><br>Prompt:<br>Given the following extracted parts of a long document and a question, create a final answer. <br>        If you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer. <br>        If the answer is not contained within the text below, say \"I don\\'t know\".<br><br>        [\\'BERT: Pre-training of Deep Bidirectional Transformers for\\\\nLanguage Understanding\\\\n\\\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\\\nGoogle AI Language\\\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\\\n\\\\n9\\\\n1\\\\n0\\\\n2\\\\n\\\\ny\\\\na\\\\nM\\\\n4\\\\n2\\\\n\\\\n]\\\\nL\\\\nC\\\\n.\\\\ns\\\\nc\\\\n[\\\\n\\\\n2\\\\nv\\\\n5\\\\n0\\\\n8\\\\n4\\\\n0\\\\n.\\\\n0\\\\n1\\\\n8\\\\n1\\\\n:\\\\nv\\\\ni\\\\nX\\\\nr\\\\na\\\\n\\\\nAbstract\\\\n\\\\nWe introduce a new language representa-\\\\ntion model called BERT, which stands for\\\\nBidirectional Encoder Representations from\\\\nTransformers. Unlike recent language repre-\\\\nsentation models (Peters et al., 2018a; Rad-\\\\nford et al., 2018), BERT is designed to pre-\\\\ntrain deep bidirectional representations from\\\\nunlabeled text by jointly conditioning on both\\\\nleft and right context in all layers. As a re-\\\\nsult, the pre-trained BERT model can be ﬁne-\\\\ntuned with just one additional output layer\\\\nto create state-of-the-art models for a wide\\\\nrange of tasks, such as question answering and\\\\nlanguage inference, without substantial task-\\\\nspeciﬁc architecture modiﬁcations.\\\\n\\\\nBERT is conceptually simple and empirically\\\\npowerful.\\\\nIt obtains new state-of-the-art re-\\\\nsults on eleven natural language processing\\\\ntasks, including pushing the GLUE score to\\\\n80.5% (7.7% point absolute improvement),\\\\nMultiNLI accuracy to 86.7% (4.6% absolute\\\\nimprovement), SQuAD v1.1 question answer-\\\\ning Test F1 to 93.2 (1.5 point absolute im-\\\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point absolute improvement).\\\\n\\\\n1\\\\n\\\\nIntroduction\\\\n\\\\nLanguage model pre-training has been shown to\\\\nbe effective for improving many natural language\\\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\\\n2018a; Radford et al., 2018; Howard and Ruder,\\\\n2018). These include sentence-level tasks such as\\\\nnatural language inference (Bowman et al., 2015;\\\\nWilliams et al., 2018) and paraphrasing (Dolan\\\\nand Brockett, 2005), which aim to predict the re-\\\\nlationships between sentences by analyzing them\\\\nholistically, as well as token-level tasks such as\\\\nnamed entity recognition and question answering,\\\\nwhere models are required to produce ﬁne-grained\\\\noutput at the token level (Tjong Kim Sang and\\\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\\\n\\\\nThere are two existing strategies for apply-\\\\ning pre-trained language representations to down-\\\\nstream tasks: feature-based and ﬁne-tuning. The\\\\nfeature-based approach, such as ELMo (Peters\\\\net al., 2018a), uses task-speciﬁc architectures that\\\\ninclude the pre-trained representations as addi-\\\\ntional features. The ﬁne-tuning approach, such as\\\\nthe Generative Pre-trained Transformer (OpenAI\\\\nGPT) (Radford et al., 2018), introduces minimal\\\\ntask-speciﬁc parameters, and is trained on the\\\\ndownstream tasks by simply ﬁne-tuning all pre-\\\\ntrained parameters. The two approaches share the\\\\nsame objective function during pre-training, where\\\\nthey use unidirectional language models to learn\\\\ngeneral language representations.\\\\n\\\\nWe argue that current techniques restrict the\\\\npower of the pre-trained representations, espe-\\\\ncially for the ﬁne-tuning approaches. The ma-\\\\njor limitation is that standard language models are\\\\nunidirectional, and this limits the choice of archi-\\\\ntectures that can be used during pre-training. For\\\\nexample, in OpenAI GPT, the authors use a left-to-\\\\nright architecture, where every token can only at-\\\\ntend to previous tokens in the self-attention layers\\\\nof the Transformer (Vaswani et al., 2017). Such re-\\\\nstrictions are sub-optimal for sentence-level tasks,\\\\nand could be very harmful when applying ﬁne-\\\\ntuning based approaches to token-level tasks such\\\\nas question answering, where it is crucial to incor-\\\\nporate context from both directions.\\\\n\\\\nIn this paper, we improve the ﬁne-tuning based\\\\napproaches by proposing BERT: Bidirectional\\\\nEncoder Representations\\\\nfrom Transformers.\\\\nBERT alleviates the previously mentioned unidi-\\\\nrectionality constraint by using a “masked lan-\\\\nguage model” (MLM) pre-training objective, in-\\\\nspired by the Cloze task (Taylor, 1953). The\\\\nmasked language model randomly masks some of\\\\nthe tokens from the input, and the objective is to\\\\npredict the original vocabulary id of the masked\\\\n\\\\n \\\\n \\\\n \\\\n \\\\n \\\\n \\\\n\\\\x0cword based only on its context. Unlike left-to-\\\\nright language model pre-training, the MLM ob-\\\\njective enables the representation to fuse the left\\\\nand the right context, which allows us to pre-\\\\nIn addi-\\\\ntrain a deep bidirectional Transformer.\\\\ntion to the masked language model, we also use\\\\na “next sentence prediction” task that jointly pre-\\\\ntrains text-pair representations. The contributions\\\\nof our paper are as follows:\\\\n\\\\n• We demonstrate the importance of bidirectional\\\\npre-training for language representations. Un-\\\\nlike Radford et al. (2018), which uses unidirec-\\\\ntional language models for pre-training, BERT\\\\nuses masked language models to enable pre-\\\\ntrained deep bidirectional representations. This\\\\nis also in contrast to Peters et al. (2018a), which\\\\nuses a shallow concatenation of independently\\\\ntrained left-to-right and right-to-left LMs.\\\\n\\\\n• We show that pre-trained representations reduce\\\\nthe need for many heavily-engineered task-\\\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\\\ntuning based representation model that achieves\\\\nstate-of-the-art performance on a large suite\\\\nof sentence-level and token-level tasks, outper-\\\\nforming many task-speciﬁc architectures.\\\\n\\\\n• BERT advances the state of the art for eleven\\\\nNLP tasks. The code and pre-trained mod-\\\\nels are available at https://github.com/\\\\ngoogle-research/bert.\\\\n\\\\n2 Related Work\\\\n\\\\nThere is a long history of pre-training general lan-\\\\nguage representations, and we brieﬂy review the\\\\nmost widely-used approaches in this section.\\\\n\\\\n2.1 Unsupervised Feature-based Approaches\\\\n\\\\nLearning widely applicable representations of\\\\nwords has been an active area of research for\\\\ndecades, including non-neural (Brown et al., 1992;\\\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\\\nneural (Mikolov et al., 2013; Pennington et al.,\\\\n2014) methods.\\\\nPre-trained word embeddings\\\\nare an integral part of modern NLP systems, of-\\\\nfering signiﬁcant improvements over embeddings\\\\nlearned from scratch (Turian et al., 2010). To pre-\\\\ntrain word embedding vectors, left-to-right lan-\\\\nguage modeling objectives have been used (Mnih\\\\nand Hinton, 2009), as well as objectives to dis-\\\\ncriminate correct from incorrect words in left and\\\\nright context (Mikolov et al., 2013).\\\\n\\\\nThese approaches have been generalized to\\\\ncoarser granularities, such as sentence embed-\\\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\\\n2018) or paragraph embeddings (Le and Mikolov,\\\\n2014). To train sentence representations, prior\\\\nwork has used objectives to rank candidate next\\\\nsentences (Jernite et al., 2017; Logeswaran and\\\\nLee, 2018), left-to-right generation of next sen-\\\\ntence words given a representation of the previous\\\\nsentence (Kiros et al., 2015), or denoising auto-\\\\nencoder derived objectives (Hill et al., 2016).\\\\n\\\\nELMo and its predecessor (Peters et al., 2017,\\\\n2018a) generalize traditional word embedding re-\\\\nsearch along a different dimension. They extract\\\\ncontext-sensitive features from a left-to-right and a\\\\nright-to-left language model. The contextual rep-\\\\nresentation of each token is the concatenation of\\\\nthe left-to-right and right-to-left representations.\\\\nWhen integrating contextual word embeddings\\\\nwith existing task-speciﬁc architectures, ELMo\\\\nadvances the state of the art for several major NLP\\\\nbenchmarks (Peters et al., 2018a) including ques-\\\\ntion answering (Rajpurkar et al., 2016), sentiment\\\\nanalysis (Socher et al., 2013), and named entity\\\\nrecognition (Tjong Kim Sang and De Meulder,\\\\n2003). Melamud et al. (2016) proposed learning\\\\ncontextual representations through a task to pre-\\\\ndict a single word from both left and right context\\\\nusing LSTMs. Similar to ELMo, their model is\\\\nfeature-based and not deeply bidirectional. Fedus\\\\net al. (2018) shows that the cloze task can be used\\\\nto improve the robustness of text generation mod-\\\\nels.\\\\n\\\\n2.2 Unsupervised Fine-tuning Approaches\\\\n\\\\nAs with the feature-based approaches, the ﬁrst\\\\nworks in this direction only pre-trained word em-\\\\n(Col-\\\\nbedding parameters from unlabeled text\\\\nlobert and Weston, 2008).\\', \\'Fine-tuning approach\\\\n\\\\nBERTLARGE\\\\nBERTBASE\\\\n\\\\nFeature-based approach (BERTBASE)\\\\n\\\\nEmbeddings\\\\nSecond-to-Last Hidden\\\\nLast Hidden\\\\nWeighted Sum Last Four Hidden\\\\nConcat Last Four Hidden\\\\nWeighted Sum All 12 Layers\\\\n\\\\n95.7\\\\n-\\\\n-\\\\n\\\\n96.6\\\\n96.4\\\\n\\\\n91.0\\\\n95.6\\\\n94.9\\\\n95.9\\\\n96.1\\\\n95.5\\\\n\\\\n92.2\\\\n92.6\\\\n93.1\\\\n\\\\n92.8\\\\n92.4\\\\n\\\\n-\\\\n-\\\\n-\\\\n-\\\\n-\\\\n-\\\\n\\\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\\\nsults. Hyperparameters were selected using the Dev\\\\nset. The reported Dev and Test scores are averaged over\\\\n5 random restarts using those hyperparameters.\\\\n\\\\nlayer in the output. We use the representation of\\\\nthe ﬁrst sub-token as the input to the token-level\\\\nclassiﬁer over the NER label set.\\\\n\\\\nTo ablate the ﬁne-tuning approach, we apply the\\\\nfeature-based approach by extracting the activa-\\\\ntions from one or more layers without ﬁne-tuning\\\\nany parameters of BERT. These contextual em-\\\\nbeddings are used as input to a randomly initial-\\\\nized two-layer 768-dimensional BiLSTM before\\\\nthe classiﬁcation layer.\\\\n\\\\nResults are presented in Table 7. BERTLARGE\\\\nperforms competitively with state-of-the-art meth-\\\\nods. The best performing method concatenates the\\\\ntoken representations from the top four hidden lay-\\\\ners of the pre-trained Transformer, which is only\\\\n0.3 F1 behind ﬁne-tuning the entire model. This\\\\ndemonstrates that BERT is effective for both ﬁne-\\\\ntuning and feature-based approaches.\\\\n\\\\n6 Conclusion\\\\n\\\\nRecent empirical improvements due to transfer\\\\nlearning with language models have demonstrated\\\\nthat rich, unsupervised pre-training is an integral\\\\npart of many language understanding systems. In\\\\nparticular, these results enable even low-resource\\\\ntasks to beneﬁt from deep unidirectional architec-\\\\ntures. Our major contribution is further general-\\\\nizing these ﬁndings to deep bidirectional architec-\\\\ntures, allowing the same pre-trained model to suc-\\\\ncessfully tackle a broad set of NLP tasks.\\\\n\\\\n\\\\x0cReferences\\\\n\\\\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\\\\n2018. Contextual string embeddings for sequence\\\\nIn Proceedings of the 27th International\\\\nlabeling.\\\\nConference on Computational Linguistics, pages\\\\n1638–1649.\\\\n\\\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\\\nGuo, and Llion Jones. 2018. Character-level lan-\\\\nguage modeling with deeper self-attention. arXiv\\\\npreprint arXiv:1808.04444.\\\\n\\\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\\\nfor learning predictive structures from multiple tasks\\\\nand unlabeled data. Journal of Machine Learning\\\\nResearch, 6(Nov):1817–1853.\\\\n\\\\nLuisa Bentivogli, Bernardo Magnini,\\\\n\\\\nIdo Dagan,\\\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\\\nThe ﬁfth PASCAL recognizing textual entailment\\\\nchallenge. In TAC. NIST.\\\\n\\\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\\\n2006. Domain adaptation with structural correspon-\\\\ndence learning. In Proceedings of the 2006 confer-\\\\nence on empirical methods in natural language pro-\\\\ncessing, pages 120–128. Association for Computa-\\\\ntional Linguistics.\\\\n\\\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\\\nand Christopher D. Manning. 2015. A large anno-\\\\ntated corpus for learning natural language inference.\\\\nIn EMNLP. Association for Computational Linguis-\\\\ntics.\\\\n\\\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\\\nClass-based n-gram models of natural\\\\nlanguage.\\\\nComputational linguistics, 18(4):467–479.\\\\n\\\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\\\ntask 1: Semantic textual similarity multilingual and\\\\nIn Proceedings\\\\ncrosslingual focused evaluation.\\\\nof the 11th International Workshop on Semantic\\\\nEvaluation (SemEval-2017), pages 1–14, Vancou-\\\\nver, Canada. Association for Computational Lin-\\\\nguistics.\\\\n\\\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\\\nson. 2013. One billion word benchmark for measur-\\\\ning progress in statistical language modeling. arXiv\\\\npreprint arXiv:1312.3005.\\\\n\\\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\\\n\\\\nQuora question pairs.\\\\n\\\\nChristopher Clark and Matt Gardner. 2018. Simple\\\\nand effective multi-paragraph reading comprehen-\\\\nsion. In ACL.\\\\n\\\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\\\nSemi-supervised se-\\\\nning, and Quoc Le. 2018.\\\\nquence modeling with cross-view training. In Pro-\\\\nceedings of the 2018 Conference on Empirical Meth-\\\\nods in Natural Language Processing, pages 1914–\\\\n1925.\\\\n\\\\nRonan Collobert and Jason Weston. 2008. A uniﬁed\\\\narchitecture for natural language processing: Deep\\\\nIn Pro-\\\\nneural networks with multitask learning.\\\\nceedings of the 25th international conference on\\\\nMachine learning, pages 160–167. ACM.\\\\n\\\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc\\\\nBarrault, and Antoine Bordes. 2017. Supervised\\\\nlearning of universal sentence representations from\\\\nnatural language inference data. In Proceedings of\\\\nthe 2017 Conference on Empirical Methods in Nat-\\\\nural Language Processing, pages 670–680, Copen-\\\\nhagen, Denmark. Association for Computational\\\\nLinguistics.\\\\n\\\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\\\nsequence learning. In Advances in neural informa-\\\\ntion processing systems, pages 3079–3087.\\\\n\\\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\\\nImageNet: A Large-Scale Hierarchical\\\\n\\\\nFei. 2009.\\\\nImage Database. In CVPR09.\\\\n\\\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\\\ncally constructing a corpus of sentential paraphrases.\\\\nIn Proceedings of the Third International Workshop\\\\non Paraphrasing (IWP2005).\\\\n\\\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\\\n2018. Maskgan: Better text generation via ﬁlling in\\\\nthe . arXiv preprint arXiv:1801.07736.\\\\n\\\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\\\nnonlinearities and stochastic regularizers with gaus-\\\\nsian error linear units. CoRR, abs/1606.08415.\\\\n\\\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\\\nLearning distributed representations of sentences\\\\nIn Proceedings of the 2016\\\\nfrom unlabelled data.\\\\nConference of the North American Chapter of the\\\\nAssociation for Computational Linguistics: Human\\\\nLanguage Technologies. Association for Computa-\\\\ntional Linguistics.\\\\n\\\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\\\nlanguage model ﬁne-tuning for text classiﬁcation. In\\\\nACL. Association for Computational Linguistics.\\\\n\\\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\\\nReinforced\\\\nFuru Wei, and Ming Zhou. 2018.\\\\nmnemonic reader for machine reading comprehen-\\\\nsion. In IJCAI.\\\\n\\\\nYacine Jernite, Samuel R. Bowman, and David Son-\\\\ntag. 2017. Discourse-based objectives for fast un-\\\\nsupervised sentence representation learning. CoRR,\\\\nabs/1705.00557.\\\\n\\\\n\\\\x0cMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\\\nsupervised challenge dataset for reading comprehen-\\\\nsion. In ACL.\\\\n\\\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\\\nand Sanja Fidler. 2015. Skip-thought vectors.\\\\nIn\\\\nAdvances in neural information processing systems,\\\\npages 3294–3302.\\\\n\\\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\\\nresentations of sentences and documents. In Inter-\\\\nnational Conference on Machine Learning, pages\\\\n1188–1196.\\\\n\\\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\\\nstern. 2011. The winograd schema challenge.\\\\nIn\\\\nAaai spring symposium: Logical formalizations of\\\\ncommonsense reasoning, volume 46, page 47.\\\\n\\\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\\\nefﬁcient framework for learning sentence represen-\\\\nIn International Conference on Learning\\\\ntations.\\\\nRepresentations.\\\\n\\\\nBryan McCann, James Bradbury, Caiming Xiong, and\\\\nRichard Socher. 2017. Learned in translation: Con-\\\\ntextualized word vectors. In NIPS.\\\\n\\\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\\\n2016. context2vec: Learning generic context em-\\\\nbedding with bidirectional LSTM. In CoNLL.\\\\n\\\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\\\nrado, and Jeff Dean. 2013. Distributed representa-\\\\ntions of words and phrases and their compositional-\\\\nity. In Advances in Neural Information Processing\\\\nSystems 26, pages 3111–3119. Curran Associates,\\\\nInc.\\', \\'2.2 Unsupervised Fine-tuning Approaches\\\\n\\\\nAs with the feature-based approaches, the ﬁrst\\\\nworks in this direction only pre-trained word em-\\\\n(Col-\\\\nbedding parameters from unlabeled text\\\\nlobert and Weston, 2008).\\\\n\\\\nMore recently, sentence or document encoders\\\\nwhich produce contextual token representations\\\\nhave been pre-trained from unlabeled text and\\\\nﬁne-tuned for a supervised downstream task (Dai\\\\nand Le, 2015; Howard and Ruder, 2018; Radford\\\\net al., 2018). The advantage of these approaches\\\\nis that few parameters need to be learned from\\\\nscratch. At least partly due to this advantage,\\\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\\\nviously state-of-the-art results on many sentence-\\\\nlevel tasks from the GLUE benchmark (Wang\\\\nlanguage model-\\\\nLeft-to-right\\\\net al., 2018a).\\\\n\\\\n\\\\x0cFigure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\\\ntions/answers).\\\\n\\\\ning and auto-encoder objectives have been used\\\\nfor pre-training such models (Howard and Ruder,\\\\n2018; Radford et al., 2018; Dai and Le, 2015).\\\\n\\\\n2.3 Transfer Learning from Supervised Data\\\\n\\\\nThere has also been work showing effective trans-\\\\nfer from supervised tasks with large datasets, such\\\\nas natural language inference (Conneau et al.,\\\\n2017) and machine translation (McCann et al.,\\\\n2017). Computer vision research has also demon-\\\\nstrated the importance of transfer learning from\\\\nlarge pre-trained models, where an effective recipe\\\\nis to ﬁne-tune models pre-trained with Ima-\\\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\\\n\\\\n3 BERT\\\\n\\\\nWe introduce BERT and its detailed implementa-\\\\ntion in this section. There are two steps in our\\\\nframework: pre-training and ﬁne-tuning. Dur-\\\\ning pre-training, the model is trained on unlabeled\\\\ndata over different pre-training tasks. For ﬁne-\\\\ntuning, the BERT model is ﬁrst initialized with\\\\nthe pre-trained parameters, and all of the param-\\\\neters are ﬁne-tuned using labeled data from the\\\\ndownstream tasks. Each downstream task has sep-\\\\narate ﬁne-tuned models, even though they are ini-\\\\ntialized with the same pre-trained parameters. The\\\\nquestion-answering example in Figure 1 will serve\\\\nas a running example for this section.\\\\n\\\\nA distinctive feature of BERT is its uniﬁed ar-\\\\nchitecture across different tasks. There is mini-\\\\n\\\\nmal difference between the pre-trained architec-\\\\nture and the ﬁnal downstream architecture.\\\\n\\\\nModel Architecture BERT’s model architec-\\\\nture is a multi-layer bidirectional Transformer en-\\\\ncoder based on the original implementation de-\\\\nscribed in Vaswani et al. (2017) and released in\\\\nthe tensor2tensor library.1 Because the use\\\\nof Transformers has become common and our im-\\\\nplementation is almost identical to the original,\\\\nwe will omit an exhaustive background descrip-\\\\ntion of the model architecture and refer readers to\\\\nVaswani et al. (2017) as well as excellent guides\\\\nsuch as “The Annotated Transformer.”2\\\\n\\\\nIn this work, we denote the number of layers\\\\n(i.e., Transformer blocks) as L, the hidden size as\\\\nH, and the number of self-attention heads as A.3\\\\nWe primarily report results on two model sizes:\\\\nBERTBASE (L=12, H=768, A=12, Total Param-\\\\neters=110M) and BERTLARGE (L=24, H=1024,\\\\nA=16, Total Parameters=340M).\\\\n\\\\nBERTBASE was chosen to have the same model\\\\nsize as OpenAI GPT for comparison purposes.\\\\nCritically, however, the BERT Transformer uses\\\\nbidirectional self-attention, while the GPT Trans-\\\\nformer uses constrained self-attention where every\\\\ntoken can only attend to context to its left.4\\\\n\\\\n1https://github.com/tensorﬂow/tensor2tensor\\\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\\\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\\\\n\\\\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\\\\n\\\\n4We note that in the literature the bidirectional Trans-\\\\n\\\\nBERTBERTE[CLS]E1 E[SEP]...ENE1’...EM’CT1T[SEP]...TNT1’...TM’[CLS]Tok 1 [SEP]...Tok NTok 1...TokMQuestionParagraphStart/End SpanBERTE[CLS]E1 E[SEP]...ENE1’...EM’CT1T[SEP]...TNT1’...TM’[CLS]Tok 1 [SEP]...Tok NTok 1...TokMMasked Sentence AMasked Sentence BPre-trainingFine-TuningNSPMask LMMask LMUnlabeled Sentence A and B Pair SQuADQuestion Answer PairNERMNLI\\\\x0cInput/Output Representations To make BERT\\\\nhandle a variety of down-stream tasks, our input\\\\nrepresentation is able to unambiguously represent\\\\nboth a single sentence and a pair of sentences\\\\n(e.g., (cid:104) Question, Answer (cid:105)) in one token sequence.\\\\nThroughout this work, a “sentence” can be an arbi-\\\\ntrary span of contiguous text, rather than an actual\\\\nlinguistic sentence. A “sequence” refers to the in-\\\\nput token sequence to BERT, which may be a sin-\\\\ngle sentence or two sentences packed together.\\\\n\\\\nWe use WordPiece embeddings (Wu et al.,\\\\n2016) with a 30,000 token vocabulary. The ﬁrst\\\\ntoken of every sequence is always a special clas-\\\\nsiﬁcation token ([CLS]). The ﬁnal hidden state\\\\ncorresponding to this token is used as the ag-\\\\ngregate sequence representation for classiﬁcation\\\\ntasks. Sentence pairs are packed together into a\\\\nsingle sequence. We differentiate the sentences in\\\\ntwo ways. First, we separate them with a special\\\\ntoken ([SEP]). Second, we add a learned embed-\\\\nding to every token indicating whether it belongs\\\\nto sentence A or sentence B. As shown in Figure 1,\\\\nwe denote input embedding as E, the ﬁnal hidden\\\\nvector of the special [CLS] token as C ∈ RH ,\\\\nand the ﬁnal hidden vector for the ith input token\\\\nas Ti ∈ RH .\\\\n\\\\nFor a given token, its input representation is\\\\nconstructed by summing the corresponding token,\\\\nsegment, and position embeddings. A visualiza-\\\\ntion of this construction can be seen in Figure 2.\\\\n\\\\n3.1 Pre-training BERT\\\\n\\\\nUnlike Peters et al. (2018a) and Radford et al.\\\\n(2018), we do not use traditional left-to-right or\\\\nright-to-left language models to pre-train BERT.\\\\nInstead, we pre-train BERT using two unsuper-\\\\nvised tasks, described in this section. This step\\\\nis presented in the left part of Figure 1.\\\\n\\\\nTask #1: Masked LM Intuitively, it is reason-\\\\nable to believe that a deep bidirectional model is\\\\nstrictly more powerful than either a left-to-right\\\\nmodel or the shallow concatenation of a left-to-\\\\nright and a right-to-left model. Unfortunately,\\\\nstandard conditional language models can only be\\\\ntrained left-to-right or right-to-left, since bidirec-\\\\ntional conditioning would allow each word to in-\\\\ndirectly “see itself”, and the model could trivially\\\\npredict the target word in a multi-layered context.\\\\n\\\\nIn order to train a deep bidirectional representa-\\\\ntion, we simply mask some percentage of the input\\\\ntokens at random, and then predict those masked\\\\ntokens. We refer to this procedure as a “masked\\\\nLM” (MLM), although it is often referred to as a\\\\nCloze task in the literature (Taylor, 1953). In this\\\\ncase, the ﬁnal hidden vectors corresponding to the\\\\nmask tokens are fed into an output softmax over\\\\nthe vocabulary, as in a standard LM. In all of our\\\\nexperiments, we mask 15% of all WordPiece to-\\\\nkens in each sequence at random. In contrast to\\\\ndenoising auto-encoders (Vincent et al., 2008), we\\\\nonly predict the masked words rather than recon-\\\\nstructing the entire input.\\']<br>        Question: What is the main difference between BERT and previous language representation models?<br>        ', 'sources': '\\nBert.pdf', 'nextQuestions': '<1> What are the two unsupervised tasks used to pre-train BERT?\\n<2> How are WordPiece embeddings used in BERT?\\n<3> How are the input representations constructed in BERT?\\n\\n', 'error': ''}}]}}\n"
     ]
    }
   ],
   "source": [
    "output = pf.flows.test(\n",
    "    \"../api/PromptFlow/QuestionAnswering/\",\n",
    "    inputs={\n",
    "\t\"question\": \"What is the main difference between BERT and previous language representation models?\",\n",
    "\t\"answer\": \"BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\",\n",
    "\t\"chainType\": \"stuff\",\n",
    "\t\"indexType\": \"cogsearchvs\",\n",
    "\t\"indexNs\": \"8fe8ee44933240fa8bc1d72858e4d1eb\",\n",
    "\t\"postBody\": {\n",
    "\t\t\"values\": [{\n",
    "\t\t\t\"recordId\": 0,\n",
    "\t\t\t\"data\": {\n",
    "\t\t\t\t\"text\": \"\",\n",
    "\t\t\t\t\"approach\": \"rtr\",\n",
    "\t\t\t\t\"overrides\": {\n",
    "\t\t\t\t\t\"semantic_ranker\": \"true\",\n",
    "\t\t\t\t\t\"semantic_captions\": \"false\",\n",
    "\t\t\t\t\t\"top\": 3,\n",
    "\t\t\t\t\t\"temperature\": 0,\n",
    "\t\t\t\t\t\"promptTemplate\": \"Given the following extracted parts of a long document and a question, create a final answer. \\n        If you don't know the answer, just say that you don't know. Don't try to make up an answer. \\n        If the answer is not contained within the text below, say \\\"I don't know\\\".\\n\\n        {summaries}\\n        Question: {question}\\n        \",\n",
    "\t\t\t\t\t\"chainType\": \"stuff\",\n",
    "\t\t\t\t\t\"tokenLength\": 1000,\n",
    "\t\t\t\t\t\"embeddingModelType\": \"azureopenai\",\n",
    "\t\t\t\t\t\"deploymentType\": \"gpt3516k\"\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}]\n",
    "\t}\n",
    "    },\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the flow with a data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-15 16:35:41 -0500   33668 execution          INFO     Start to run 8 nodes with concurrency level 2.\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     INFO     Executing node parse_postBody. node run id: questionanswering_default_20230915_163531_883530_parse_postBody_3\n",
      "2023-09-15 16:35:41 -0500    9504 execution          INFO     Start to run 8 nodes with concurrency level 2.\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     INFO     Node parse_postBody completes.\n",
      "ode run id: questionanswering_default_20230915_163531_883530_parse_postBody_1\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     INFO     Executing node create_llm. node run id: questionanswering_default_20230915_163531_883530_create_llm_3\n",
      "2023-09-15 16:35:41 -0500    9504 execution.flow     INFO     Node parse_postBody completes.\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     INFO     Executing node embed_the_question. node run id: questionanswering_default_20230915_163531_883530_embed_the_question_3\n",
      "2023-09-15 16:35:41 -0500    9504 execution.flow     INFO     Executing node create_llm. node run id: questionanswering_default_20230915_163531_883530_create_llm_1\n",
      "2023-09-15 16:35:41 -0500    9504 execution.flow     INFO     Executing node embed_the_question. node run id: questionanswering_default_20230915_163531_883530_embed_the_question_1\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     WARNING  Output of create_llm is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     INFO     Node create_llm completes.\n",
      "2023-09-15 16:35:41 -0500    9504 execution.flow     WARNING  Output of create_llm is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:41 -0500    9504 execution.flow     INFO     Node create_llm completes.\n",
      "2023-09-15 16:35:41 -0500   37880 execution          INFO     Start to run 8 nodes with concurrency level 2.\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     INFO     Executing node parse_postBody. node run id: questionanswering_default_20230915_163531_883530_parse_postBody_4\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     INFO     Node parse_postBody completes.\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     INFO     Executing node create_llm. node run id: questionanswering_default_20230915_163531_883530_create_llm_4\n",
      "2023-09-15 16:35:41 -0500   32100 execution          INFO     Start to run 8 nodes with concurrency level 2.\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     INFO     Executing node embed_the_question. node run id: questionanswering_default_20230915_163531_883530_embed_the_question_4\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     INFO     Executing node parse_postBody. node run id: questionanswering_default_20230915_163531_883530_parse_postBody_0\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     WARNING  Output of create_llm is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     INFO     Node create_llm completes.\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     INFO     Node parse_postBody completes.\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     INFO     Executing node create_llm. node run id: questionanswering_default_20230915_163531_883530_create_llm_0\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     INFO     Executing node embed_the_question. node run id: questionanswering_default_20230915_163531_883530_embed_the_question_0\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     WARNING  Output of create_llm is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     INFO     Node create_llm completes.\n",
      "2023-09-15 16:35:41 -0500   38412 execution          INFO     Start to run 8 nodes with concurrency level 2.\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     INFO     Executing node parse_postBody. node run id: questionanswering_default_20230915_163531_883530_parse_postBody_2\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     INFO     Node parse_postBody completes.\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     INFO     Executing node create_llm. node run id: questionanswering_default_20230915_163531_883530_create_llm_2\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     INFO     Executing node embed_the_question. node run id: questionanswering_default_20230915_163531_883530_embed_the_question_2\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     WARNING  Output of create_llm is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     INFO     Node create_llm completes.\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     INFO     Node embed_the_question completes.\n",
      "2023-09-15 16:35:41 -0500   37880 execution.flow     INFO     Executing node check_cache_answer. node run id: questionanswering_default_20230915_163531_883530_check_cache_answer_4\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     INFO     Node embed_the_question completes.\n",
      "2023-09-15 16:35:41 -0500   33668 execution.flow     INFO     Executing node check_cache_answer. node run id: questionanswering_default_20230915_163531_883530_check_cache_answer_3\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     INFO     Node embed_the_question completes.\n",
      "2023-09-15 16:35:41 -0500   32100 execution.flow     INFO     Executing node check_cache_answer. node run id: questionanswering_default_20230915_163531_883530_check_cache_answer_0\n",
      "2023-09-15 16:35:41 -0500    9504 execution.flow     INFO     Node embed_the_question completes.\n",
      "2023-09-15 16:35:41 -0500    9504 execution.flow     INFO     Executing node check_cache_answer. node run id: questionanswering_default_20230915_163531_883530_check_cache_answer_1\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     INFO     Node embed_the_question completes.\n",
      "2023-09-15 16:35:41 -0500   38412 execution.flow     INFO     Executing node check_cache_answer. node run id: questionanswering_default_20230915_163531_883530_check_cache_answer_2\n",
      "2023-09-15 16:35:42 -0500   32100 execution.flow     INFO     [check_cache_answer in line 0 (index starts from 0)] stdout> Search index aoaikb already exists\n",
      "2023-09-15 16:35:43 -0500   33668 execution.flow     INFO     [check_cache_answer in line 3 (index starts from 0)] stdout> Search index aoaikb already exists\n",
      "2023-09-15 16:35:43 -0500   37880 execution.flow     INFO     [check_cache_answer in line 4 (index starts from 0)] stdout> Search index aoaikb already exists\n",
      "2023-09-15 16:35:43 -0500   38412 execution.flow     INFO     [check_cache_answer in line 2 (index starts from 0)] stdout> Search index aoaikb already exists\n",
      "2023-09-15 16:35:43 -0500   33668 execution.flow     INFO     [check_cache_answer in line 3 (index starts from 0)] stdout> KB Search Count: 1\n",
      "2023-09-15 16:35:43 -0500   37880 execution.flow     INFO     [check_cache_answer in line 4 (index starts from 0)] stdout> KB Search Count: 1\n",
      "2023-09-15 16:35:43 -0500   32100 execution.flow     INFO     [check_cache_answer in line 0 (index starts from 0)] stdout> KB Search Count: 1\n",
      "2023-09-15 16:35:43 -0500    9504 execution.flow     INFO     [check_cache_answer in line 1 (index starts from 0)] stdout> KB Search Count: 1\n",
      "2023-09-15 16:35:43 -0500   33668 execution.flow     INFO     Node check_cache_answer completes.\n",
      "2023-09-15 16:35:43 -0500   33668 execution.flow     INFO     Executing node followup_questions. node run id: questionanswering_default_20230915_163531_883530_followup_questions_3\n",
      "2023-09-15 16:35:43 -0500   33668 execution.flow     WARNING  Input 'llm' of followup_questions is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:43 -0500   33668 execution.flow     INFO     Node followup_questions completes.\n",
      "2023-09-15 16:35:43 -0500   37880 execution.flow     INFO     Node check_cache_answer completes.\n",
      "2023-09-15 16:35:43 -0500   37880 execution.flow     INFO     Executing node followup_questions. node run id: questionanswering_default_20230915_163531_883530_followup_questions_4\n",
      "2023-09-15 16:35:43 -0500   37880 execution.flow     WARNING  Input 'llm' of followup_questions is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:43 -0500   37880 execution.flow     INFO     Node followup_questions completes.\n",
      "2023-09-15 16:35:43 -0500   32100 execution.flow     INFO     Node check_cache_answer completes.\n",
      "2023-09-15 16:35:43 -0500   32100 execution.flow     INFO     Executing node followup_questions. node run id: questionanswering_default_20230915_163531_883530_followup_questions_0\n",
      "2023-09-15 16:35:43 -0500   32100 execution.flow     WARNING  Input 'llm' of followup_questions is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:43 -0500   32100 execution.flow     INFO     Node followup_questions completes.\n",
      "2023-09-15 16:35:43 -0500    9504 execution.flow     INFO     Node check_cache_answer completes.\n",
      "2023-09-15 16:35:43 -0500    9504 execution.flow     INFO     Executing node followup_questions. node run id: questionanswering_default_20230915_163531_883530_followup_questions_1\n",
      "2023-09-15 16:35:43 -0500    9504 execution.flow     WARNING  Input 'llm' of followup_questions is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:43 -0500    9504 execution.flow     INFO     Node followup_questions completes.\n",
      "2023-09-15 16:35:43 -0500   38412 execution.flow     INFO     [check_cache_answer in line 2 (index starts from 0)] stdout> KB Search Count: 1\n",
      "2023-09-15 16:35:44 -0500   38412 execution.flow     INFO     Node check_cache_answer completes.\n",
      "2023-09-15 16:35:44 -0500   38412 execution.flow     INFO     Executing node followup_questions. node run id: questionanswering_default_20230915_163531_883530_followup_questions_2\n",
      "2023-09-15 16:35:44 -0500   38412 execution.flow     WARNING  Input 'llm' of followup_questions is not json serializable, use str to store it.\n",
      "2023-09-15 16:35:44 -0500   38412 execution.flow     INFO     Node followup_questions completes.\n",
      "2023-09-15 16:35:45 -0500   35760 execution          INFO     Process 2 queue empty, exit.\n",
      "2023-09-15 16:35:45 -0500   35760 execution          INFO     Process 0 queue empty, exit.\n",
      "2023-09-15 16:35:45 -0500   35760 execution          INFO     Process 4 queue empty, exit.\n",
      "2023-09-15 16:35:45 -0500   35760 execution          INFO     Process 1 queue empty, exit.\n",
      "2023-09-15 16:35:45 -0500   35760 execution          INFO     Process 3 queue empty, exit.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"questionanswering_default_20230915_163531_883530\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2023-09-15 16:35:31.882238\"\n",
      "Duration: \"0:00:13.773588\"\n",
      "Output path: \"C:\\Users\\astalati\\.promptflow\\.runs\\questionanswering_default_20230915_163531_883530\"\n",
      "\n",
      "name: questionanswering_default_20230915_163531_883530\n",
      "created_on: '2023-09-15T16:35:31.882238'\n",
      "status: Completed\n",
      "display_name: questionanswering_default_20230915_163531_883530\n",
      "description: null\n",
      "tags: null\n",
      "properties:\n",
      "  flow_path: D:/repos/chatpdf/api/PromptFlow/QuestionAnswering\n",
      "  output_path: C:/Users/astalati/.promptflow/.runs/questionanswering_default_20230915_163531_883530\n",
      "flow_name: QuestionAnswering\n",
      "data: D:/repos/chatpdf/api/PromptFlow/QuestionAnswering/bert.jsonl\n",
      "output: C:/Users/astalati/.promptflow/.runs/questionanswering_default_20230915_163531_883530/flow_outputs/output.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flowPath = \"../api/PromptFlow/QuestionAnswering/\"\n",
    "dataPath = \"../api/PromptFlow/QuestionAnswering/bert.jsonl\"\n",
    "\n",
    "columnMapping = {\n",
    "\t\"question\": \"${data.question}\",\n",
    "\t\"answer\": \"${data.answer}\",\n",
    "\t\"chainType\": \"${data.chainType}\",\n",
    "\t\"indexType\": \"${data.indexType}\",\n",
    "\t\"indexNs\": \"${data.indexNs}\",\n",
    "\t\"postBody\": \"${data.postBody}\"\n",
    "    }\n",
    "\n",
    "bertContext = pf.run(flow=flowPath, data=dataPath, column_mapping=columnMapping)\n",
    "pf.stream(bertContext)\n",
    "\n",
    "print(bertContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.chainType</th>\n",
       "      <th>inputs.indexNs</th>\n",
       "      <th>inputs.indexType</th>\n",
       "      <th>inputs.postBody</th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.answer</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.context</th>\n",
       "      <th>outputs.output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stuff</td>\n",
       "      <td>8fe8ee44933240fa8bc1d72858e4d1eb</td>\n",
       "      <td>cogsearchvs</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'text': '...</td>\n",
       "      <td>What is the name of the new language represent...</td>\n",
       "      <td>BERT</td>\n",
       "      <td>0</td>\n",
       "      <td>The name of the new language representation mo...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'data_poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stuff</td>\n",
       "      <td>8fe8ee44933240fa8bc1d72858e4d1eb</td>\n",
       "      <td>cogsearchvs</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'text': '...</td>\n",
       "      <td>What is the main difference between BERT and p...</td>\n",
       "      <td>BERT is designed to pretrain deep bidirectiona...</td>\n",
       "      <td>1</td>\n",
       "      <td>The main difference between BERT and previous ...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'data_poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stuff</td>\n",
       "      <td>8fe8ee44933240fa8bc1d72858e4d1eb</td>\n",
       "      <td>cogsearchvs</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'text': '...</td>\n",
       "      <td>What is the advantage of fine-tuning BERT over...</td>\n",
       "      <td>Fine-tuning BERT reduces the need for many hea...</td>\n",
       "      <td>2</td>\n",
       "      <td>The advantage of fine-tuning BERT over using f...</td>\n",
       "      <td>[Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n...</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'data_poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stuff</td>\n",
       "      <td>8fe8ee44933240fa8bc1d72858e4d1eb</td>\n",
       "      <td>cogsearchvs</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'text': '...</td>\n",
       "      <td>What are the two unsupervised tasks used to pr...</td>\n",
       "      <td>Masked LM and next sentence prediction</td>\n",
       "      <td>3</td>\n",
       "      <td>The two unsupervised tasks used to pre-train B...</td>\n",
       "      <td>[2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'data_poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stuff</td>\n",
       "      <td>8fe8ee44933240fa8bc1d72858e4d1eb</td>\n",
       "      <td>cogsearchvs</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'text': '...</td>\n",
       "      <td>How does BERT handle single sentence and sente...</td>\n",
       "      <td>It uses a special classification token ([CLS])...</td>\n",
       "      <td>4</td>\n",
       "      <td>BERT handles single sentence and sentence pair...</td>\n",
       "      <td>[2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...</td>\n",
       "      <td>{'values': [{'recordId': 0, 'data': {'data_poi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  inputs.chainType                    inputs.indexNs inputs.indexType  \\\n",
       "0            stuff  8fe8ee44933240fa8bc1d72858e4d1eb      cogsearchvs   \n",
       "1            stuff  8fe8ee44933240fa8bc1d72858e4d1eb      cogsearchvs   \n",
       "2            stuff  8fe8ee44933240fa8bc1d72858e4d1eb      cogsearchvs   \n",
       "3            stuff  8fe8ee44933240fa8bc1d72858e4d1eb      cogsearchvs   \n",
       "4            stuff  8fe8ee44933240fa8bc1d72858e4d1eb      cogsearchvs   \n",
       "\n",
       "                                     inputs.postBody  \\\n",
       "0  {'values': [{'recordId': 0, 'data': {'text': '...   \n",
       "1  {'values': [{'recordId': 0, 'data': {'text': '...   \n",
       "2  {'values': [{'recordId': 0, 'data': {'text': '...   \n",
       "3  {'values': [{'recordId': 0, 'data': {'text': '...   \n",
       "4  {'values': [{'recordId': 0, 'data': {'text': '...   \n",
       "\n",
       "                                     inputs.question  \\\n",
       "0  What is the name of the new language represent...   \n",
       "1  What is the main difference between BERT and p...   \n",
       "2  What is the advantage of fine-tuning BERT over...   \n",
       "3  What are the two unsupervised tasks used to pr...   \n",
       "4  How does BERT handle single sentence and sente...   \n",
       "\n",
       "                                       inputs.answer  inputs.line_number  \\\n",
       "0                                               BERT                   0   \n",
       "1  BERT is designed to pretrain deep bidirectiona...                   1   \n",
       "2  Fine-tuning BERT reduces the need for many hea...                   2   \n",
       "3             Masked LM and next sentence prediction                   3   \n",
       "4  It uses a special classification token ([CLS])...                   4   \n",
       "\n",
       "                                      outputs.answer  \\\n",
       "0  The name of the new language representation mo...   \n",
       "1  The main difference between BERT and previous ...   \n",
       "2  The advantage of fine-tuning BERT over using f...   \n",
       "3  The two unsupervised tasks used to pre-train B...   \n",
       "4  BERT handles single sentence and sentence pair...   \n",
       "\n",
       "                                     outputs.context  \\\n",
       "0  [BERT: Pre-training of Deep Bidirectional Tran...   \n",
       "1  [BERT: Pre-training of Deep Bidirectional Tran...   \n",
       "2  [Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n...   \n",
       "3  [2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...   \n",
       "4  [2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...   \n",
       "\n",
       "                                      outputs.output  \n",
       "0  {'values': [{'recordId': 0, 'data': {'data_poi...  \n",
       "1  {'values': [{'recordId': 0, 'data': {'data_poi...  \n",
       "2  {'values': [{'recordId': 0, 'data': {'data_poi...  \n",
       "3  {'values': [{'recordId': 0, 'data': {'data_poi...  \n",
       "4  {'values': [{'recordId': 0, 'data': {'data_poi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_details(bertContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the \"groundedness\"\n",
    "The [eval-groundedness flow](../../evaluation/eval-groundedness/) is using ChatGPT/GPT4 model to grade the answers generated by chat-with-pdf flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-15 16:38:25 -0500   37048 execution          INFO     Start to run 2 nodes with concurrency level 2.\n",
      "2023-09-15 16:38:25 -0500   37048 execution.flow     INFO     Executing node gpt_groundedness. node run id: evalgroundness_default_20230915_163820_277505_gpt_groundedness_2\n",
      "2023-09-15 16:38:25 -0500   27180 execution          INFO     Start to run 2 nodes with concurrency level 2.\n",
      "2023-09-15 16:38:25 -0500   14868 execution          INFO     Start to run 2 nodes with concurrency level 2.\n",
      "2023-09-15 16:38:25 -0500   27180 execution.flow     INFO     Executing node gpt_groundedness. node run id: evalgroundness_default_20230915_163820_277505_gpt_groundedness_0\n",
      "2023-09-15 16:38:25 -0500   14868 execution.flow     INFO     Executing node gpt_groundedness. node run id: evalgroundness_default_20230915_163820_277505_gpt_groundedness_1\n",
      "2023-09-15 16:38:25 -0500   37284 execution          INFO     Start to run 2 nodes with concurrency level 2.\n",
      "2023-09-15 16:38:25 -0500   22752 execution          INFO     Start to run 2 nodes with concurrency level 2.\n",
      "2023-09-15 16:38:25 -0500   37284 execution.flow     INFO     Executing node gpt_groundedness. node run id: evalgroundness_default_20230915_163820_277505_gpt_groundedness_3\n",
      "2023-09-15 16:38:25 -0500   22752 execution.flow     INFO     Executing node gpt_groundedness. node run id: evalgroundness_default_20230915_163820_277505_gpt_groundedness_4\n",
      "2023-09-15 16:38:26 -0500   14868 execution.flow     INFO     Node gpt_groundedness completes.\n",
      "2023-09-15 16:38:26 -0500   14868 execution.flow     INFO     Executing node parse_score. node run id: evalgroundness_default_20230915_163820_277505_parse_score_1\n",
      "2023-09-15 16:38:26 -0500   14868 execution.flow     INFO     Node parse_score completes.\n",
      "2023-09-15 16:38:26 -0500   27180 execution.flow     INFO     Node gpt_groundedness completes.\n",
      "2023-09-15 16:38:26 -0500   27180 execution.flow     INFO     Executing node parse_score. node run id: evalgroundness_default_20230915_163820_277505_parse_score_0\n",
      "2023-09-15 16:38:26 -0500   27180 execution.flow     INFO     Node parse_score completes.\n",
      "2023-09-15 16:38:26 -0500   37284 execution.flow     INFO     Node gpt_groundedness completes.\n",
      "2023-09-15 16:38:26 -0500   37284 execution.flow     INFO     Executing node parse_score. node run id: evalgroundness_default_20230915_163820_277505_parse_score_3\n",
      "2023-09-15 16:38:26 -0500   37284 execution.flow     INFO     Node parse_score completes.\n",
      "2023-09-15 16:38:26 -0500   37048 execution.flow     INFO     Node gpt_groundedness completes.\n",
      "2023-09-15 16:38:26 -0500   37048 execution.flow     INFO     Executing node parse_score. node run id: evalgroundness_default_20230915_163820_277505_parse_score_2\n",
      "2023-09-15 16:38:26 -0500   37048 execution.flow     INFO     Node parse_score completes.\n",
      "2023-09-15 16:38:26 -0500   22752 execution.flow     INFO     Node gpt_groundedness completes.\n",
      "2023-09-15 16:38:26 -0500   22752 execution.flow     INFO     Executing node parse_score. node run id: evalgroundness_default_20230915_163820_277505_parse_score_4\n",
      "2023-09-15 16:38:26 -0500   22752 execution.flow     INFO     Node parse_score completes.\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Process 0 queue empty, exit.\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Process 3 queue empty, exit.\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Process 2 queue empty, exit.\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Process 4 queue empty, exit.\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Process 1 queue empty, exit.\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Executing aggregation nodes...\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Start to run 1 nodes with concurrency level 2.\n",
      "2023-09-15 16:38:27 -0500   35760 execution.flow     INFO     Executing node aggregate. node run id: evalgroundness_default_20230915_163820_277505_aggregate_reduce\n",
      "2023-09-15 16:38:27 -0500   35760 execution.flow     INFO     Node aggregate completes.\n",
      "2023-09-15 16:38:27 -0500   35760 execution          INFO     Finish executing aggregation nodes.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"evalgroundness_default_20230915_163820_277505\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2023-09-15 16:38:20.275499\"\n",
      "Duration: \"0:00:07.433571\"\n",
      "Output path: \"C:\\Users\\astalati\\.promptflow\\.runs\\evalgroundness_default_20230915_163820_277505\"\n",
      "\n",
      "name: evalgroundness_default_20230915_163820_277505\n",
      "created_on: '2023-09-15T16:38:20.275499'\n",
      "status: Completed\n",
      "display_name: bertContext\n",
      "description: null\n",
      "tags: null\n",
      "properties:\n",
      "  flow_path: D:/repos/chatpdf/Workshop/promptflow/EvalGroundness\n",
      "  output_path: C:/Users/astalati/.promptflow/.runs/evalgroundness_default_20230915_163820_277505\n",
      "flow_name: EvalGroundness\n",
      "data: null\n",
      "output: C:/Users/astalati/.promptflow/.runs/evalgroundness_default_20230915_163820_277505/flow_outputs/output.jsonl\n",
      "run: questionanswering_default_20230915_163531_883530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evalFlowPath = \"../Workshop/PromptFlow/EvalGroundness/\"\n",
    "evalContext = pf.run(\n",
    "    flow=evalFlowPath,\n",
    "    run=bertContext,\n",
    "    column_mapping={\n",
    "        \"question\": \"${run.inputs.question}\",\n",
    "        \"answer\": \"${run.outputs.answer}\",\n",
    "        \"context\": \"${run.outputs.context}\",\n",
    "    },\n",
    "    display_name=\"bertContext\",\n",
    ")\n",
    "pf.stream(evalContext)\n",
    "\n",
    "print(evalContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.answer</th>\n",
       "      <th>inputs.context</th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.groundedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The name of the new language representation mo...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "      <td>What is the name of the new language represent...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The main difference between BERT and previous ...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "      <td>What is the main difference between BERT and p...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The advantage of fine-tuning BERT over using f...</td>\n",
       "      <td>[Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n...</td>\n",
       "      <td>What is the advantage of fine-tuning BERT over...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The two unsupervised tasks used to pre-train B...</td>\n",
       "      <td>[2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...</td>\n",
       "      <td>What are the two unsupervised tasks used to pr...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT handles single sentence and sentence pair...</td>\n",
       "      <td>[2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...</td>\n",
       "      <td>How does BERT handle single sentence and sente...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       inputs.answer  \\\n",
       "0  The name of the new language representation mo...   \n",
       "1  The main difference between BERT and previous ...   \n",
       "2  The advantage of fine-tuning BERT over using f...   \n",
       "3  The two unsupervised tasks used to pre-train B...   \n",
       "4  BERT handles single sentence and sentence pair...   \n",
       "\n",
       "                                      inputs.context  \\\n",
       "0  [BERT: Pre-training of Deep Bidirectional Tran...   \n",
       "1  [BERT: Pre-training of Deep Bidirectional Tran...   \n",
       "2  [Fine-tuning approach\\n\\nBERTLARGE\\nBERTBASE\\n...   \n",
       "3  [2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...   \n",
       "4  [2.2 Unsupervised Fine-tuning Approaches\\n\\nAs...   \n",
       "\n",
       "                                     inputs.question  inputs.line_number  \\\n",
       "0  What is the name of the new language represent...                   0   \n",
       "1  What is the main difference between BERT and p...                   1   \n",
       "2  What is the advantage of fine-tuning BERT over...                   2   \n",
       "3  What are the two unsupervised tasks used to pr...                   3   \n",
       "4  How does BERT handle single sentence and sente...                   4   \n",
       "\n",
       "   outputs.groundedness  \n",
       "0                    10  \n",
       "1                    10  \n",
       "2                    10  \n",
       "3                    10  \n",
       "4                    10  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_details(evalContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groundedness': 10.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_metrics(evalContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HTML file is generated at 'C:\\\\Users\\\\astalati\\\\AppData\\\\Local\\\\Temp\\\\pf-visualize-detail-x0ekmtxd.html'.\n",
      "Trying to view the result in a web browser...\n",
      "Successfully visualized from the web browser.\n"
     ]
    }
   ],
   "source": [
    "pf.visualize(evalContext)"
   ]
  }
 ],
 "metadata": {
  "description": "A tutorial of chat-with-pdf flow that allows user ask questions about the content of a PDF file and get answers",
  "kernelspec": {
   "display_name": "prompt-flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
