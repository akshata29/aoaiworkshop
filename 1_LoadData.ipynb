{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Index Data - Vector Store\n",
    "\n",
    "We will use [Azure Cognitive Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) to load and index the data.  Azure Cognitive Search is a cloud search service with built-in AI capabilities that enrich all types of information to easily identify and explore relevant content at scale. It uses the same integrated Microsoft natural language stack that Bing and Office have used for more than a decade, and AI services across vision, language, and speech, to deliver knowledge from structured and unstructured data.\n",
    "\n",
    "Cognitive search enabled the vector search feature! When done correctly, vector search is a proven technique for significantly increasing the semantic relevance of search results.  It is a technique that uses machine learning to embed text into a vector space, where the distance between vectors is a measure of semantic similarity.  This allows for the use of vector similarity search to find relevant results.  [Sign up]\n",
    "(https://aka.ms/VectorSearchSignUp) for Private Preview of Vector Search.\n",
    "\n",
    "Cognitive Search can index and store vectors, but it doesn't generate them out of the box. The documents that you push to your search service must contain vectors within the payload. Alternatively, you can use the Indexer to pull vectors from your data sources such as Blob Storage JSON files or CSVs. You can also use a Custom Skill to generate embeddings as part of the AI Enrichment process.\n",
    "\n",
    "\n",
    "[Sample repo](https://github.com/Azure/cognitive-search-vector-pr) to get started with vector search. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisite:\n",
    "- To run the code, install the following packages from local Wheel file. Alternatively, install azure-search-documents==11.4.0a20230509004 from the Dev Feed. For instructions on how to connect to the dev feed, please visit Azure-Python-SDK Azure Search Documents [Dev Feed](https://dev.azure.com/azure-sdk/public/_artifacts/feed/azure-sdk-for-python/connect/pip).\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/).\n",
    "- An Azure Cognitive Search service (any tier, any region). [Create a service](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) or find an [existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ./azure_search_documents-11.4.0b4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "#%pip install langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI\n",
    "from Utilities.cogSearch import createSearchIndex, indexSections\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "azure_endpoint  = f\"{OpenAiEndPoint}\"\n",
    "api_key = OpenAiKey\n",
    "api_version = OpenAiVersion\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,  \n",
    "    api_version=api_version,\n",
    "    #base_url=f\"{os.getenv('OpenAiWestUsEp')}openai/deployments/{os.getenv('OpenAiGpt4v')}/extensions\",\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain.llms.openai import AzureOpenAI, OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PDFMinerLoader,\n",
    "    UnstructuredFileLoader,\n",
    ")\n",
    "#from Utilities.cogSearch import createSearchIndex, indexSections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the PDF, create the chunk and push to Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexibility to change the call to OpenAI or Azure OpenAI\n",
    "embeddingModelType = \"azureopenai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating oaiworkshop search index\n"
     ]
    }
   ],
   "source": [
    "# Set the file name and the namespace for the index\n",
    "fileName = \"Fabric Get Started.pdf\"\n",
    "fabricGetStartedPath = \"Data/PDF/\" + fileName\n",
    "# Load the PDF with Document Loader available from Langchain\n",
    "loader = PDFMinerLoader(fabricGetStartedPath)\n",
    "rawDocs = loader.load()\n",
    "# Set the source \n",
    "for doc in rawDocs:\n",
    "    doc.metadata['source'] = fabricGetStartedPath\n",
    "\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
    "docs = textSplitter.split_documents(rawDocs)\n",
    "# Call Helper function to create Index and Index the sections\n",
    "createSearchIndex(SearchService, SearchKey, indexName)\n",
    "indexSections(SearchService, SearchKey, embeddingModelType, fileName, indexName, docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: Fabric_Get_Started_pdf-1\n",
      "Content: Tell us about your PDF experience.\n",
      "\n",
      "Microsoft Fabric get started\n",
      "documentation\n",
      "\n",
      "Microsoft Fabric is a unified platform that can meet your organization's data and\n",
      "analytics needs. Discover the Fabric shared and platform documentation from this page.\n",
      "\n",
      "About Microsoft Fabric\n",
      "\n",
      "ｅ OVERVIEW\n",
      "\n",
      "What is Fabric?\n",
      "\n",
      "Fabric terminology\n",
      "\n",
      "ｂ GET STARTED\n",
      "\n",
      "Start a Fabric trial\n",
      "\n",
      "Fabric home navigation\n",
      "\n",
      "End-to-end tutorials\n",
      "\n",
      "Context sensitive Help pane\n",
      "\n",
      "Get started with Fabric items\n",
      "\n",
      "ｐ CONCEPT\n",
      "\n",
      "Find items in OneLake data hub\n",
      "\n",
      "Promote and certify items\n",
      "\n",
      "ｃ HOW-TO GUIDE\n",
      "\n",
      "Apply sensitivity labels\n",
      "\n",
      "Workspaces\n",
      "\n",
      "ｐ CONCEPT\n",
      "\n",
      "Fabric workspace\n",
      "\n",
      "\fWorkspace roles\n",
      "\n",
      "ｂ GET STARTED\n",
      "\n",
      "Create a workspace\n",
      "\n",
      "ｃ HOW-TO GUIDE\n",
      "\n",
      "Workspace access control\n",
      "\n",
      "\fWhat is Microsoft Fabric?\n",
      "\n",
      "Article • 05/23/2023\n",
      "\n",
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything\n",
      "\n",
      "from data movement to data science, Real-Time Analytics, and business intelligence. It\n",
      "\n",
      "offers a comprehensive suite of services, including data lake, data engineering, and data\n",
      "\n",
      "integration, all in one place.\n",
      "\n",
      "With Fabric, you don't need to piece together different services from multiple vendors.\n",
      "\n",
      "Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is\n",
      "\n",
      "designed to simplify your analytics needs.\n",
      "\n",
      "The platform is built on a foundation of Software as a Service (SaaS), which takes\n",
      "\n",
      "simplicity and integration to a whole new level.\n",
      "\n",
      "） Important\n",
      "Source File: Fabric Get Started.pdf\n",
      "\n",
      "Id: Fabric_Get_Started_pdf-5\n",
      "Content: Fabric brings together all these experiences into a unified platform to offer the most\n",
      "\n",
      "comprehensive big data analytics platform in the industry.\n",
      "\n",
      "Microsoft Fabric enables organizations, and individuals, to turn large and complex data\n",
      "\n",
      "repositories into actionable workloads and analytics, and is an implementation of data\n",
      "\n",
      "mesh architecture. To learn more about data mesh, visit the article that explains data\n",
      "\n",
      "mesh architecture.\n",
      "\n",
      "OneLake and lakehouse - the unification of\n",
      "lakehouses\n",
      "\n",
      "\fThe Microsoft Fabric platform unifies the OneLake and lakehouse architecture across the\n",
      "\n",
      "enterprises.\n",
      "\n",
      "OneLake\n",
      "\n",
      "The data lake is the foundation on which all the Fabric services are built. Microsoft Fabric\n",
      "\n",
      "Lake is also known as OneLake. It's built into the Fabric service and provides a unified\n",
      "\n",
      "location to store all organizational data where the experiences operate.\n",
      "\n",
      "OneLake is built on top of ADLS (Azure Data Lake Storage) Gen2. It provides a single\n",
      "\n",
      "SaaS experience and a tenant-wide store for data that serves both professional and\n",
      "\n",
      "citizen developers. The OneLake SaaS experience simplifies the experiences, eliminating\n",
      "\n",
      "the need for users to understand any infrastructure concepts such as resource groups,\n",
      "\n",
      "RBAC (Role-Based Access Control), Azure Resource Manager, redundancy, or regions.\n",
      "\n",
      "Additionally it doesn't require the user to even have an Azure account.\n",
      "\n",
      "OneLake eliminates today's pervasive and chaotic data silos, which individual developers\n",
      "Source File: Fabric Get Started.pdf\n",
      "\n",
      "Id: Fabric_Get_Started_pdf-3\n",
      "Content: Fabric allows creators to concentrate on producing their best work, freeing them from\n",
      "\n",
      "the need to integrate, manage, or understand the underlying infrastructure that\n",
      "\n",
      "supports the experience.\n",
      "\n",
      "Components of Microsoft Fabric\n",
      "\n",
      "Microsoft Fabric offers the comprehensive set of analytics experiences designed to work\n",
      "\n",
      "together seamlessly. Each experience is tailored to a specific persona and a specific task.\n",
      "\n",
      "Fabric includes industry-leading experiences in the following categories for an end-to-\n",
      "\n",
      "end analytical need.\n",
      "\n",
      "Data Engineering - Data Engineering experience provides a world class Spark\n",
      "\n",
      "platform with great authoring experiences, enabling data engineers to perform\n",
      "\n",
      "large scale data transformation and democratize data through the lakehouse.\n",
      "\n",
      "Microsoft Fabric Spark's integration with Data Factory enables notebooks and\n",
      "\n",
      "spark jobs to be scheduled and orchestrated. For more information, see What is\n",
      "\n",
      "Data engineering in Microsoft Fabric?\n",
      "\n",
      "\fData Factory - Azure Data Factory combines the simplicity of Power Query with the\n",
      "\n",
      "scale and power of Azure Data Factory. You can use more than 200 native\n",
      "\n",
      "connectors to connect to data sources on-premises and in the cloud. For more\n",
      "\n",
      "information, see What is Data Factory in Microsoft Fabric?\n",
      "\n",
      "Data Science - Data Science experience enables you to build, deploy, and\n",
      "\n",
      "operationalize machine learning models seamlessly within your Fabric experience.\n",
      "\n",
      "It integrates with Azure Machine Learning to provide built-in experiment tracking\n",
      "Source File: Fabric Get Started.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Utilities.cogSearch import performCogSearch\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"What is Microsoft Fabric\"  \n",
    "\n",
    "results = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, 3)\n",
    "\n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Source File: {result['sourcefile']}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: Fabric_Get_Started_pdf-1\n",
      "Content: Tell us about your PDF experience.\n",
      "\n",
      "Microsoft Fabric get started\n",
      "documentation\n",
      "\n",
      "Microsoft Fabric is a unified platform that can meet your organization's data and\n",
      "analytics needs. Discover the Fabric shared and platform documentation from this page.\n",
      "\n",
      "About Microsoft Fabric\n",
      "\n",
      "ｅ OVERVIEW\n",
      "\n",
      "What is Fabric?\n",
      "\n",
      "Fabric terminology\n",
      "\n",
      "ｂ GET STARTED\n",
      "\n",
      "Start a Fabric trial\n",
      "\n",
      "Fabric home navigation\n",
      "\n",
      "End-to-end tutorials\n",
      "\n",
      "Context sensitive Help pane\n",
      "\n",
      "Get started with Fabric items\n",
      "\n",
      "ｐ CONCEPT\n",
      "\n",
      "Find items in OneLake data hub\n",
      "\n",
      "Promote and certify items\n",
      "\n",
      "ｃ HOW-TO GUIDE\n",
      "\n",
      "Apply sensitivity labels\n",
      "\n",
      "Workspaces\n",
      "\n",
      "ｐ CONCEPT\n",
      "\n",
      "Fabric workspace\n",
      "\n",
      "\fWorkspace roles\n",
      "\n",
      "ｂ GET STARTED\n",
      "\n",
      "Create a workspace\n",
      "\n",
      "ｃ HOW-TO GUIDE\n",
      "\n",
      "Workspace access control\n",
      "\n",
      "\fWhat is Microsoft Fabric?\n",
      "\n",
      "Article • 05/23/2023\n",
      "\n",
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything\n",
      "\n",
      "from data movement to data science, Real-Time Analytics, and business intelligence. It\n",
      "\n",
      "offers a comprehensive suite of services, including data lake, data engineering, and data\n",
      "\n",
      "integration, all in one place.\n",
      "\n",
      "With Fabric, you don't need to piece together different services from multiple vendors.\n",
      "\n",
      "Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is\n",
      "\n",
      "designed to simplify your analytics needs.\n",
      "\n",
      "The platform is built on a foundation of Software as a Service (SaaS), which takes\n",
      "\n",
      "simplicity and integration to a whole new level.\n",
      "\n",
      "） Important\n",
      "Source File: Fabric Get Started.pdf\n",
      "\n",
      "Id: Fabric_Get_Started_pdf-5\n",
      "Content: Fabric brings together all these experiences into a unified platform to offer the most\n",
      "\n",
      "comprehensive big data analytics platform in the industry.\n",
      "\n",
      "Microsoft Fabric enables organizations, and individuals, to turn large and complex data\n",
      "\n",
      "repositories into actionable workloads and analytics, and is an implementation of data\n",
      "\n",
      "mesh architecture. To learn more about data mesh, visit the article that explains data\n",
      "\n",
      "mesh architecture.\n",
      "\n",
      "OneLake and lakehouse - the unification of\n",
      "lakehouses\n",
      "\n",
      "\fThe Microsoft Fabric platform unifies the OneLake and lakehouse architecture across the\n",
      "\n",
      "enterprises.\n",
      "\n",
      "OneLake\n",
      "\n",
      "The data lake is the foundation on which all the Fabric services are built. Microsoft Fabric\n",
      "\n",
      "Lake is also known as OneLake. It's built into the Fabric service and provides a unified\n",
      "\n",
      "location to store all organizational data where the experiences operate.\n",
      "\n",
      "OneLake is built on top of ADLS (Azure Data Lake Storage) Gen2. It provides a single\n",
      "\n",
      "SaaS experience and a tenant-wide store for data that serves both professional and\n",
      "\n",
      "citizen developers. The OneLake SaaS experience simplifies the experiences, eliminating\n",
      "\n",
      "the need for users to understand any infrastructure concepts such as resource groups,\n",
      "\n",
      "RBAC (Role-Based Access Control), Azure Resource Manager, redundancy, or regions.\n",
      "\n",
      "Additionally it doesn't require the user to even have an Azure account.\n",
      "\n",
      "OneLake eliminates today's pervasive and chaotic data silos, which individual developers\n",
      "Source File: Fabric Get Started.pdf\n",
      "\n",
      "Id: Fabric_Get_Started_pdf-3\n",
      "Content: Fabric allows creators to concentrate on producing their best work, freeing them from\n",
      "\n",
      "the need to integrate, manage, or understand the underlying infrastructure that\n",
      "\n",
      "supports the experience.\n",
      "\n",
      "Components of Microsoft Fabric\n",
      "\n",
      "Microsoft Fabric offers the comprehensive set of analytics experiences designed to work\n",
      "\n",
      "together seamlessly. Each experience is tailored to a specific persona and a specific task.\n",
      "\n",
      "Fabric includes industry-leading experiences in the following categories for an end-to-\n",
      "\n",
      "end analytical need.\n",
      "\n",
      "Data Engineering - Data Engineering experience provides a world class Spark\n",
      "\n",
      "platform with great authoring experiences, enabling data engineers to perform\n",
      "\n",
      "large scale data transformation and democratize data through the lakehouse.\n",
      "\n",
      "Microsoft Fabric Spark's integration with Data Factory enables notebooks and\n",
      "\n",
      "spark jobs to be scheduled and orchestrated. For more information, see What is\n",
      "\n",
      "Data engineering in Microsoft Fabric?\n",
      "\n",
      "\fData Factory - Azure Data Factory combines the simplicity of Power Query with the\n",
      "\n",
      "scale and power of Azure Data Factory. You can use more than 200 native\n",
      "\n",
      "connectors to connect to data sources on-premises and in the cloud. For more\n",
      "\n",
      "information, see What is Data Factory in Microsoft Fabric?\n",
      "\n",
      "Data Science - Data Science experience enables you to build, deploy, and\n",
      "\n",
      "operationalize machine learning models seamlessly within your Fabric experience.\n",
      "\n",
      "It integrates with Azure Machine Learning to provide built-in experiment tracking\n",
      "Source File: Fabric Get Started.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector Search with Multi-language support\n",
    "query = \"¿Qué es Microsoft Fabric?\"\n",
    "\n",
    "results = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, 3)\n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Id: {result['id']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Source File: {result['sourcefile']}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from unstructured.chunking.title import chunk_by_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PartitionFile(fileExtension: str, fileName: str):      \n",
    "    \"\"\" uses the unstructured.io libraries to analyse a document\n",
    "    Returns:\n",
    "        elements: A list of available models\n",
    "    \"\"\"  \n",
    "    # Send a GET request to the URL to download the file\n",
    "    #readBytes  = getBlob(OpenAiDocConnStr, OpenAiDocContainer, fileName)\n",
    "    with open(fileName, \"rb\") as file:\n",
    "        readByte = file.read()\n",
    "        readBytes = BytesIO(readByte)\n",
    "\n",
    "    metadata = [] \n",
    "    elements = None\n",
    "    try:        \n",
    "        if fileExtension == '.csv':\n",
    "            from unstructured.partition.csv import partition_csv\n",
    "            elements = partition_csv(file=readBytes)               \n",
    "                     \n",
    "        elif fileExtension == '.doc':\n",
    "            from unstructured.partition.doc import partition_doc\n",
    "            elements = partition_doc(file=readBytes) \n",
    "            \n",
    "        elif fileExtension == '.docx':\n",
    "            from unstructured.partition.docx import partition_docx\n",
    "            elements = partition_docx(file=readBytes)\n",
    "            \n",
    "        elif fileExtension == '.eml' or fileExtension == '.msg':\n",
    "            if fileExtension == '.msg':\n",
    "                from unstructured.partition.msg import partition_msg\n",
    "                elements = partition_msg(file=readBytes) \n",
    "            else:        \n",
    "                from unstructured.partition.email import partition_email\n",
    "                elements = partition_email(file=readBytes)\n",
    "            metadata.append(f'Subject: {elements[0].metadata.subject}')\n",
    "            metadata.append(f'From: {elements[0].metadata.sent_from[0]}')\n",
    "            sent_to_str = 'To: '\n",
    "            for sent_to in elements[0].metadata.sent_to:\n",
    "                sent_to_str = sent_to_str + \" \" + sent_to\n",
    "            metadata.append(sent_to_str)\n",
    "            \n",
    "        elif fileExtension == '.html' or fileExtension == '.htm':  \n",
    "            from unstructured.partition.html import partition_html\n",
    "            elements = partition_html(file=readBytes) \n",
    "            \n",
    "        elif fileExtension == '.md':\n",
    "            from unstructured.partition.md import partition_md\n",
    "            elements = partition_md(file=readBytes)\n",
    "                       \n",
    "        elif fileExtension == '.ppt':\n",
    "            from unstructured.partition.ppt import partition_ppt\n",
    "            elements = partition_ppt(file=readBytes)\n",
    "            \n",
    "        elif fileExtension == '.pptx':    \n",
    "            from unstructured.partition.pptx import partition_pptx\n",
    "            elements = partition_pptx(file=readBytes)\n",
    "            \n",
    "        elif any(fileExtension in x for x in ['.txt', '.json']):\n",
    "            from unstructured.partition.text import partition_text\n",
    "            elements = partition_text(file=readBytes)\n",
    "            \n",
    "        elif fileExtension == '.xlsx':\n",
    "            from unstructured.partition.xlsx import partition_xlsx\n",
    "            elements = partition_xlsx(file=readBytes)\n",
    "            \n",
    "        elif fileExtension == '.xml':\n",
    "            from unstructured.partition.xml import partition_xml\n",
    "            elements = partition_xml(file=readBytes)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred trying to parse the file: {str(e)}\")\n",
    "         \n",
    "    return elements, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Mode elements to let UIO chunk it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain.document_loaders import UnstructuredFileLoader\n",
    "    from unstructured.cleaners.core import clean_extra_whitespace, group_broken_paragraphs\n",
    "\n",
    "    fileName = \"./Data/Gru/Multi-modal RAG.pptx\"\n",
    "\n",
    "    loader = UnstructuredFileLoader(fileName, mode=\"elements\", strategy=\"fast\", post_processors=[clean_extra_whitespace, group_broken_paragraphs])\n",
    "                                    # unstructured_kwargs={\"multipage_sections\":True, \n",
    "                                    #                      \"new_after_n_chars\":1500, \n",
    "                                    #                      \"combine_text_under_n_chars\":500, \n",
    "                                    #                      \"max_characters\":2500}) \n",
    "    rawDocs = loader.load()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred trying to parse the file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Research CoPilot', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Multimodal RAG with Code Execution (RAG-CE)', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 1, 'languages': ['eng'], 'parent_id': 'd31b823e761a10ea51bd38072fe0fa6d', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Agenda', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Process', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': '0fdf485f5bfd3762467100246df7a6ce', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Why do we need this?', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'e083bd83e9d3e97e3b3e7dbe6cca0dca', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Examples and Findings', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'e083bd83e9d3e97e3b3e7dbe6cca0dca', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='User Interface', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'e083bd83e9d3e97e3b3e7dbe6cca0dca', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='The Process', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'e083bd83e9d3e97e3b3e7dbe6cca0dca', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Ingestion', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 3, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'ceee59b5dd272eb7c9051ebcfcea332b', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Search', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 3, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'ceee59b5dd272eb7c9051ebcfcea332b', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Content Generation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 3, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'ceee59b5dd272eb7c9051ebcfcea332b', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Limitations', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'e083bd83e9d3e97e3b3e7dbe6cca0dca', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Improvements and Detection Process.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 3, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': '6f1c4cf81c3c2e0a0d042e7ae38d4012', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Error control', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 3, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': '6f1c4cf81c3c2e0a0d042e7ae38d4012', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Live Demo', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 2, 'languages': ['eng'], 'parent_id': 'e083bd83e9d3e97e3b3e7dbe6cca0dca', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Why do we need this?', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 3, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Existing Challenges', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 4, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='To be able to search through a knowledge base with RAG, text from documents need to be extracted, chunked and stored in a vector database', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 4, 'languages': ['eng'], 'parent_id': '67f5753c5fae111a52601aa425b2fede', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='This process now is purely concerned with text:', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 4, 'languages': ['eng'], 'parent_id': '67f5753c5fae111a52601aa425b2fede', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='If the documents have any images, graphs or tables, these elements are usually either ignored or extracted as messy unstructured text', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 4, 'languages': ['eng'], 'parent_id': '11587f732a66b9fa0182519844f63795', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Retrieving unstructured table data through RAG will lead to very low accuracy answers', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 4, 'languages': ['eng'], 'parent_id': '11587f732a66b9fa0182519844f63795', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='LLMs are usually very bad with numbers. If the query requires any sort of calculations, LLMs usually hallucinate or make basic math mistakes', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 4, 'languages': ['eng'], 'parent_id': '67f5753c5fae111a52601aa425b2fede', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Why do we need this?', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 5, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Ingest and interact with multi-modal analytics documents with lots of graphs, numbers and tables', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 5, 'languages': ['eng'], 'parent_id': 'a4d03c0b194a4eb64c85f26517e240e7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'UncategorizedText'}),\n",
       " Document(page_content='Extract structured information from some elements in documents which wasn’t possible before:', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 5, 'languages': ['eng'], 'parent_id': 'a4d03c0b194a4eb64c85f26517e240e7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Images', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 5, 'languages': ['eng'], 'parent_id': 'a4d03c0b194a4eb64c85f26517e240e7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Graphs', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 5, 'languages': ['eng'], 'parent_id': 'a4d03c0b194a4eb64c85f26517e240e7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Tables', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 5, 'languages': ['eng'], 'parent_id': 'a4d03c0b194a4eb64c85f26517e240e7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Use the Code Interpreter to formulate answers where calculations are needed based on search results', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 5, 'languages': ['eng'], 'parent_id': 'e3fe2a3fe52c9c35874478d22328281e', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Examples of Industry Applications', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Analyze Investment opportunity documents for Private Equity deals', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'parent_id': 'a8f49d9d42efe9795cf7d996e704748f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Analyze tables from tax documents for audit purposes', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'parent_id': 'a8f49d9d42efe9795cf7d996e704748f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Analyze financial statements and perform initial computations', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'parent_id': '8daec2ceb05dc9a6d61c442b940e4a03', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Analyze and interact with multi-modal Manufacturing documents', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'parent_id': 'a8f49d9d42efe9795cf7d996e704748f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Process academic and research papers', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'parent_id': 'a8f49d9d42efe9795cf7d996e704748f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Ingest and interact with textbooks, manuals and guides', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'parent_id': 'a8f49d9d42efe9795cf7d996e704748f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Analyze traffic and city planning documents', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 6, 'languages': ['eng'], 'parent_id': 'a8f49d9d42efe9795cf7d996e704748f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Examples and Findings', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 7, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Table Extraction – Data Table', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 8, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Original Table', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 8, 'languages': ['eng'], 'parent_id': '0c8bd5a9aa4855460aa08b074a6bc5d5', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Generated Python and Markdown', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 8, 'languages': ['eng'], 'parent_id': '0c8bd5a9aa4855460aa08b074a6bc5d5', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Table Extraction \\n\\n-\\n\\n Graph', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 9, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Original Table', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 9, 'languages': ['eng'], 'parent_id': '5f67f35807681e2ca792243017e6e28d', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Generated Python and Markdown', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 9, 'languages': ['eng'], 'parent_id': '5f67f35807681e2ca792243017e6e28d', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Image Explanation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 10, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Image', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 10, 'languages': ['eng'], 'parent_id': '598b81264d0bedc39446600c4b187764', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Excerpts from the Text Explanation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 10, 'languages': ['eng'], 'parent_id': '598b81264d0bedc39446600c4b187764', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='The image depicts a vibrant and busy laboratory setting filled with numerous minions, which are small, yellow, cylindrical creatures wearing goggles and blue overalls. The laboratory is packed with a variety of whimsical and colorful machinery, gadgets, and equipment that give off a cartoonish and inventive vibe.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 10, 'languages': ['eng'], 'parent_id': 'de5b871aecd5442083e7ba2f7f91d514', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content=\"There are multiple levels to the laboratory, connected by stairs and walkways. The minions are engaged in various activities, some are working on machinery, others are carrying objects, and a few are interacting with each other. The central area features a round platform with a character standing at a desk, surrounded by stools. The character's face is blurred, but they appear to be working on a blueprint or design. The overall setting is indoors, with large windows allowing natural light to illuminate the space, and the walls are lined with shelves holding various tools and devices.\", metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 10, 'languages': ['eng'], 'parent_id': 'de5b871aecd5442083e7ba2f7f91d514', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Mermaid', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Original Image', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Extracted Mermaid Code', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='graph TD;', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='CEO[Felonius Gru CEO] -->|Directs| LS[Dr. Nefario Lead Scientist]', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='CEO -->|Manages| HO[Kevin Head of Operations]', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='CEO -->|Oversees| CFO[Stuart Chief Financial Officer]', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='LS --> RnD[R&D Team 50 Minions]', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='HO --> Manu[Manufacturing Team 75 Minions]', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='CFO --> SnM[Sales and Marketing 30 Minions]', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Link', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 11, 'languages': ['eng'], 'parent_id': '1356a57a7a4b2869bf5e612c3d1761f1', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Numbers - Code Harvesting from Plain Text', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 12, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Processed Extracted Text', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 12, 'languages': ['eng'], 'parent_id': '72fc97737fe7284e5800ab943d449b4b', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Harvested Code', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 12, 'languages': ['eng'], 'parent_id': '72fc97737fe7284e5800ab943d449b4b', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='RAG\\n\\n-\\n\\nCE: Using Taskweaver', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 13, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Findings', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 14, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='GPT-4-Turbo is a great help with its large 128k token window', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 14, 'languages': ['eng'], 'parent_id': 'e171c2ff25b55e5a2d63d081ec3a65e2', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='GPT-4-Turbo with Vision is great at extracting tables from unstructured document formats', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 14, 'languages': ['eng'], 'parent_id': 'e171c2ff25b55e5a2d63d081ec3a65e2', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='GPT-4 models can understand a wide variety of formats (Python, Markdown, Mermaid, GraphViz DOT, etc..) which was essential in maximizing information extraction', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 14, 'languages': ['eng'], 'parent_id': 'e171c2ff25b55e5a2d63d081ec3a65e2', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='A new approach to vector index searching based on tags was needed because the Generation Prompts were very lengthy compared to the usual user queries', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 14, 'languages': ['eng'], 'parent_id': 'e171c2ff25b55e5a2d63d081ec3a65e2', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Taskweaver’s and Assistants API’s Code Interpreters were introduced to conduct open-ended analytics questions', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 14, 'languages': ['eng'], 'parent_id': 'e171c2ff25b55e5a2d63d081ec3a65e2', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='User Interface', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 15, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='User interface', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 16, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='References in the UI', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 17, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='References that can be easily be read', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 17, 'languages': ['eng'], 'parent_id': '2e416a7aa5911e8688a45db148bcf089', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='PDF Viewer of the reference document as part of the information audit trail', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 17, 'languages': ['eng'], 'parent_id': '2e416a7aa5911e8688a45db148bcf089', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'UncategorizedText'}),\n",
       " Document(page_content='User Interface – A full Audit Trail', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 18, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Automatically generated Excel sheet by Taskweaver or Assistants API that includes the numbers mentioned in the final answer, so that the user has a working version to interact with.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 18, 'languages': ['eng'], 'parent_id': 'b5f6d5a95ed9b10a3783860afd4c4cb3', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='The Process', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 19, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Overall approach', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 20, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Phase 1: Ingestion Design and Implementation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 20, 'languages': ['eng'], 'parent_id': 'a39ac20c90ace9039ffa06e0e4f03a8f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Phase 2: Search Design and Implementation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 20, 'languages': ['eng'], 'parent_id': 'a39ac20c90ace9039ffa06e0e4f03a8f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Phase 3: Generation Design and Implementation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 20, 'languages': ['eng'], 'parent_id': 'a39ac20c90ace9039ffa06e0e4f03a8f', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Phase 1: Exploration overview', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Data analysis of the source documents provided by ADIA.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '9788b34c4aa7ae9759bdee5725a50b75', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='3 Screening outputs for 3 different cases', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '9788b34c4aa7ae9759bdee5725a50b75', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Set of input documents per case', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '9788b34c4aa7ae9759bdee5725a50b75', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Open AI models evaluation:', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '9788b34c4aa7ae9759bdee5725a50b75', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='GPT 4 Turbo', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '4e776b96999aadf07039603209306425', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='GPT 4 Vision', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '4e776b96999aadf07039603209306425', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Ada', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '4e776b96999aadf07039603209306425', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='GPT\\n\\n-\\n\\n35\\n\\n-\\n\\nTurbo', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 2, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '4e776b96999aadf07039603209306425', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Framework assessed: Autogen, Taskweaver', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '9788b34c4aa7ae9759bdee5725a50b75', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Python Libraries assessed: many PDF processing packages, visual processing packages of tables', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 21, 'languages': ['eng'], 'parent_id': '4f7074c57d8bce9590d9df194ec31806', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Phase 2: Document Ingestion Process', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 22, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Document Conversion: All documents, including MS Word and Powerpoint, are first converted to PDF format (for now manually, but can be programmatic)', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 22, 'languages': ['eng'], 'parent_id': '4604d9a916241a3613a6eec99be160b7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Page-Level Processing: Each PDF is broken into pages, with text extraction, image and table detection using GPT-4 and GPT-4V, followed by post-processing of images and tables.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 22, 'languages': ['eng'], 'parent_id': '4604d9a916241a3613a6eec99be160b7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Content Analysis and Saving: Extracted text is enhanced for readability with GPT-4, images and tables are described and converted to Markdown or Mermaid code, and relevant data is harvested as Python code.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 22, 'languages': ['eng'], 'parent_id': '4604d9a916241a3613a6eec99be160b7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Consolidation and Indexing: All processed content (text, image descriptions, table data) is consolidated per page and pushed into a vector index (Cognitive Search) with metadata including file links and page details.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 22, 'languages': ['eng'], 'parent_id': '4604d9a916241a3613a6eec99be160b7', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Phase 2: Document Ingestion Process \\x0bText, Images, Tables', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 23, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Phase 3: Search Process', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 24, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Based on a new tag-generated technique', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 24, 'languages': ['eng'], 'parent_id': '32c404f05f8eb652223205f7d8439f84', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Taskweaver is a MS-developed OSS library, and acts as a Code Interpreter with a Planner (2 Agents).', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 24, 'languages': ['eng'], 'parent_id': '32c404f05f8eb652223205f7d8439f84', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='It’s designed as a “Code-first” approach, which means the objects passed between Agents are Python native (so no need for string parsing).', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 24, 'languages': ['eng'], 'parent_id': '32c404f05f8eb652223205f7d8439f84', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Code Interpreter for Computation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 24, 'languages': ['eng'], 'parent_id': '32c404f05f8eb652223205f7d8439f84', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Phase 4: Content Generation', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Reverse engineering using GPT 4 Turbo and Vision.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': 'b985a4b153ab0d7e3f128f7642b7a126', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Processing the screening main sections through these models.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': 'fc509d255ab1c6741f8576be35629fb5', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Summarization and sanitization of data.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': 'b985a4b153ab0d7e3f128f7642b7a126', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Prompt engineering techniques', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': 'b985a4b153ab0d7e3f128f7642b7a126', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Prompt versioning exponential addition.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': '4ee149a20c92b374a36a3247c8aa2dd6', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Dynamic prompting techniques by Industry and Section.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': '4ee149a20c92b374a36a3247c8aa2dd6', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Model Self\\n\\n-\\n\\nevaluation.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': 'b985a4b153ab0d7e3f128f7642b7a126', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Correlation with the ingested data and AI search output to show relevant context information to the PED analyst.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 25, 'languages': ['eng'], 'parent_id': 'a6fcd1cce934fe6ff553a03399042739', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Limitations', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 26, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Limits of GPT-4V – Problems in Table and Graph Extraction', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 27, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='Error and Hallucination Detection', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 28, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'}),\n",
       " Document(page_content='We need a Human\\n\\n-\\n\\nin\\n\\n-\\n\\nthe\\n\\n-\\n\\nloop', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 28, 'languages': ['eng'], 'parent_id': '213b4563e8a0d597785cb361bcbb7eec', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Images or tables might be incorrectly read by the GPT models', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 28, 'languages': ['eng'], 'parent_id': '213b4563e8a0d597785cb361bcbb7eec', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='When it comes to Search, fallacies in the logic for producing the result might exist', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 28, 'languages': ['eng'], 'parent_id': '213b4563e8a0d597785cb361bcbb7eec', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='How do we manage this? There are 2 complementing ways:', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 28, 'languages': ['eng'], 'parent_id': '213b4563e8a0d597785cb361bcbb7eec', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Error Detection: with each generated answers, references are generated by the LLM for cross-checking', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 28, 'languages': ['eng'], 'parent_id': '6d6d394311e60179235af4a1e855e8cd', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Error Correction: if an error was detected in the ingested document, a UI view can be built to manually correct the error, and re-commit the contents as a file and in the vector store', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 1, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 28, 'languages': ['eng'], 'parent_id': '6d6d394311e60179235af4a1e855e8cd', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'NarrativeText'}),\n",
       " Document(page_content='Live Demo', metadata={'source': './Data/Gru/Multi-modal RAG.pptx', 'category_depth': 0, 'file_directory': './Data/Gru', 'filename': 'Multi-modal RAG.pptx', 'last_modified': '2024-03-06T10:46:51', 'page_number': 29, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation', 'category': 'Title'})]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Mode Single, we can use langchain to chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "fileName = \"./Data/Gru/Multi-modal RAG.pptx\"\n",
    "loader = UnstructuredFileLoader(fileName, post_processors=[clean_extra_whitespace, group_broken_paragraphs])\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\\n\", \"\\n\\n\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Research CoPilot\\n\\nMultimodal RAG with Code Execution (RAG-CE)\\n\\nAgenda\\n\\nProcess\\n\\nWhy do we need this?\\n\\nExamples and Findings\\n\\nUser Interface\\n\\nThe Process\\n\\nIngestion\\n\\nSearch\\n\\nContent Generation\\n\\nLimitations\\n\\nImprovements and Detection Process.\\n\\nError control\\n\\nLive Demo\\n\\nWhy do we need this?\\n\\nExisting Challenges\\n\\nTo be able to search through a knowledge base with RAG, text from documents need to be extracted, chunked and stored in a vector database\\n\\nThis process now is purely concerned with text:\\n\\nIf the documents have any images, graphs or tables, these elements are usually either ignored or extracted as messy unstructured text\\n\\nRetrieving unstructured table data through RAG will lead to very low accuracy answers\\n\\nLLMs are usually very bad with numbers. If the query requires any sort of calculations, LLMs usually hallucinate or make basic math mistakes\\n\\nWhy do we need this?\\n\\nIngest and interact with multi-modal analytics documents with lots of graphs, numbers and tables', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='LLMs are usually very bad with numbers. If the query requires any sort of calculations, LLMs usually hallucinate or make basic math mistakes\\n\\nWhy do we need this?\\n\\nIngest and interact with multi-modal analytics documents with lots of graphs, numbers and tables\\n\\nExtract structured information from some elements in documents which wasn’t possible before:\\n\\nImages\\n\\nGraphs\\n\\nTables\\n\\nUse the Code Interpreter to formulate answers where calculations are needed based on search results\\n\\nExamples of Industry Applications\\n\\nAnalyze Investment opportunity documents for Private Equity deals\\n\\nAnalyze tables from tax documents for audit purposes\\n\\nAnalyze financial statements and perform initial computations\\n\\nAnalyze and interact with multi-modal Manufacturing documents\\n\\nProcess academic and research papers\\n\\nIngest and interact with textbooks, manuals and guides\\n\\nAnalyze traffic and city planning documents\\n\\nExamples and Findings\\n\\nTable Extraction – Data Table\\n\\nOriginal Table', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='Analyze and interact with multi-modal Manufacturing documents\\n\\nProcess academic and research papers\\n\\nIngest and interact with textbooks, manuals and guides\\n\\nAnalyze traffic and city planning documents\\n\\nExamples and Findings\\n\\nTable Extraction – Data Table\\n\\nOriginal Table\\n\\nGenerated Python and Markdown\\n\\nTable Extraction \\n\\n-\\n\\n Graph\\n\\nOriginal Table\\n\\nGenerated Python and Markdown\\n\\nImage Explanation\\n\\nImage\\n\\nExcerpts from the Text Explanation\\n\\nThe image depicts a vibrant and busy laboratory setting filled with numerous minions, which are small, yellow, cylindrical creatures wearing goggles and blue overalls. The laboratory is packed with a variety of whimsical and colorful machinery, gadgets, and equipment that give off a cartoonish and inventive vibe.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content=\"There are multiple levels to the laboratory, connected by stairs and walkways. The minions are engaged in various activities, some are working on machinery, others are carrying objects, and a few are interacting with each other. The central area features a round platform with a character standing at a desk, surrounded by stools. The character's face is blurred, but they appear to be working on a blueprint or design. The overall setting is indoors, with large windows allowing natural light to illuminate the space, and the walls are lined with shelves holding various tools and devices.\\n\\nMermaid\\n\\nOriginal Image\\n\\nExtracted Mermaid Code\\n\\ngraph TD;\\n\\nCEO[Felonius Gru CEO] -->|Directs| LS[Dr. Nefario Lead Scientist]\\n\\nCEO -->|Manages| HO[Kevin Head of Operations]\\n\\nCEO -->|Oversees| CFO[Stuart Chief Financial Officer]\\n\\nLS --> RnD[R&D Team 50 Minions]\\n\\nHO --> Manu[Manufacturing Team 75 Minions]\\n\\nCFO --> SnM[Sales and Marketing 30 Minions]\\n\\nLink\\n\\nNumbers - Code Harvesting from Plain Text\", metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='CEO -->|Manages| HO[Kevin Head of Operations]\\n\\nCEO -->|Oversees| CFO[Stuart Chief Financial Officer]\\n\\nLS --> RnD[R&D Team 50 Minions]\\n\\nHO --> Manu[Manufacturing Team 75 Minions]\\n\\nCFO --> SnM[Sales and Marketing 30 Minions]\\n\\nLink\\n\\nNumbers - Code Harvesting from Plain Text\\n\\nProcessed Extracted Text\\n\\nHarvested Code\\n\\nRAG\\n\\n-\\n\\nCE: Using Taskweaver\\n\\nFindings\\n\\nGPT-4-Turbo is a great help with its large 128k token window\\n\\nGPT-4-Turbo with Vision is great at extracting tables from unstructured document formats\\n\\nGPT-4 models can understand a wide variety of formats (Python, Markdown, Mermaid, GraphViz DOT, etc..) which was essential in maximizing information extraction\\n\\nA new approach to vector index searching based on tags was needed because the Generation Prompts were very lengthy compared to the usual user queries\\n\\nTaskweaver’s and Assistants API’s Code Interpreters were introduced to conduct open-ended analytics questions\\n\\nUser Interface\\n\\nUser interface\\n\\nReferences in the UI', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='Taskweaver’s and Assistants API’s Code Interpreters were introduced to conduct open-ended analytics questions\\n\\nUser Interface\\n\\nUser interface\\n\\nReferences in the UI\\n\\nReferences that can be easily be read\\n\\nPDF Viewer of the reference document as part of the information audit trail\\n\\nUser Interface – A full Audit Trail\\n\\nAutomatically generated Excel sheet by Taskweaver or Assistants API that includes the numbers mentioned in the final answer, so that the user has a working version to interact with.\\n\\nThe Process\\n\\nOverall approach\\n\\nPhase 1: Ingestion Design and Implementation\\n\\nPhase 2: Search Design and Implementation\\n\\nPhase 3: Generation Design and Implementation\\n\\nPhase 1: Exploration overview\\n\\nData analysis of the source documents provided by ADIA.\\n\\n3 Screening outputs for 3 different cases\\n\\nSet of input documents per case\\n\\nOpen AI models evaluation:\\n\\nGPT 4 Turbo\\n\\nGPT 4 Vision\\n\\nAda\\n\\nGPT\\n\\n-\\n\\n35\\n\\n-\\n\\nTurbo\\n\\nFramework assessed: Autogen, Taskweaver', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='Phase 1: Exploration overview\\n\\nData analysis of the source documents provided by ADIA.\\n\\n3 Screening outputs for 3 different cases\\n\\nSet of input documents per case\\n\\nOpen AI models evaluation:\\n\\nGPT 4 Turbo\\n\\nGPT 4 Vision\\n\\nAda\\n\\nGPT\\n\\n-\\n\\n35\\n\\n-\\n\\nTurbo\\n\\nFramework assessed: Autogen, Taskweaver\\n\\nPython Libraries assessed: many PDF processing packages, visual processing packages of tables\\n\\nPhase 2: Document Ingestion Process\\n\\nDocument Conversion: All documents, including MS Word and Powerpoint, are first converted to PDF format (for now manually, but can be programmatic)\\n\\nPage-Level Processing: Each PDF is broken into pages, with text extraction, image and table detection using GPT-4 and GPT-4V, followed by post-processing of images and tables.\\n\\nContent Analysis and Saving: Extracted text is enhanced for readability with GPT-4, images and tables are described and converted to Markdown or Mermaid code, and relevant data is harvested as Python code.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='Content Analysis and Saving: Extracted text is enhanced for readability with GPT-4, images and tables are described and converted to Markdown or Mermaid code, and relevant data is harvested as Python code.\\n\\nConsolidation and Indexing: All processed content (text, image descriptions, table data) is consolidated per page and pushed into a vector index (Cognitive Search) with metadata including file links and page details.\\n\\nPhase 2: Document Ingestion Process \\x0bText, Images, Tables\\n\\nPhase 3: Search Process\\n\\nBased on a new tag-generated technique\\n\\nTaskweaver is a MS-developed OSS library, and acts as a Code Interpreter with a Planner (2 Agents).\\n\\nIt’s designed as a “Code-first” approach, which means the objects passed between Agents are Python native (so no need for string parsing).\\n\\nCode Interpreter for Computation\\n\\nPhase 4: Content Generation\\n\\nReverse engineering using GPT 4 Turbo and Vision.\\n\\nProcessing the screening main sections through these models.', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='Code Interpreter for Computation\\n\\nPhase 4: Content Generation\\n\\nReverse engineering using GPT 4 Turbo and Vision.\\n\\nProcessing the screening main sections through these models.\\n\\nSummarization and sanitization of data.\\n\\nPrompt engineering techniques\\n\\nPrompt versioning exponential addition.\\n\\nDynamic prompting techniques by Industry and Section.\\n\\nModel Self\\n\\n-\\n\\nevaluation.\\n\\nCorrelation with the ingested data and AI search output to show relevant context information to the PED analyst.\\n\\nLimitations\\n\\nLimits of GPT-4V – Problems in Table and Graph Extraction\\n\\nError and Hallucination Detection\\n\\nWe need a Human\\n\\n-\\n\\nin\\n\\n-\\n\\nthe\\n\\n-\\n\\nloop\\n\\nImages or tables might be incorrectly read by the GPT models\\n\\nWhen it comes to Search, fallacies in the logic for producing the result might exist\\n\\nHow do we manage this? There are 2 complementing ways:\\n\\nError Detection: with each generated answers, references are generated by the LLM for cross-checking', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'}),\n",
       " Document(page_content='When it comes to Search, fallacies in the logic for producing the result might exist\\n\\nHow do we manage this? There are 2 complementing ways:\\n\\nError Detection: with each generated answers, references are generated by the LLM for cross-checking\\n\\nError Correction: if an error was detected in the ingested document, a UI view can be built to manually correct the error, and re-commit the contents as a file and in the vector store\\n\\nLive Demo', metadata={'source': './Data/Gru/Multi-modal RAG.pptx'})]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternatively we can use our own way to define those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"./Data/Gru/Multi-modal RAG.pptx\"\n",
    "elements, uioMetadata = PartitionFile(os.path.splitext(fileName)[1], fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaDataText = ''\n",
    "for metadata_value in uioMetadata:\n",
    "    metaDataText += metadata_value + '\\n'    \n",
    "\n",
    "title = ''\n",
    "# Capture the file title\n",
    "try:\n",
    "    for i, element in enumerate(elements):\n",
    "        if title == '' and element.category == 'Title':\n",
    "            # capture the first title\n",
    "            title = element.text\n",
    "            break\n",
    "except:\n",
    "    # if this type of element does not include title, then process with empty value\n",
    "    pass\n",
    "chunks = chunk_by_title(elements, multipage_sections=True, new_after_n_chars=1500, combine_text_under_n_chars=500, max_characters=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Research CoPilot\\n\\nMultimodal RAG with Code Execution (RAG-CE)\\n\\nAgenda\\n\\nProcess\\n\\nWhy do we need this?\\n\\nExamples and Findings\\n\\nUser Interface\\n\\nThe Process\\n\\nIngestion\\n\\nSearch\\n\\nContent Generation\\n\\nLimitations \\n\\nImprovements and Detection Process. \\n\\nError control\\n\\nLive Demo\\n\\nWhy do we need this?\\n\\nExisting Challenges\\n\\nTo be able to search through a knowledge base with RAG, text from documents need to be extracted, chunked and stored in a vector database\\n\\nThis process now is purely concerned with text: \\n\\nIf the documents have any images, graphs or tables, these elements are usually either ignored or extracted as messy unstructured text\\n\\nRetrieving unstructured table data through RAG will lead to very low accuracy answers\\n\\nLLMs are usually very bad with numbers. If the query requires any sort of calculations, LLMs usually hallucinate or make basic math mistakes', metadata={'id': 0, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [1]}),\n",
       " Document(page_content='Why do we need this? \\n\\nIngest and interact with multi-modal analytics documents with lots of graphs, numbers and tables\\n\\nExtract structured information from some elements in documents which wasn’t possible before:\\n\\nImages\\n\\nGraphs\\n\\nTables\\n\\nUse the Code Interpreter to formulate answers where calculations are needed based on search results \\n\\nExamples of Industry Applications\\n\\nAnalyze Investment opportunity documents for Private Equity deals\\n\\nAnalyze tables from tax documents for audit purposes\\n\\nAnalyze financial statements and perform initial computations', metadata={'id': 1, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [5]}),\n",
       " Document(page_content=\"Analyze and interact with multi-modal Manufacturing documents \\n\\nProcess academic and research papers\\n\\nIngest and interact with textbooks, manuals and guides\\n\\nAnalyze traffic and city planning documents \\n\\nExamples and Findings\\n\\nTable Extraction – Data Table\\n\\nOriginal Table\\n\\nGenerated Python and Markdown\\n\\nTable Extraction - Graph\\n\\nOriginal Table\\n\\nGenerated Python and Markdown\\n\\nImage Explanation \\n\\nImage\\n\\nExcerpts from the Text Explanation\\n\\nThe image depicts a vibrant and busy laboratory setting filled with numerous minions, which are small, yellow, cylindrical creatures wearing goggles and blue overalls. The laboratory is packed with a variety of whimsical and colorful machinery, gadgets, and equipment that give off a cartoonish and inventive vibe. \\n\\nThere are multiple levels to the laboratory, connected by stairs and walkways. The minions are engaged in various activities, some are working on machinery, others are carrying objects, and a few are interacting with each other. The central area features a round platform with a character standing at a desk, surrounded by stools. The character's face is blurred, but they appear to be working on a blueprint or design. The overall setting is indoors, with large windows allowing natural light to illuminate the space, and the walls are lined with shelves holding various tools and devices.\", metadata={'id': 2, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [6]}),\n",
       " Document(page_content='Mermaid\\n\\nOriginal Image\\n\\nExtracted Mermaid Code\\n\\ngraph TD;\\n\\n    CEO[Felonius Gru CEO] -->|Directs| LS[Dr. Nefario Lead Scientist]\\n\\n    CEO -->|Manages| HO[Kevin Head of Operations]\\n\\n    CEO -->|Oversees| CFO[Stuart Chief Financial Officer]\\n\\n    LS --> RnD[R&D Team 50 Minions]\\n\\n    HO --> Manu[Manufacturing Team 75 Minions]\\n\\n    CFO --> SnM[Sales and Marketing 30 Minions]\\n\\nLink\\n\\nNumbers - Code Harvesting from Plain Text\\n\\nProcessed Extracted Text\\n\\nHarvested Code\\n\\nRAG-CE: Using Taskweaver\\n\\nFindings\\n\\nGPT-4-Turbo is a great help with its large 128k token window\\n\\nGPT-4-Turbo with Vision is great at extracting tables from unstructured document formats\\n\\nGPT-4 models can understand a wide variety of formats (Python, Markdown, Mermaid, GraphViz DOT, etc..) which was essential in maximizing  information extraction\\n\\nA new approach to vector index searching based on tags was needed because the Generation Prompts were very lengthy compared to the usual user queries\\n\\nTaskweaver’s and Assistants API’s Code Interpreters were introduced to conduct open-ended analytics questions', metadata={'id': 3, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [11]}),\n",
       " Document(page_content='User Interface\\n\\nUser interface\\n\\nReferences in the UI\\n\\nReferences that can be easily be read\\n\\nPDF Viewer of the reference document as part of the information audit trail\\n\\nUser Interface – A full Audit Trail\\n\\nAutomatically generated Excel sheet by Taskweaver or Assistants API that includes the numbers mentioned in the final answer, so that the user has a working version to interact with.\\n\\nThe Process\\n\\nOverall approach\\n\\nPhase 1: Ingestion Design and Implementation\\n\\nPhase 2: Search Design and Implementation', metadata={'id': 4, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [15]}),\n",
       " Document(page_content='Phase 3: Generation Design and Implementation\\n\\nPhase 1: Exploration overview\\n\\nData analysis of the source documents provided by ADIA. \\n\\n3 Screening outputs for 3 different cases\\n\\nSet of input documents per case\\n\\nOpen AI models evaluation:\\n\\nGPT 4 Turbo\\n\\nGPT 4 Vision\\n\\nAda\\n\\nGPT-35-Turbo\\n\\nFramework assessed: Autogen, Taskweaver\\n\\nPython Libraries assessed: many PDF processing packages, visual processing packages of tables\\n\\nPhase 2: Document Ingestion Process\\n\\nDocument Conversion: All documents, including MS Word and Powerpoint, are first converted to PDF format (for now manually, but can be programmatic)\\n\\nPage-Level Processing: Each PDF is broken into pages, with text extraction, image and table detection using GPT-4 and GPT-4V, followed by post-processing of images and tables.\\n\\nContent Analysis and Saving: Extracted text is enhanced for readability with GPT-4, images and tables are described and converted to Markdown or Mermaid code, and relevant data is harvested as Python code.\\n\\nConsolidation and Indexing: All processed content (text, image descriptions, table data) is consolidated per page and pushed into a vector index (Cognitive Search) with metadata including file links and page details.', metadata={'id': 5, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [20]}),\n",
       " Document(page_content='Phase 2: Document Ingestion Process \\x0bText, Images, Tables\\n\\nPhase 3: Search Process\\n\\nBased on a new tag-generated technique\\n\\nTaskweaver is a MS-developed OSS library, and acts as a Code Interpreter with a Planner (2 Agents).\\n\\nIt’s designed as a “Code-first” approach, which means the objects passed between Agents are Python native (so no need for string parsing).\\n\\nCode Interpreter for Computation\\n\\nPhase 4: Content Generation\\n\\nReverse engineering using GPT 4 Turbo and Vision.\\n\\nProcessing the screening main sections through these models.', metadata={'id': 6, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [23]}),\n",
       " Document(page_content='Summarization and sanitization of data.\\n\\nPrompt engineering techniques\\n\\nPrompt versioning exponential addition.\\n\\nDynamic prompting techniques by Industry and Section.\\n\\nModel Self-evaluation.\\n\\nCorrelation with the ingested data and AI search output to show relevant context information to the PED analyst.\\n\\nLimitations\\n\\nLimits of GPT-4V – Problems in Table and Graph Extraction\\n\\nError and Hallucination Detection\\n\\nWe need a Human-in-the-loop\\n\\nImages or tables might be incorrectly read by the GPT models\\n\\nWhen it comes to Search, fallacies in the logic for producing the result might exist\\n\\nHow do we manage this? There are 2 complementing ways:\\n\\nError Detection: with each generated answers, references are generated by the LLM for cross-checking\\n\\nError Correction: if an error was detected in the ingested document, a UI view can be built to manually correct the error, and re-commit the contents as a file and in the vector store', metadata={'id': 7, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [25]}),\n",
       " Document(page_content='Live Demo', metadata={'id': 8, 'source': './Data/Gru/Multi-modal RAG.pptx', 'title': 'Research CoPilot', 'subtitle': '', 'section': '', 'page': [29]})]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subTitleName = ''\n",
    "sectionName = ''\n",
    "rawDocs1 = []\n",
    "from langchain.docstore.document import Document\n",
    "# Complete and write chunks\n",
    "for i, chunk in enumerate(chunks):      \n",
    "    if chunk.metadata.page_number == None:\n",
    "        page_list = [1]\n",
    "    else:\n",
    "        page_list = [chunk.metadata.page_number] \n",
    "    # substitute html if text is a table            \n",
    "    if chunk.category == 'Table':\n",
    "        chunk_text = chunk.metadata.text_as_html\n",
    "    else:\n",
    "        chunk_text = chunk.text\n",
    "    # add filetype specific metadata as chunk text header\n",
    "    chunk_text = metaDataText + chunk_text\n",
    "    #print(f\"Chunk {i} - Page: {page_list} - Category: {chunk.category} - Text: {chunk_text}\")\n",
    "    rawDocs1.append(Document(page_content=chunk_text, \n",
    "                             metadata={\"id\": i, \"source\": fileName, \n",
    "                                       \"title\": title, \n",
    "                                       \"subtitle\": subTitleName, \n",
    "                                       \"section\": sectionName, \n",
    "                                       \"page\": page_list}))\n",
    "    \n",
    "rawDocs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1004, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='What is the sales and trading revenue for equities for all year across all companies.  Display the output as Table with columns as company, year and revenue(figures in million)\\nHow differently those banks are handling CCAR.  Give me the answer in bulleted format with breakdown by company and by each year\\nCompare and contrast the revenue between 2021 and 2022.  Display the output as JSON object with keys as company, year and revenue\\nWhat strategies each company is using to optimize cash management?. Give me the answer in bulleted format with breakdown by company\\nWhat is the status of LIBOR Transitions over the years for all companies. If there\\'s no information for a specific year or company, just say \"No Information\" for that specific year and company.  Breakdown the answer in bulleted list by company and year with minimum of 3 paragraphs to maximum of 7 paragraphs for each company and year\\nWhat is the status of LIBOR Transitions?  Provide the information with breakdown for year and company', metadata={'id': 'doc:deleteme:e1ea367f6a5f44b6b5a699dba0b49e5a', 'source': './Data/Compare and Contrast.txt'}), 0.2704), (Document(page_content='Compare and contrast the cash flow between 2021 and 2022. Show me the information in Table format with columns as company, year, cash flow(figures in millions)\\nWhich customer segments and geographies grew the fastest for JPMC\\nCan you compare and contrast the risk factors in 2021 vs. 2020 in bulleted list for each company?\\nWhat is the revenue from equity derivatives business for the companies over the years where the information is available.  The output should be in table format with columns as company, year and amount\\nCompare revenue growth of Morgan Stanley from 2020 to 2021.  Show the growth comparison in millions\\nCompare and contrast the cash flow between 2021 and 2022 across Bank of America, JP Morgan Chase  . Show me the information in Table format with columns as company, year, cash flow(figures in millions)\\nWhat is the status of LIBOR Transitions for the year 2022.  Display the information with breakdown across each company', metadata={'id': 'doc:deleteme:b5336b3d9f8d4e5ca815ae0459009a13', 'source': './Data/Compare and Contrast.txt'}), 0.2712)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.redis import Redis\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(azure_endpoint=OpenAiEndPoint, azure_deployment=OpenAiEmbedding, api_key=OpenAiKey, openai_api_type=\"azure\")\n",
    "\n",
    "loader = TextLoader(\"./Data/Compare and Contrast.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "redisUrl = \"redis://default:\" + os.getenv(\"RedisPassword\") + \"@\" + os.getenv(\"RedisAddress\") + \":\" + os.getenv(\"RedisPort\")\n",
    "rds = Redis.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    redis_url=redisUrl,\n",
    "    index_name=\"deleteme\",\n",
    ")\n",
    "\n",
    "\n",
    "results = rds.similarity_search_with_score(\"What is document about.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds.write_schema(\"redis_schema.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexSchema = {\n",
    "    \"text\": [{\"name\": \"source\"}, {\"name\": \"content\"}],\n",
    "    \"vector\": [{\"name\": \"content_vector\", \"dims\": 768, \"algorithm\": \"FLAT\", \"distance_metric\": \"COSINE\"}],\n",
    "}\n",
    "\n",
    "redisUrl = \"redis://default:\" + os.getenv(\"RedisPassword\") + \"@\" + os.getenv(\"RedisAddress\") + \":\" + os.getenv(\"RedisPort\")\n",
    "\n",
    "existingRds = Redis.from_existing_index(\n",
    "    embeddings,\n",
    "    index_name=\"deleteme\",\n",
    "    redis_url=redisUrl,\n",
    "    schema=indexSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = existingRds.similarity_search(\"Sales revenue\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What is the sales and trading revenue for equities for all year across all companies.  Display the output as Table with columns as company, year and revenue(figures in million)\\nHow differently those banks are handling CCAR.  Give me the answer in bulleted format with breakdown by company and by each year\\nCompare and contrast the revenue between 2021 and 2022.  Display the output as JSON object with keys as company, year and revenue\\nWhat strategies each company is using to optimize cash management?. Give me the answer in bulleted format with breakdown by company\\nWhat is the status of LIBOR Transitions over the years for all companies. If there\\'s no information for a specific year or company, just say \"No Information\" for that specific year and company.  Breakdown the answer in bulleted list by company and year with minimum of 3 paragraphs to maximum of 7 paragraphs for each company and year\\nWhat is the status of LIBOR Transitions?  Provide the information with breakdown for year and company', metadata={'id': 'doc:deleteme:e1ea367f6a5f44b6b5a699dba0b49e5a', 'source': './Data/Compare and Contrast.txt'}),\n",
       " Document(page_content='Compare and contrast the cash flow between 2021 and 2022. Show me the information in Table format with columns as company, year, cash flow(figures in millions)\\nWhich customer segments and geographies grew the fastest for JPMC\\nCan you compare and contrast the risk factors in 2021 vs. 2020 in bulleted list for each company?\\nWhat is the revenue from equity derivatives business for the companies over the years where the information is available.  The output should be in table format with columns as company, year and amount\\nCompare revenue growth of Morgan Stanley from 2020 to 2021.  Show the growth comparison in millions\\nCompare and contrast the cash flow between 2021 and 2022 across Bank of America, JP Morgan Chase  . Show me the information in Table format with columns as company, year, cash flow(figures in millions)\\nWhat is the status of LIBOR Transitions for the year 2022.  Display the information with breakdown across each company', metadata={'id': 'doc:deleteme:b5336b3d9f8d4e5ca815ae0459009a13', 'source': './Data/Compare and Contrast.txt'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0      20000    2          2         1   24      2      2     -1     -1   \n",
       "1     120000    2          2         2   26     -1      2      0      0   \n",
       "2      90000    2          2         2   34      0      0      0      0   \n",
       "3      50000    2          2         1   37      0      0      0      0   \n",
       "4      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0     -2  ...          0          0          0         0       689         0   \n",
       "1      0  ...       3272       3455       3261         0      1000      1000   \n",
       "2      0  ...      14331      14948      15549      1518      1500      1000   \n",
       "3      0  ...      28314      28959      29547      2000      2019      1200   \n",
       "4      0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default  \n",
       "0         0         0         0        1  \n",
       "1      1000         0      2000        1  \n",
       "2      1000      1000      5000        0  \n",
       "3      1100      1069      1000        0  \n",
       "4      9000       689       679        0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"azureml://subscriptions/e2171f6d-2650-45e6-af7e-6d6e44ca92b1/resourcegroups/dataai/workspaces/dataaiamlwks/datastores/workspaceblobstore/paths/LocalUpload/49e795399bb8232f9e9a479a1c443f29/cleaned-credit-card.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
