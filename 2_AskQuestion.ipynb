{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "[LangChain](https://python.langchain.com/en/latest/index.html) is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:\n",
    "- Data-aware: connect a language model to other sources of data\n",
    "- Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The LangChain framework is designed around these principles.\n",
    "\n",
    "We will use Langchain framework for rest of the workshop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering over the docs/index\n",
    "Question answering in this context refers to question answering over your document data.  For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "from openai import OpenAI, AzureOpenAI, AsyncAzureOpenAI\n",
    "from Utilities.envVars import *\n",
    "import os\n",
    "\n",
    "os.environ[\"AZURESEARCH_FIELDS_ID\"] = \"id\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT\"] = \"content\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"] = \"contentVector\"\n",
    "os.environ[\"AZURESEARCH_FIELDS_TAG\"] = \"{}\"\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = SearchIndex\n",
    "\n",
    "azure_endpoint  = f\"{OpenAiEndPoint}\"\n",
    "api_key = OpenAiKey\n",
    "api_version = OpenAiVersion\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,  \n",
    "    api_version=api_version,\n",
    "    #base_url=f\"{os.getenv('OpenAiWestUsEp')}openai/deployments/{os.getenv('OpenAiGpt4v')}/extensions\",\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate answer for a question from the document we already indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from Utilities.cogSearch import performCogSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0.3\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        llm = AzureChatOpenAI(\n",
    "                azure_endpoint=OpenAiEndPoint,\n",
    "                api_version=OpenAiVersion,\n",
    "                azure_deployment=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "\n",
    "        embeddings = AzureOpenAIEmbeddings(azure_endpoint=OpenAiEndPoint, azure_deployment=OpenAiEmbedding, api_key=OpenAiKey, openai_api_type=\"azure\")\n",
    "        logging.info(\"LLM Setup done\")\n",
    "elif embeddingModelType == \"openai\":\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\astalati\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is a unified platform that provides data and analytics solutions for organizations. It covers various aspects such as data movement, data science, real-time analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. Fabric is built on a foundation of Software as a Service (SaaS) and aims to simplify analytics needs by providing an integrated and easy-to-use product. It brings together different experiences into a unified platform and offers a comprehensive big data analytics platform. Fabric includes components such as Data Engineering, Data Factory, and Data Science, each tailored to specific personas and tasks. It is built on top of ADLS (Azure Data Lake Storage) Gen2 and eliminates data silos. Fabric allows creators to focus on their work without the need to manage or understand the underlying infrastructure. \n",
      "SOURCES: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "# We already created our index and loaded the data, so we can skip that part. Let's try to ask a question:\n",
    "# Question answering involves fetching multiple documents, and then asking a question of them. \n",
    "# The LLM response will contain the answer to your question, based on the content of the documents.\n",
    "# The simplest way of using Langchain and LLM is to use load_qa_chain and run it with a query and a list of documents.\n",
    "\n",
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<What is Microsoft Fabric?>\\n<What are the components of Microsoft Fabric?>\\n<What is the purpose of OneLake in Microsoft Fabric?>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import QAGenerationChain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Let's generate followup questions\n",
    "followupTemplate = \"\"\"\n",
    "    Generate three very brief questions that the user would likely ask next.\n",
    "    Use double angle brackets to reference the questions, e.g. <What is Azure?>.\n",
    "    Try not to repeat questions that have already been asked.  Don't include the context in the answer.\n",
    "\n",
    "    Return the questions in the following format:\n",
    "    <>\n",
    "    <>\n",
    "    <>\n",
    "    \n",
    "    ALWAYS return a \"NEXT QUESTIONS\" part in your answer.\n",
    "\n",
    "    {context}\n",
    "    \"\"\"\n",
    "followupPrompt = PromptTemplate(template=followupTemplate, input_variables=[\"context\"])\n",
    "# followupChain = load_qa_chain(llm, chain_type='stuff', prompt=followupPrompt)\n",
    "# followupAnswer = followupChain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "# nextQuestions = followupAnswer['output_text'].replace(\"Answer: \", '').replace(\"Sources:\", 'SOURCES:').replace(\"Next Questions:\", 'NEXT QUESTIONS:').replace('NEXT QUESTIONS:', '').replace('NEXT QUESTIONS', '')\n",
    "# print(nextQuestions)\n",
    "\n",
    "rawDocs = \"\"\n",
    "llm_chain = LLMChain(prompt=followupPrompt, llm=llm)\n",
    "for doc in docs:\n",
    "    rawDocs = rawDocs + \"\\\\r\\\\n\" + doc.page_content\n",
    "\n",
    "llm_chain.predict(context=docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about we ask a question for which the answer is not in the document we have indexed in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I don't have the capability to tell jokes.\n"
     ]
    }
   ],
   "source": [
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Tell me a Joke\"\n",
    "#query = \"Who is the CEO of Microsoft\"\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we don't want to have LLM answer the question outside of the document we have indexed in Vector Store. We can use the custom prompt to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to the question \"Who is the CEO of Microsoft?\" is not contained within the provided text.\n"
     ]
    }
   ],
   "source": [
    "chainType = \"stuff\"\n",
    "topK = 3\n",
    "query = \"Who is the CEO of Microsoft\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "template = \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "            If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "            QUESTION: {question}\n",
    "            =========\n",
    "            {summaries}\n",
    "            =========\n",
    "            \"\"\"\n",
    "#qaPrompt = load_prompt('lc://prompts/qa_with_sources/stuff/basic.json')\n",
    "qaPrompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, prompt=qaPrompt)\n",
    "#qaChain.run(input_documents=docs, question=query)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain type\n",
    "This category of chains are used for interacting with indexes. The purpose these chains is to combine your own data (stored in the indexes) with LLMs. The best example of this is question answering over your own documents.\n",
    "\n",
    "A big part of this is understanding how to pass multiple documents to the language model. There are a few different methods, or chains, for doing so. LangChain supports four of the more common ones - and we are actively looking to include more, so if you have any ideas please reach out! Note that there is not one best method - the decision of which one to use is often very context specific. In order from simplest to most complex\n",
    "\n",
    "##### Stuff\n",
    "Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n",
    "\n",
    "- Pros: Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n",
    "- Cons: Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
    "\n",
    "##### Map-Reduce\n",
    "This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk). Then a different prompt is run to combine all the initial outputs. This is implemented in the LangChain as the MapReduceDocumentsChain.\n",
    "\n",
    "- Pros: Can scale to larger documents (and more documents) than StuffDocumentsChain. The calls to the LLM on individual documents are independent and can therefore be parallelized.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. Loses some information during the final combined call.\n",
    "\n",
    "##### Refine\n",
    "This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
    "\n",
    "- Pros: Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
    "- Cons: Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents.\n",
    "\n",
    "##### Map-Rerank\n",
    "This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n",
    "\n",
    "- Pros: Similar pros as MapReduceDocumentsChain. Requires fewer calls, compared to MapReduceDocumentsChain.\n",
    "- Cons: Cannot combine information between documents. This means it is most useful when you expect there to be a single simple answer in a single document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's test the same question with Map Reduce Chaintype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. Fabric brings together various experiences and offers a comprehensive big data analytics platform, allowing organizations and individuals to turn large and complex data repositories into actionable workloads and analytics. It unifies the OneLake and lakehouse architecture across enterprises, providing a unified location to store all organizational data and simplifying the user experience. Fabric allows creators to concentrate on producing their best work without the need to integrate, manage, or understand the underlying infrastructure. It offers a comprehensive set of analytics experiences tailored to specific personas and tasks, including Data Engineering, Data Factory, and Data Science. Data Engineering provides a Spark platform for data transformation and democratizing data through the lakehouse. Data Factory combines the simplicity of Power Query with the scale and power of Azure Data Factory. Data Science enables building, deploying, and operationalizing machine learning models seamlessly within the Fabric experience. \n",
      "\n",
      "SOURCES:\n",
      "- Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"map_reduce\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "\n",
    "qaTemplate = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
    "            Return any relevant text.\n",
    "            {context}\n",
    "            Question: {question}\n",
    "            Relevant text, if any :\"\"\"\n",
    "\n",
    "qaPrompt = PromptTemplate(\n",
    "    template=qaTemplate, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "combinePromptTemplate = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\"\"\"\n",
    "combinePrompt = PromptTemplate(\n",
    "    template=combinePromptTemplate, input_variables=[\"summaries\", \"question\"]\n",
    ")\n",
    "\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, \n",
    "                                     combine_prompt=combinePrompt, \n",
    "                                     return_intermediate_steps=True)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query})\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. With Fabric, you don't need to piece together different services from multiple vendors. Instead, you can enjoy a highly integrated, end-to-end, and easy-to-use product that is designed to simplify your analytics needs. The platform is built on a foundation of Software as a Service (SaaS), which takes simplicity and integration to a whole new level."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is a platform that brings together various experiences and offers a comprehensive big data analytics platform. It enables organizations and individuals to turn large and complex data repositories into actionable workloads and analytics. Microsoft Fabric unifies the OneLake and lakehouse architecture across enterprises. OneLake is the foundation of the Fabric services and is built on top of Azure Data Lake Storage (ADLS) Gen2. It provides a unified location to store all organizational data and simplifies the user experience by eliminating the need to understand infrastructure concepts or have an Azure account."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience. Microsoft Fabric offers the comprehensive set of analytics experiences designed to work together seamlessly. Each experience is tailored to a specific persona and a specific task. Fabric includes industry-leading experiences in the following categories for an end-to- end analytical need. Data Engineering - Data Engineering experience provides a world class Spark platform with great authoring experiences, enabling data engineers to perform large scale data transformation and democratize data through the lakehouse. Microsoft Fabric Spark's integration with Data Factory enables notebooks and spark jobs to be scheduled and orchestrated. For more information, see What is Data engineering in Microsoft Fabric? Data Factory - Azure Data Factory combines the simplicity of Power Query with the scale and power of Azure Data Factory. You can use more than 200 native connectors to connect to data sources on-premises and in the cloud. For more information, see What is Data Factory in Microsoft Fabric? Data Science - Data Science experience enables you to build, deploy, and operationalize machine learning models seamlessly within your Fabric experience. It integrates with Azure Machine Learning to provide built-in experiment tracking."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This time with Refine Chain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that brings together data movement, data science, real-time analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, data integration, and data science, all in one platform. Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience.\n",
      "\n",
      "Fabric includes industry-leading experiences in the following categories for an end-to-end analytical need:\n",
      "\n",
      "1. Data Engineering: The Data Engineering experience provides a world-class Spark platform with great authoring experiences, enabling data engineers to perform large-scale data transformation and democratize data through the lakehouse. Microsoft Fabric Spark's integration with Data Factory enables notebooks and Spark jobs to be scheduled and orchestrated.\n",
      "\n",
      "2. Data Factory: Azure Data Factory combines the simplicity of Power Query with the scale and power of Azure Data Factory. It offers more than 200 native connectors to connect to data sources on-premises and in the cloud.\n",
      "\n",
      "3. Data Science: The Data Science experience enables users to build, deploy, and operationalize machine learning models seamlessly within the Fabric platform. It integrates with Azure Machine Learning to provide built-in experiment tracking.\n",
      "\n",
      "Microsoft Fabric simplifies the user experience by eliminating the need to understand infrastructure concepts and does not require an Azure account. It eliminates data silos and enables seamless collaboration between professional and citizen developers. OneLake, also known as Microsoft Fabric Lake, serves as the foundation for all Fabric services and provides a unified location to store organizational data.\n",
      "\n",
      "Source: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "topK = 3\n",
    "query = \"What is Microsoft Fabric\"\n",
    "chainType = \"refine\"\n",
    "\n",
    "# Since we already index our document, we can perform the search on the query to retrieve \"TopK\" documents\n",
    "r = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, topK)\n",
    "\n",
    "if r == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in r\n",
    "        ]\n",
    "    \n",
    "refineTemplate = (\n",
    "                    \"The original question is as follows: {question}\\n\"\n",
    "                    \"We have provided an existing answer, including sources: {existing_answer}\\n\"\n",
    "                    \"We have the opportunity to refine the existing answer\"\n",
    "                    \"(only if needed) with some more context below.\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"{context_str}\\n\"\n",
    "                    \"------------\\n\"\n",
    "                    \"Given the new context, refine the original answer to better \"\n",
    "                    \"If you do update it, please update the sources as well. \"\n",
    "                    \"If the context isn't useful, return the original answer.\"\n",
    "                )\n",
    "refinePrompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "    template=refineTemplate,\n",
    ")\n",
    "\n",
    "qaTemplate = (\n",
    "    \"Answer the question as truthfully as possible using the provided text below, and if the answer is not contained within the text below, say \\\"I don't know\\\"\\n\"\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {question}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    ")\n",
    "qaPrompt = PromptTemplate(\n",
    "    input_variables=[\"context_str\", \"question\"], template=qaTemplate\n",
    ")\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, question_prompt=qaPrompt, refine_prompt=refinePrompt,\n",
    "                                     return_intermediate_steps=True)\n",
    "\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "modifiedAnswer = answer['output_text']\n",
    "print(modifiedAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is an all-in-one analytics solution for enterprises that brings together data movement, data science, real-time analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one platform. Microsoft Fabric enables organizations and individuals to transform large and complex data repositories into actionable workloads and analytics. It is an implementation of data mesh architecture and unifies the OneLake and lakehouse architecture across enterprises. OneLake, also known as Microsoft Fabric Lake, serves as the foundation for all Fabric services and provides a unified location to store organizational data. It simplifies the user experience by eliminating the need to understand infrastructure concepts and does not require an Azure account. OneLake eliminates data silos and enables seamless collaboration between professional and citizen developers. (Source: Fabric Get Started.pdf)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Chunk Summary:</b> Microsoft Fabric is an all-in-one analytics solution for enterprises that brings together data movement, data science, real-time analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, data integration, and data science, all in one platform. Fabric allows creators to concentrate on producing their best work, freeing them from the need to integrate, manage, or understand the underlying infrastructure that supports the experience.\n",
       "\n",
       "Fabric includes industry-leading experiences in the following categories for an end-to-end analytical need:\n",
       "\n",
       "1. Data Engineering: The Data Engineering experience provides a world-class Spark platform with great authoring experiences, enabling data engineers to perform large-scale data transformation and democratize data through the lakehouse. Microsoft Fabric Spark's integration with Data Factory enables notebooks and Spark jobs to be scheduled and orchestrated.\n",
       "\n",
       "2. Data Factory: Azure Data Factory combines the simplicity of Power Query with the scale and power of Azure Data Factory. It offers more than 200 native connectors to connect to data sources on-premises and in the cloud.\n",
       "\n",
       "3. Data Science: The Data Science experience enables users to build, deploy, and operationalize machine learning models seamlessly within the Fabric platform. It integrates with Azure Machine Learning to provide built-in experiment tracking.\n",
       "\n",
       "Microsoft Fabric simplifies the user experience by eliminating the need to understand infrastructure concepts and does not require an Azure account. It eliminates data silos and enables seamless collaboration between professional and citizen developers. OneLake, also known as Microsoft Fabric Lake, serves as the foundation for all Fabric services and provides a unified location to store organizational data.\n",
       "\n",
       "Source: Fabric Get Started.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the chaintype of MapReduce and Refine, we can also get insight into intermediate steps of the pipeline.\n",
    "# This way you can inspect the results from map_reduce chain type, each top similar chunk summary\n",
    "intermediateSteps = answer['intermediate_steps']\n",
    "for step in intermediateSteps:\n",
    "        display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
