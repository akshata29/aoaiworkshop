{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Index Data - Vector Store\n",
    "\n",
    "We will use [Azure Cognitive Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) to load and index the data.  Azure Cognitive Search is a cloud search service with built-in AI capabilities that enrich all types of information to easily identify and explore relevant content at scale. It uses the same integrated Microsoft natural language stack that Bing and Office have used for more than a decade, and AI services across vision, language, and speech, to deliver knowledge from structured and unstructured data.\n",
    "\n",
    "Cognitive search enabled the vector search feature! When done correctly, vector search is a proven technique for significantly increasing the semantic relevance of search results.  It is a technique that uses machine learning to embed text into a vector space, where the distance between vectors is a measure of semantic similarity.  This allows for the use of vector similarity search to find relevant results.  [Sign up]\n",
    "(https://aka.ms/VectorSearchSignUp) for Private Preview of Vector Search.\n",
    "\n",
    "Cognitive Search can index and store vectors, but it doesn't generate them out of the box. The documents that you push to your search service must contain vectors within the payload. Alternatively, you can use the Indexer to pull vectors from your data sources such as Blob Storage JSON files or CSVs. You can also use a Custom Skill to generate embeddings as part of the AI Enrichment process.\n",
    "\n",
    "\n",
    "[Sample repo](https://github.com/Azure/cognitive-search-vector-pr) to get started with vector search. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisite:\n",
    "- To run the code, install the following packages from local Wheel file. Alternatively, install azure-search-documents==11.4.0a20230509004 from the Dev Feed. For instructions on how to connect to the dev feed, please visit Azure-Python-SDK Azure Search Documents [Dev Feed](https://dev.azure.com/azure-sdk/public/_artifacts/feed/azure-sdk-for-python/connect/pip).\n",
    "- An Azure account with an active subscription. [Create an account for free](https://azure.microsoft.com/free/).\n",
    "- An Azure Cognitive Search service (any tier, any region). [Create a service](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) or find an [existing service](https://portal.azure.com/#blade/HubsExtension/BrowseResourceBlade/resourceType/Microsoft.Search%2FsearchServices) under your current subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ./azure_search_documents-11.4.0b4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "#%pip install langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai\n",
    "from Utilities.envVars import *\n",
    "\n",
    "# Set Search Service endpoint, index name, and API key from environment variables\n",
    "indexName = \"fabricbp\"\n",
    "\n",
    "# Set OpenAI API key and endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OpenAiVersion\n",
    "openai_api_key = OpenAiKey\n",
    "assert openai_api_key, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = openai_api_key\n",
    "openAiEndPoint = f\"{OpenAiEndPoint}\"\n",
    "openai.api_base = openAiEndPoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from langchain.llms.openai import AzureOpenAI, OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import (\n",
    "    PDFMinerLoader,\n",
    "    UnstructuredFileLoader,\n",
    ")\n",
    "from Utilities.cogSearch import createSearchIndex, indexSections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the PDF, create the chunk and push to Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexibility to change the call to OpenAI or Azure OpenAI\n",
    "embeddingModelType = \"azureopenai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file name and the namespace for the index\n",
    "fileName = \"Fabric Get Started.pdf\"\n",
    "fabricGetStartedPath = \"Data/PDF/\" + fileName\n",
    "# Load the PDF with Document Loader available from Langchain\n",
    "loader = PDFMinerLoader(fabricGetStartedPath)\n",
    "rawDocs = loader.load()\n",
    "# Set the source \n",
    "for doc in rawDocs:\n",
    "    doc.metadata['source'] = fabricGetStartedPath\n",
    "\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=125)\n",
    "docs = textSplitter.split_documents(rawDocs)\n",
    "# Call Helper function to create Index and Index the sections\n",
    "#createSearchIndex(SearchService, SearchKey, indexName)\n",
    "#indexSections(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, fileName, indexName, docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "embeddingModelType = \"azureopenai\"\n",
    "temperature = 0\n",
    "tokenLength = 1000\n",
    "\n",
    "if (embeddingModelType == 'azureopenai'):\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_key = OpenAiKey\n",
    "        openai.api_version = OpenAiVersion\n",
    "        openai.api_base = f\"{OpenAiEndPoint}\"\n",
    "\n",
    "        llm = AzureChatOpenAI(\n",
    "                openai_api_base=openai.api_base,\n",
    "                openai_api_version=OpenAiVersion,\n",
    "                deployment_name=OpenAiChat,\n",
    "                temperature=temperature,\n",
    "                openai_api_key=OpenAiKey,\n",
    "                openai_api_type=\"azure\",\n",
    "                max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(engine=OpenAiEmbedding, chunk_size=1, openai_api_key=OpenAiKey)\n",
    "elif embeddingModelType == \"openai\":\n",
    "        openai.api_type = \"open_ai\"\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_version = '2020-11-07' \n",
    "        openai.api_key = OpenAiApiKey\n",
    "        llm = ChatOpenAI(temperature=temperature,\n",
    "        openai_api_key=OpenAiApiKey,\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        max_tokens=tokenLength)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OpenAiApiKey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities.cogSearch import performCogSemanticHybridSearch\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Pure Vector Search\n",
    "#query = \"What is Microsoft Fabric\"\n",
    "#query = \"Is there a feature to run spark jobs on fabric?\"\n",
    "#query = \"Can you build ETL Pipelines and if so what tool do you use?\"\n",
    "#query = \"What exactly is Compute Unit in Fabric\"\n",
    "#query = \"What is the advantage of OneLake and OneSecurity in Fabric?\"\n",
    "#query = \"What kind of Data Governance features are available?\"\n",
    "query = \"¿Qué es Microsoft Fabric?\"\n",
    "\n",
    "results = performCogSemanticHybridSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, 3)\n",
    "# for result in results:\n",
    "#     print(f\"Id: {result['id']}\")  \n",
    "#     print(f\"Content: {result['content']}\")  \n",
    "#     print(f\"Source File: {result['sourcefile']}\\n\")\n",
    "\n",
    "if results == None:\n",
    "    docs = [Document(page_content=\"No results found\")]\n",
    "else :\n",
    "    docs = [\n",
    "        Document(page_content=doc['content'], metadata={\"id\": doc['id'], \"source\": doc['sourcefile']})\n",
    "        for doc in results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fabric home navigation\\n\\nEnd-to-end tutorials\\n\\nContext sensitive Help pane\\n\\nGet started with Fabric items\\n\\nｐ CONCEPT\\n\\nFind items in OneLake data hub\\n\\nPromote and certify items\\n\\nｃ HOW-TO GUIDE\\n\\nApply sensitivity labels\\n\\nWorkspaces\\n\\nｐ CONCEPT\\n\\nFabric workspace\\n\\n\\x0cWorkspace roles\\n\\nｂ GET STARTED\\n\\nCreate a workspace\\n\\nｃ HOW-TO GUIDE\\n\\nWorkspace access control\\n\\n\\x0cWhat is Microsoft Fabric?\\n\\nArticle • 05/23/2023\\n\\nMicrosoft Fabric is an all-in-one analytics solution for enterprises that covers everything', metadata={'id': 'Fabric_Get_Started_pdf-2', 'source': 'Fabric Get Started.pdf'}),\n",
       " Document(page_content=\"Article • 05/23/2023\\n\\nMicrosoft Fabric is an all-in-one analytics solution for enterprises that covers everything\\n\\nfrom data movement to data science, Real-Time Analytics, and business intelligence. It\\n\\noffers a comprehensive suite of services, including data lake, data engineering, and data\\n\\nintegration, all in one place.\\n\\nWith Fabric, you don't need to piece together different services from multiple vendors.\", metadata={'id': 'Fabric_Get_Started_pdf-3', 'source': 'Fabric Get Started.pdf'}),\n",
       " Document(page_content=\"Tell us about your PDF experience.\\n\\nMicrosoft Fabric get started\\ndocumentation\\n\\nMicrosoft Fabric is a unified platform that can meet your organization's data and\\nanalytics needs. Discover the Fabric shared and platform documentation from this page.\\n\\nAbout Microsoft Fabric\\n\\nｅ OVERVIEW\\n\\nWhat is Fabric?\\n\\nFabric terminology\\n\\nｂ GET STARTED\\n\\nStart a Fabric trial\\n\\nFabric home navigation\\n\\nEnd-to-end tutorials\\n\\nContext sensitive Help pane\\n\\nGet started with Fabric items\\n\\nｐ CONCEPT\", metadata={'id': 'Fabric_Get_Started_pdf-1', 'source': 'Fabric Get Started.pdf'})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Answer: Article • 05/23/2023\n",
      "\n",
      "Microsoft Fabric is<em> an all-in-one analytics solution for enterprises that covers everything\n",
      "\n",
      "from data movement to data science, Real-Time Analytics, and business intelligence.</em> It\n",
      "\n",
      "offers a comprehensive suite of services, including data lake, data engineering, and data\n",
      "\n",
      "integration, all in one place.\n",
      "\n",
      "With Fabric, you don't need to piece together different services from multiple vendors..\n",
      "Semantic Answer Score: 0.98876953125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Fabric is an all-in-one analytics solution for enterprises that covers everything from data movement to data science, Real-Time Analytics, and business intelligence. It offers a comprehensive suite of services, including data lake, data engineering, and data integration, all in one place. With Fabric, you don't need to piece together different services from multiple vendors.\n",
      "SOURCES: Fabric Get Started.pdf\n"
     ]
    }
   ],
   "source": [
    "chainType = \"stuff\"\n",
    "template = \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer. \n",
    "            If the answer is not contained within the text below, say \\\"I don't know\\\".\n",
    "\n",
    "            QUESTION: {question}\n",
    "            =========\n",
    "            {summaries}\n",
    "            =========\n",
    "            \"\"\"\n",
    "#qaPrompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "#qaChain = load_qa_with_sources_chain(llm, chain_type=chainType, prompt=qaPrompt)\n",
    "qaChain = load_qa_with_sources_chain(llm, chain_type=chainType)\n",
    "answer = qaChain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "outputAnswer = answer['output_text']\n",
    "print(outputAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vector Search with Multi-language support\n",
    "# query = \"¿Qué es Microsoft Fabric?\"\n",
    "\n",
    "# results = performCogSearch(OpenAiEndPoint, OpenAiKey, OpenAiVersion, OpenAiApiKey, SearchService, SearchKey, embeddingModelType, OpenAiEmbedding, query, indexName, 3)\n",
    "  \n",
    "# for result in results:  \n",
    "#     print(f\"Id: {result['id']}\")  \n",
    "#     print(f\"Content: {result['content']}\")  \n",
    "#     print(f\"Source File: {result['sourcefile']}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
